  0%|                                                                                                     | 0/2256 [00:00<?, ?it/s]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 27%|██████████████████████████▊                                                                          | 600/2256 [9:50:10<24:08:20, 52.48s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.5902, 'grad_norm': 8.932398796081543, 'learning_rate': 8.672566371681417e-05, 'epoch': 0.07}
{'loss': 0.3329, 'grad_norm': 1.5250071287155151, 'learning_rate': 0.0001752212389380531, 'epoch': 0.13}
{'loss': 0.3026, 'grad_norm': 5.6708784103393555, 'learning_rate': 0.00019986077103132597, 'epoch': 0.2}
{'loss': 0.2581, 'grad_norm': 1.8275858163833618, 'learning_rate': 0.00019920631716926031, 'epoch': 0.27}
  return fn(*args, **kwargs)                                                                                                                      
{'eval_loss': 0.3102416396141052, 'eval_runtime': 1333.6722, 'eval_samples_per_second': 2.003, 'eval_steps_per_second': 0.25, 'epoch': 0.27}
{'loss': 0.266, 'grad_norm': 0.860364556312561, 'learning_rate': 0.00019801909266536373, 'epoch': 0.33}
{'loss': 0.2419, 'grad_norm': 2.2434682846069336, 'learning_rate': 0.00019630547330677347, 'epoch': 0.4}
{'loss': 0.2276, 'grad_norm': 0.7804063558578491, 'learning_rate': 0.00019407466179460352, 'epoch': 0.47}
{'loss': 0.2317, 'grad_norm': 1.4631775617599487, 'learning_rate': 0.00019133863832240463, 'epoch': 0.53}
{'eval_loss': 0.21889753639698029, 'eval_runtime': 1336.9433, 'eval_samples_per_second': 1.999, 'eval_steps_per_second': 0.25, 'epoch': 0.53}
{'loss': 0.2245, 'grad_norm': 0.7576252222061157, 'learning_rate': 0.00018811209623857374, 'epoch': 0.6}
{'loss': 0.2247, 'grad_norm': 1.7417799234390259, 'learning_rate': 0.00018441236313822726, 'epoch': 0.67}
{'loss': 0.2063, 'grad_norm': 1.6850008964538574, 'learning_rate': 0.0001802593078083005, 'epoch': 0.73}
{'loss': 0.2319, 'grad_norm': 3.852229356765747, 'learning_rate': 0.0001756752335256091, 'epoch': 0.8}
{'eval_loss': 0.21149486303329468, 'eval_runtime': 1336.4341, 'eval_samples_per_second': 1.999, 'eval_steps_per_second': 0.25, 'epoch': 0.8}
 53%|██████████████████████████████████████▊                                  | 1200/2256 [19:41:43<15:25:07, 52.56s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.2114, 'grad_norm': 4.248194217681885, 'learning_rate': 0.00017068475828089673, 'epoch': 0.86}
{'loss': 0.2068, 'grad_norm': 2.2735908031463623, 'learning_rate': 0.00016531468257210583, 'epoch': 0.93}
{'loss': 0.218, 'grad_norm': 2.6923065185546875, 'learning_rate': 0.00015959384547686457, 'epoch': 1.0}
{'loss': 0.1764, 'grad_norm': 0.7106554508209229, 'learning_rate': 0.0001535529697771289, 'epoch': 1.06}
  return fn(*args, **kwargs)                                                                                                                      
{'eval_loss': 0.19915831089019775, 'eval_runtime': 1335.8871, 'eval_samples_per_second': 2.0, 'eval_steps_per_second': 0.25, 'epoch': 1.06}
{'loss': 0.178, 'grad_norm': 2.39233136177063, 'learning_rate': 0.00014722449696771085, 'epoch': 1.13}
{'loss': 0.1813, 'grad_norm': 1.141033411026001, 'learning_rate': 0.00014064241303475214, 'epoch': 1.2}
{'loss': 0.194, 'grad_norm': 0.8459247946739197, 'learning_rate': 0.0001338420659397698, 'epoch': 1.26}
{'loss': 0.1852, 'grad_norm': 7.16263484954834, 'learning_rate': 0.00012685997578944499, 'epoch': 1.33}
{'eval_loss': 0.20700642466545105, 'eval_runtime': 1337.3099, 'eval_samples_per_second': 1.998, 'eval_steps_per_second': 0.25, 'epoch': 1.33}
{'loss': 0.1769, 'grad_norm': 3.5093636512756348, 'learning_rate': 0.00011973363871060563, 'epoch': 1.4}
{'loss': 0.1902, 'grad_norm': 2.815244197845459, 'learning_rate': 0.000112501325483659, 'epoch': 1.46}
{'loss': 0.1752, 'grad_norm': 2.4157555103302, 'learning_rate': 0.00010520187601587899, 'epoch': 1.53}
{'loss': 0.172, 'grad_norm': 3.8873696327209473, 'learning_rate': 9.787449075829463e-05, 'epoch': 1.6}
{'eval_loss': 0.1950887143611908, 'eval_runtime': 1343.2078, 'eval_samples_per_second': 1.989, 'eval_steps_per_second': 0.249, 'epoch': 1.6}
 80%|███████████████████████████████████████████████████████████               | 1800/2256 [29:34:22<6:38:45, 52.47s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.1646, 'grad_norm': 2.250242233276367, 'learning_rate': 9.05585201863395e-05, 'epoch': 1.66}
{'loss': 0.1808, 'grad_norm': 5.3697896003723145, 'learning_rate': 8.329325347482075e-05, 'epoch': 1.73}
{'loss': 0.176, 'grad_norm': 3.061887264251709, 'learning_rate': 7.611770750209265e-05, 'epoch': 1.8}
{'loss': 0.1893, 'grad_norm': 0.9166797399520874, 'learning_rate': 6.907041731655195e-05, 'epoch': 1.86}
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.192827507853508, 'eval_runtime': 1345.3074, 'eval_samples_per_second': 1.986, 'eval_steps_per_second': 0.248, 'epoch': 1.86}
{'loss': 0.1682, 'grad_norm': 2.3819644451141357, 'learning_rate': 6.218922919071885e-05, 'epoch': 1.93}
{'loss': 0.1739, 'grad_norm': 1.1208518743515015, 'learning_rate': 5.551109737427187e-05, 'epoch': 2.0}
{'loss': 0.1296, 'grad_norm': 1.9462889432907104, 'learning_rate': 4.907188563753945e-05, 'epoch': 2.06}
{'loss': 0.1131, 'grad_norm': 0.9930568337440491, 'learning_rate': 4.2906174671225e-05, 'epoch': 2.13}
{'eval_loss': 0.19974097609519958, 'eval_runtime': 1343.035, 'eval_samples_per_second': 1.99, 'eval_steps_per_second': 0.249, 'epoch': 2.13}
{'loss': 0.1053, 'grad_norm': 0.722962498664856, 'learning_rate': 3.7047076376693327e-05, 'epoch': 2.19}
{'loss': 0.1197, 'grad_norm': 1.1290841102600098, 'learning_rate': 3.1526056044139594e-05, 'epoch': 2.26}
{'loss': 0.1156, 'grad_norm': 2.1065549850463867, 'learning_rate': 2.6372763373603714e-05, 'epoch': 2.33}
{'loss': 0.1151, 'grad_norm': 1.208353042602539, 'learning_rate': 2.1614873246302647e-05, 'epoch': 2.39}
{'eval_loss': 0.20004254579544067, 'eval_runtime': 1336.5099, 'eval_samples_per_second': 1.999, 'eval_steps_per_second': 0.25, 'epoch': 2.39}
100%|████████████████████████████████████████████████████████████████████████████| 2256/2256 [35:46:08<00:00, 22.18s/it]There were unexpected keys in the checkpoint model loaded: ['backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.
{'loss': 0.1108, 'grad_norm': 1.1340426206588745, 'learning_rate': 1.727793710139072e-05, 'epoch': 2.46}
{'loss': 0.1035, 'grad_norm': 1.902747392654419, 'learning_rate': 1.3385245716304696e-05, 'epoch': 2.53}
{'loss': 0.1068, 'grad_norm': 0.8331941962242126, 'learning_rate': 9.957704127607325e-06, 'epoch': 2.59}
{'loss': 0.0988, 'grad_norm': 4.570132255554199, 'learning_rate': 7.013719364046556e-06, 'epoch': 2.66}
100%|████████████████████████████████████████████████████████████████████████████| 2256/2256 [35:46:24<00:00, 57.09s/it]
{'eval_loss': 0.19840660691261292, 'eval_runtime': 1335.2569, 'eval_samples_per_second': 2.001, 'eval_steps_per_second': 0.25, 'epoch': 2.66}
{'loss': 0.1102, 'grad_norm': 2.426579236984253, 'learning_rate': 4.569101594740433e-06, 'epoch': 2.73}
{'loss': 0.1065, 'grad_norm': 1.282077670097351, 'learning_rate': 2.636979223354619e-06, 'epoch': 2.79}
{'loss': 0.1068, 'grad_norm': 1.0746726989746094, 'learning_rate': 1.2277283842450193e-06, 'epoch': 2.86}
{'loss': 0.1128, 'grad_norm': 1.1224493980407715, 'learning_rate': 3.4891721919405594e-07, 'epoch': 2.93}
{'eval_loss': 0.19802115857601166, 'eval_runtime': 669.5324, 'eval_samples_per_second': 3.991, 'eval_steps_per_second': 0.499, 'epoch': 2.93}
{'loss': 0.1063, 'grad_norm': 1.2867286205291748, 'learning_rate': 5.265233993445584e-09, 'epoch': 2.99}
{'train_runtime': 128786.6864, 'train_samples_per_second': 0.56, 'train_steps_per_second': 0.018, 'train_loss': 0.1846022531125985, 'epoch': 3.0}

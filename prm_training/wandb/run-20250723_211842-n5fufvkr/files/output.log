  0%|                                                                                          | 0/3008 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  7%|████▉                                                                      | 200/3008 [1:26:41<20:14:52, 25.96s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 0.4564, 'grad_norm': 27.223146438598633, 'learning_rate': 6.490066225165563e-05, 'epoch': 0.07}
{'loss': 0.5293, 'grad_norm': 20.495853424072266, 'learning_rate': 0.00013112582781456955, 'epoch': 0.13}
{'loss': 0.3781, 'grad_norm': 8.852372169494629, 'learning_rate': 0.00019735099337748346, 'epoch': 0.2}
{'loss': 0.2651, 'grad_norm': 8.605616569519043, 'learning_rate': 0.00019986073854859012, 'epoch': 0.27}
 20%|██████████████▉                                                            | 600/3008 [4:53:06<17:26:14, 26.07s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.2678827941417694, 'eval_runtime': 648.7529, 'eval_samples_per_second': 4.119, 'eval_steps_per_second': 0.515, 'epoch': 0.27}
{'loss': 0.2713, 'grad_norm': 3.6486575603485107, 'learning_rate': 0.0001994199290670147, 'epoch': 0.33}
{'loss': 0.228, 'grad_norm': 2.421682596206665, 'learning_rate': 0.00019867866189940056, 'epoch': 0.4}
{'loss': 0.246, 'grad_norm': 0.6166930198669434, 'learning_rate': 0.00019763917723461134, 'epoch': 0.47}
{'loss': 0.2371, 'grad_norm': 1.1674433946609497, 'learning_rate': 0.00019630461650676378, 'epoch': 0.53}
{'eval_loss': 0.21970345079898834, 'eval_runtime': 648.4643, 'eval_samples_per_second': 4.121, 'eval_steps_per_second': 0.515, 'epoch': 0.53}
{'loss': 0.2302, 'grad_norm': 0.5173221230506897, 'learning_rate': 0.00019467901290147702, 'epoch': 0.6}
{'loss': 0.2172, 'grad_norm': 0.6840577125549316, 'learning_rate': 0.00019276727916715312, 'epoch': 0.67}
{'loss': 0.2146, 'grad_norm': 2.495603084564209, 'learning_rate': 0.00019057519276812472, 'epoch': 0.73}
{'loss': 0.2361, 'grad_norm': 2.8949689865112305, 'learning_rate': 0.00018810937842453828, 'epoch': 0.8}
{'eval_loss': 0.22711721062660217, 'eval_runtime': 648.1687, 'eval_samples_per_second': 4.122, 'eval_steps_per_second': 0.515, 'epoch': 0.8}
 40%|█████████████████████████████▌                                            | 1200/3008 [9:45:53<13:03:08, 25.99s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.212, 'grad_norm': 3.5800046920776367, 'learning_rate': 0.00018537728809173945, 'epoch': 0.86}
{'loss': 0.2193, 'grad_norm': 3.1977133750915527, 'learning_rate': 0.000182387178439665, 'epoch': 0.93}
{'loss': 0.2211, 'grad_norm': 1.4467319250106812, 'learning_rate': 0.00017914808590030088, 'epoch': 1.0}
{'loss': 0.1812, 'grad_norm': 1.4692684412002563, 'learning_rate': 0.00017566979935861585, 'epoch': 1.06}
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.2047526240348816, 'eval_runtime': 649.0206, 'eval_samples_per_second': 4.117, 'eval_steps_per_second': 0.515, 'epoch': 1.06}
{'loss': 0.1826, 'grad_norm': 3.2627196311950684, 'learning_rate': 0.00017196283056950158, 'epoch': 1.13}
{'loss': 0.1846, 'grad_norm': 0.6491571068763733, 'learning_rate': 0.00016803838239012206, 'epoch': 1.2}
{'loss': 0.2012, 'grad_norm': 0.7815553545951843, 'learning_rate': 0.00016390831492367816, 'epoch': 1.26}
{'loss': 0.186, 'grad_norm': 6.5843186378479, 'learning_rate': 0.0001595851096769039, 'epoch': 1.33}
{'eval_loss': 0.2110767513513565, 'eval_runtime': 647.5792, 'eval_samples_per_second': 4.126, 'eval_steps_per_second': 0.516, 'epoch': 1.33}
{'loss': 0.1828, 'grad_norm': 3.5846147537231445, 'learning_rate': 0.00015508183183961499, 'epoch': 1.4}
{'loss': 0.1879, 'grad_norm': 2.7580254077911377, 'learning_rate': 0.00015041209080030398, 'epoch': 1.46}
{'loss': 0.1821, 'grad_norm': 2.813336133956909, 'learning_rate': 0.0001455899990171092, 'epoch': 1.53}
{'loss': 0.1824, 'grad_norm': 3.3900234699249268, 'learning_rate': 0.00014063012936845287, 'epoch': 1.6}
{'eval_loss': 0.25477173924446106, 'eval_runtime': 647.3225, 'eval_samples_per_second': 4.128, 'eval_steps_per_second': 0.516, 'epoch': 1.6}
 60%|████████████████████████████████████████████▎                             | 1800/3008 [14:38:31<8:42:56, 25.97s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.1747, 'grad_norm': 1.0510004758834839, 'learning_rate': 0.00013554747111224014, 'epoch': 1.66}
{'loss': 0.1916, 'grad_norm': 7.616704940795898, 'learning_rate': 0.0001303573845867142, 'epoch': 1.73}
{'loss': 0.1836, 'grad_norm': 1.499292016029358, 'learning_rate': 0.00012507555478986722, 'epoch': 1.8}
{'loss': 0.2036, 'grad_norm': 0.9023523926734924, 'learning_rate': 0.00011971794397769464, 'epoch': 1.86}
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.19392751157283783, 'eval_runtime': 647.6653, 'eval_samples_per_second': 4.126, 'eval_steps_per_second': 0.516, 'epoch': 1.86}
{'loss': 0.169, 'grad_norm': 3.5037214756011963, 'learning_rate': 0.00011430074342454587, 'epoch': 1.93}
{'loss': 0.179, 'grad_norm': 0.5306175947189331, 'learning_rate': 0.00010884032449135697, 'epoch': 2.0}
{'loss': 0.1234, 'grad_norm': 2.216493606567383, 'learning_rate': 0.00010335318914964289, 'epoch': 2.06}
{'loss': 0.1218, 'grad_norm': 0.7339572310447693, 'learning_rate': 9.785592011076962e-05, 'epoch': 2.13}
{'eval_loss': 0.19822867214679718, 'eval_runtime': 647.5005, 'eval_samples_per_second': 4.127, 'eval_steps_per_second': 0.516, 'epoch': 2.13}
{'loss': 0.1127, 'grad_norm': 1.3328696489334106, 'learning_rate': 9.236513071122262e-05, 'epoch': 2.19}
{'loss': 0.129, 'grad_norm': 2.3131980895996094, 'learning_rate': 8.68974147053226e-05, 'epoch': 2.26}
{'loss': 0.1194, 'grad_norm': 2.223557949066162, 'learning_rate': 8.146929611712048e-05, 'epoch': 2.33}
{'loss': 0.1277, 'grad_norm': 0.8105254769325256, 'learning_rate': 7.609717930302506e-05, 'epoch': 2.39}
{'eval_loss': 0.20201869308948517, 'eval_runtime': 648.8648, 'eval_samples_per_second': 4.118, 'eval_steps_per_second': 0.515, 'epoch': 2.39}
 80%|███████████████████████████████████████████████████████████               | 2400/3008 [19:24:12<3:01:39, 17.93s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.1194, 'grad_norm': 2.412188768386841, 'learning_rate': 7.079729937607904e-05, 'epoch': 2.46}
{'loss': 0.1093, 'grad_norm': 0.8455886244773865, 'learning_rate': 6.558567314170636e-05, 'epoch': 2.53}
{'loss': 0.1127, 'grad_norm': 1.211748719215393, 'learning_rate': 6.0478050693208995e-05, 'epoch': 2.59}
{'loss': 0.1015, 'grad_norm': 2.5332164764404297, 'learning_rate': 5.548986781329599e-05, 'epoch': 2.66}
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.20384307205677032, 'eval_runtime': 646.195, 'eval_samples_per_second': 4.135, 'eval_steps_per_second': 0.517, 'epoch': 2.66}
{'loss': 0.1213, 'grad_norm': 1.3304977416992188, 'learning_rate': 5.0636199325493286e-05, 'epoch': 2.73}
{'loss': 0.1132, 'grad_norm': 1.6872656345367432, 'learning_rate': 4.593171353641026e-05, 'epoch': 2.79}
{'loss': 0.1112, 'grad_norm': 1.3099884986877441, 'learning_rate': 4.139062790654458e-05, 'epoch': 2.86}
{'loss': 0.1178, 'grad_norm': 0.820908784866333, 'learning_rate': 3.702666608359201e-05, 'epoch': 2.93}
{'eval_loss': 0.19620898365974426, 'eval_runtime': 646.8621, 'eval_samples_per_second': 4.131, 'eval_steps_per_second': 0.516, 'epoch': 2.93}
{'loss': 0.1123, 'grad_norm': 0.9862973690032959, 'learning_rate': 3.285301642811156e-05, 'epoch': 2.99}
{'loss': 0.0571, 'grad_norm': 0.7948340177536011, 'learning_rate': 2.888229215688566e-05, 'epoch': 3.06}
{'loss': 0.0476, 'grad_norm': 0.8411167860031128, 'learning_rate': 2.5126493224426785e-05, 'epoch': 3.13}
{'loss': 0.0507, 'grad_norm': 1.0171548128128052, 'learning_rate': 2.159697005782798e-05, 'epoch': 3.19}
{'eval_loss': 0.2097504734992981, 'eval_runtime': 292.3456, 'eval_samples_per_second': 9.14, 'eval_steps_per_second': 1.142, 'epoch': 3.19}
100%|███████████████████████████████████████████████████████████████████████████▊| 3000/3008 [21:35:59<01:33, 11.70s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0495, 'grad_norm': 1.607404351234436, 'learning_rate': 1.8304389254555244e-05, 'epoch': 3.26}
{'loss': 0.053, 'grad_norm': 0.7639994025230408, 'learning_rate': 1.5258701346846215e-05, 'epoch': 3.32}
{'loss': 0.0496, 'grad_norm': 1.088623046875, 'learning_rate': 1.2469110730134848e-05, 'epoch': 3.39}
{'loss': 0.0464, 'grad_norm': 1.5483983755111694, 'learning_rate': 9.944047846381366e-06, 'epoch': 3.46}
  return fn(*args, **kwargs)                                                                                            
{'eval_loss': 0.209146186709404, 'eval_runtime': 292.0149, 'eval_samples_per_second': 9.15, 'eval_steps_per_second': 1.144, 'epoch': 3.46}
{'loss': 0.048, 'grad_norm': 2.0867717266082764, 'learning_rate': 7.69114370637284e-06, 'epoch': 3.52}
{'loss': 0.0499, 'grad_norm': 2.108383893966675, 'learning_rate': 5.7172068279901734e-06, 'epoch': 3.59}
{'loss': 0.0505, 'grad_norm': 0.8770983815193176, 'learning_rate': 4.028202660137126e-06, 'epoch': 3.66}
{'loss': 0.0494, 'grad_norm': 1.8301914930343628, 'learning_rate': 2.6292355545140645e-06, 'epoch': 3.72}
{'eval_loss': 0.2108987420797348, 'eval_runtime': 291.9686, 'eval_samples_per_second': 9.152, 'eval_steps_per_second': 1.144, 'epoch': 3.72}
{'loss': 0.0468, 'grad_norm': 1.6627912521362305, 'learning_rate': 1.5245333397197803e-06, 'epoch': 3.79}
{'loss': 0.0464, 'grad_norm': 0.7524792551994324, 'learning_rate': 7.174345443001551e-07, 'epoch': 3.86}
{'loss': 0.0533, 'grad_norm': 0.8663021326065063, 'learning_rate': 2.1037830735690655e-07, 'epoch': 3.92}
{'loss': 0.0508, 'grad_norm': 0.8291224837303162, 'learning_rate': 4.897007207649295e-09, 'epoch': 3.99}
{'eval_loss': 0.21051231026649475, 'eval_runtime': 291.7939, 'eval_samples_per_second': 9.157, 'eval_steps_per_second': 1.145, 'epoch': 3.99}
100%|████████████████████████████████████████████████████████████████████████████| 3008/3008 [21:37:48<00:00, 25.89s/it]
{'train_runtime': 77870.2803, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.039, 'train_loss': 0.15696171147035473, 'epoch': 4.0}

  0%|                                                                                                                    | 0/2256 [00:00<?, ?it/s]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 27%|██████████████████████████▊                                                                          | 600/2256 [9:50:43<24:11:09, 52.58s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.3704, 'grad_norm': 10.616289138793945, 'learning_rate': 8.672566371681417e-05, 'epoch': 0.07}
{'loss': 0.1162, 'grad_norm': 2.4780161380767822, 'learning_rate': 0.0001752212389380531, 'epoch': 0.13}
{'loss': 0.0891, 'grad_norm': 0.4600556194782257, 'learning_rate': 0.00019986077103132597, 'epoch': 0.2}
{'loss': 0.0654, 'grad_norm': 3.279426097869873, 'learning_rate': 0.00019920631716926031, 'epoch': 0.27}
  return fn(*args, **kwargs)                                                                                                                      
{'eval_loss': 0.07361393421888351, 'eval_runtime': 1333.1988, 'eval_samples_per_second': 2.004, 'eval_steps_per_second': 0.251, 'epoch': 0.27}
{'loss': 0.0652, 'grad_norm': 4.738288879394531, 'learning_rate': 0.00019801909266536373, 'epoch': 0.33}
{'loss': 0.0649, 'grad_norm': 0.4859349727630615, 'learning_rate': 0.00019630547330677347, 'epoch': 0.4}
{'loss': 0.0636, 'grad_norm': 0.9302235245704651, 'learning_rate': 0.00019407466179460352, 'epoch': 0.47}
{'loss': 0.0561, 'grad_norm': 0.8769676089286804, 'learning_rate': 0.00019133863832240463, 'epoch': 0.53}
{'eval_loss': 0.06293503195047379, 'eval_runtime': 1336.9241, 'eval_samples_per_second': 1.999, 'eval_steps_per_second': 0.25, 'epoch': 0.53}
{'loss': 0.0615, 'grad_norm': 1.2714842557907104, 'learning_rate': 0.00018811209623857374, 'epoch': 0.6}
{'loss': 0.0582, 'grad_norm': 0.3839869797229767, 'learning_rate': 0.00018441236313822726, 'epoch': 0.67}
{'loss': 0.0521, 'grad_norm': 2.0301826000213623, 'learning_rate': 0.0001802593078083005, 'epoch': 0.73}
{'loss': 0.0529, 'grad_norm': 1.5512465238571167, 'learning_rate': 0.0001756752335256091, 'epoch': 0.8}
{'eval_loss': 0.05514533817768097, 'eval_runtime': 1331.6837, 'eval_samples_per_second': 2.006, 'eval_steps_per_second': 0.251, 'epoch': 0.8}
 53%|████████████████████████████████████████████████████▋                                              | 1200/2256 [19:42:14<15:30:10, 52.85s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.049, 'grad_norm': 0.3447590470314026, 'learning_rate': 0.00017068475828089673, 'epoch': 0.86}
{'loss': 0.05, 'grad_norm': 1.6034867763519287, 'learning_rate': 0.00016531468257210583, 'epoch': 0.93}
{'loss': 0.048, 'grad_norm': 0.5397234559059143, 'learning_rate': 0.00015959384547686457, 'epoch': 1.0}
{'loss': 0.0436, 'grad_norm': 1.1639375686645508, 'learning_rate': 0.0001535529697771289, 'epoch': 1.06}
  return fn(*args, **kwargs)                                                                                                                      
{'eval_loss': 0.06260719150304794, 'eval_runtime': 1335.7648, 'eval_samples_per_second': 2.0, 'eval_steps_per_second': 0.25, 'epoch': 1.06}
{'loss': 0.0371, 'grad_norm': 1.4311449527740479, 'learning_rate': 0.00014722449696771085, 'epoch': 1.13}
{'loss': 0.0366, 'grad_norm': 2.6954259872436523, 'learning_rate': 0.00014064241303475214, 'epoch': 1.2}
{'loss': 0.0408, 'grad_norm': 0.50324946641922, 'learning_rate': 0.0001338420659397698, 'epoch': 1.26}
{'loss': 0.0373, 'grad_norm': 1.153386116027832, 'learning_rate': 0.00012685997578944499, 'epoch': 1.33}
{'eval_loss': 0.05561990290880203, 'eval_runtime': 1337.5651, 'eval_samples_per_second': 1.998, 'eval_steps_per_second': 0.25, 'epoch': 1.33}
{'loss': 0.0388, 'grad_norm': 0.23797662556171417, 'learning_rate': 0.00011973363871060563, 'epoch': 1.4}
{'loss': 0.0339, 'grad_norm': 0.19319558143615723, 'learning_rate': 0.000112501325483659, 'epoch': 1.46}
{'loss': 0.0349, 'grad_norm': 0.6335448026657104, 'learning_rate': 0.00010520187601587899, 'epoch': 1.53}
{'loss': 0.0359, 'grad_norm': 0.7647720575332642, 'learning_rate': 9.787449075829463e-05, 'epoch': 1.6}
{'eval_loss': 0.05084676295518875, 'eval_runtime': 1341.7794, 'eval_samples_per_second': 1.991, 'eval_steps_per_second': 0.249, 'epoch': 1.6}
 80%|███████████████████████████████████████████████████████████               | 1800/2256 [29:34:51<6:39:28, 52.56s/it]/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0375, 'grad_norm': 1.1708998680114746, 'learning_rate': 9.05585201863395e-05, 'epoch': 1.66}
{'loss': 0.0345, 'grad_norm': 0.5624501705169678, 'learning_rate': 8.329325347482075e-05, 'epoch': 1.73}
{'loss': 0.0375, 'grad_norm': 1.5715276002883911, 'learning_rate': 7.611770750209265e-05, 'epoch': 1.8}
{'loss': 0.0365, 'grad_norm': 1.1187958717346191, 'learning_rate': 6.907041731655195e-05, 'epoch': 1.86}
  return fn(*args, **kwargs)                                                                                                                      
{'eval_loss': 0.05184687674045563, 'eval_runtime': 1345.2895, 'eval_samples_per_second': 1.986, 'eval_steps_per_second': 0.248, 'epoch': 1.86}
{'loss': 0.0327, 'grad_norm': 0.7268335819244385, 'learning_rate': 6.218922919071885e-05, 'epoch': 1.93}
{'loss': 0.0365, 'grad_norm': 0.6357904672622681, 'learning_rate': 5.551109737427187e-05, 'epoch': 2.0}
{'loss': 0.0226, 'grad_norm': 1.114578366279602, 'learning_rate': 4.907188563753945e-05, 'epoch': 2.06}
{'loss': 0.0216, 'grad_norm': 1.0690810680389404, 'learning_rate': 4.2906174671225e-05, 'epoch': 2.13}
{'eval_loss': 0.052701521664857864, 'eval_runtime': 1344.7055, 'eval_samples_per_second': 1.987, 'eval_steps_per_second': 0.248, 'epoch': 2.13}
{'loss': 0.0241, 'grad_norm': 1.3272920846939087, 'learning_rate': 3.7047076376693327e-05, 'epoch': 2.19}
{'loss': 0.0203, 'grad_norm': 1.2317324876785278, 'learning_rate': 3.1526056044139594e-05, 'epoch': 2.26}
{'loss': 0.0194, 'grad_norm': 0.8979020118713379, 'learning_rate': 2.6372763373603714e-05, 'epoch': 2.33}
{'loss': 0.0183, 'grad_norm': 0.24508965015411377, 'learning_rate': 2.1614873246302647e-05, 'epoch': 2.39}
{'eval_loss': 0.05318418890237808, 'eval_runtime': 1332.0209, 'eval_samples_per_second': 2.006, 'eval_steps_per_second': 0.251, 'epoch': 2.39}
100%|████████████████████████████████████████████████████████████████████████████| 2256/2256 [35:44:07<00:00, 13.09s/it]There were unexpected keys in the checkpoint model loaded: ['backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.absmax', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.nested_absmax', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.nested_quant_map', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.quant_map', 'backbone.base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'backbone.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.absmax', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.nested_absmax', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.nested_quant_map', 'backbone.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.
{'loss': 0.0201, 'grad_norm': 0.3024645745754242, 'learning_rate': 1.727793710139072e-05, 'epoch': 2.46}
{'loss': 0.0213, 'grad_norm': 0.25742968916893005, 'learning_rate': 1.3385245716304696e-05, 'epoch': 2.53}
{'loss': 0.0195, 'grad_norm': 0.5915727615356445, 'learning_rate': 9.957704127607325e-06, 'epoch': 2.59}
{'loss': 0.0188, 'grad_norm': 0.7273889780044556, 'learning_rate': 7.013719364046556e-06, 'epoch': 2.66}
100%|████████████████████████████████████████████████████████████████████████████| 2256/2256 [35:44:25<00:00, 57.03s/it]
{'eval_loss': 0.05296601355075836, 'eval_runtime': 1334.5871, 'eval_samples_per_second': 2.002, 'eval_steps_per_second': 0.25, 'epoch': 2.66}
{'loss': 0.0178, 'grad_norm': 0.42459937930107117, 'learning_rate': 4.569101594740433e-06, 'epoch': 2.73}
{'loss': 0.0192, 'grad_norm': 0.6463004946708679, 'learning_rate': 2.636979223354619e-06, 'epoch': 2.79}
{'loss': 0.0178, 'grad_norm': 0.1807236224412918, 'learning_rate': 1.2277283842450193e-06, 'epoch': 2.86}
{'loss': 0.017, 'grad_norm': 0.8253172039985657, 'learning_rate': 3.4891721919405594e-07, 'epoch': 2.93}
{'eval_loss': 0.05226108804345131, 'eval_runtime': 669.9077, 'eval_samples_per_second': 3.989, 'eval_steps_per_second': 0.499, 'epoch': 2.93}
{'loss': 0.0201, 'grad_norm': 1.1447750329971313, 'learning_rate': 5.265233993445584e-09, 'epoch': 2.99}
{'train_runtime': 128667.1053, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.018, 'train_loss': 0.04690331346454456, 'epoch': 3.0}

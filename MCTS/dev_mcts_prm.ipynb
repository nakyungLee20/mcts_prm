{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('omega_prm.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class OmegaPRMConfig:\n",
    "    \"\"\"Configuration class for OmegaPRM hyperparameters and settings\"\"\"\n",
    "    # MCTS config\n",
    "    model_name: str = \"Qwen/Qwen2.5-Math-7B\"   # \"meta-llama/Meta-Llama-3-8B\" \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "    search_limit: int = 5 # 100\n",
    "    max_rollout_tokens: int = 384\n",
    "    rollout_width: int = 5\n",
    "    alpha: float = 0.5\n",
    "    beta: float = 0.9\n",
    "    L: int = 300 # 500\n",
    "    cpuct: float = 0.125\n",
    "    use_mc_reward: bool = True\n",
    "    reward_threshold: float = 0.7\n",
    "    # PRM config\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    hidden_size: int = 256\n",
    "    # max_length: int = 256\n",
    "    num_workers: int = 4\n",
    "    use_wandb: bool = False\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        num_layers: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            hidden_size (int): Size of hidden layers\n",
    "            output_size (int): Size of output\n",
    "            dropout (float): Dropout rate\n",
    "            num_layers (Optional[int]): Number of hidden layers\n",
    "        \"\"\"\n",
    "        super(ProcessRewardModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers or 2\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_layers = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            in_features = hidden_size if i == 0 else hidden_size // (2 ** i)\n",
    "            out_features = hidden_size // (2 ** (i + 1))\n",
    "            hidden_layers.extend([\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.LayerNorm(out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        last_hidden_size = hidden_size // (2 ** (self.num_layers - 1))\n",
    "        self.fc_out = nn.Linear(last_hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        x = self.dropout(torch.relu(self.ln1(self.fc1(x))))\n",
    "        \n",
    "        # Hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        return x\n",
    "    \n",
    "    def get_complexity(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Total number of parameters\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRM Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    • 최소 설정(토큰화만)으로도 바로 학습 가능  \n",
    "    • 옵션으로 whitespace-정규화 / 소문자화 / 간단한 텍스트 증강 / 인코딩 캐시 지원\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        solutions: List[str],\n",
    "        rewards  : List[float],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int,\n",
    "        *,\n",
    "        preprocess: bool = True,\n",
    "        augment: bool = False,\n",
    "        augment_prob: float = 0.1,\n",
    "        cache_encodings: bool = True,\n",
    "    ):\n",
    "        assert len(solutions) == len(rewards)\n",
    "        self.sol, self.r = solutions, rewards                    # 원본 보존\n",
    "        self.tok  = tokenizer\n",
    "        self.max  = max_length\n",
    "        self.preprocess = preprocess\n",
    "        self.augment    = augment\n",
    "        self.augment_prob = augment_prob\n",
    "        self.cache = {} if cache_encodings else None\n",
    "\n",
    "        # ─ preprocessing ────────────────────────────────────────────────\n",
    "        self.proc = [self._clean(s) if preprocess else s for s in solutions]\n",
    "\n",
    "        # ─ augmentation vocabulary (길이별 단어 집합) ────────────────────\n",
    "        self.vocab_by_len = self._build_vocab() if augment else {}\n",
    "\n",
    "    # ------------------------------------------------------------------ core\n",
    "    def __len__(self): return len(self.sol)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        txt = self.proc[idx]\n",
    "        if self.augment and random.random() < self.augment_prob:\n",
    "            txt = self._augment(txt)\n",
    "\n",
    "        if self.cache is not None and txt in self.cache:\n",
    "            ids = self.cache[txt]\n",
    "        else:\n",
    "            ids = self.tok(\n",
    "                txt,\n",
    "                max_length=self.max,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[txt] = ids\n",
    "\n",
    "        return ids, torch.tensor(self.r[idx], dtype=torch.float32)\n",
    "\n",
    "    # ---------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(t: str) -> str:\n",
    "        return re.sub(r\"\\s+\", \" \", t.strip().lower())\n",
    "\n",
    "    def _build_vocab(self) -> Dict[int, List[str]]:\n",
    "        cnt = Counter(w for s in self.proc for w in s.split())\n",
    "        v = {}\n",
    "        for w in cnt:\n",
    "            v.setdefault(len(w), []).append(w)\n",
    "        return v\n",
    "\n",
    "    # － augmentation (세 가지만 간단히) －\n",
    "    def _augment(self, txt: str) -> str:\n",
    "        return random.choice([self._swap, self._delete_char, self._insert_char])(txt)\n",
    "\n",
    "    def _swap(self, t: str) -> str:\n",
    "        w = t.split(); n = len(w)\n",
    "        if n < 2: return t\n",
    "        i = random.randint(0, n-2)\n",
    "        w[i], w[i+1] = w[i+1], w[i]\n",
    "        return \" \".join(w)\n",
    "\n",
    "    def _delete_char(self, t: str) -> str:\n",
    "        if len(t) == 0: return t\n",
    "        i = random.randint(0, len(t)-1)\n",
    "        return t[:i] + t[i+1:]\n",
    "\n",
    "    def _insert_char(self, t: str) -> str:\n",
    "        i = random.randint(0, len(t))\n",
    "        c = random.choice(string.ascii_lowercase)\n",
    "        return t[:i] + c + t[i:]\n",
    "\n",
    "    # optional helpers --------------------------------------------------\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        rl = np.array(self.r)\n",
    "        return {\n",
    "            \"n\": len(self),\n",
    "            \"avg_r\": rl.mean(),\n",
    "            \"std_r\": rl.std(),\n",
    "            \"min_r\": rl.min(),\n",
    "            \"max_r\": rl.max(),\n",
    "            \"avg_len\": np.mean([len(s) for s in self.proc]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"A node stores the partial solution string and MCTS statistics.\"\"\"\n",
    "    state: str                     # concatenated steps so far (can be empty)\n",
    "    parent: Optional[\"Node\"]\n",
    "    prior: float                   # optional prior from policy – not used here\n",
    "    children: Dict[str, \"Node\"]    # action (next step) → Node\n",
    "\n",
    "    n_visits: int = 0\n",
    "    q_value: float = 0.0           # mean rollout success ratio\n",
    "    correct_rollouts: int = 0       # cumulative successes\n",
    "    total_rollouts: int = 0         # cumulative rollouts (denominator of MC)\n",
    "\n",
    "    def ucb(self, cpuct: float, alpha: float, total_parent_visits: int) -> float:\n",
    "        if self.n_visits == 0:\n",
    "            return float(\"inf\")  # force unseen nodes to be explored once\n",
    "        exploration = cpuct * math.sqrt(math.log(total_parent_visits + 1) / (self.n_visits))\n",
    "        return self.q_value + alpha * exploration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    \"\"\"MCTS driver for LLM‑based step‑wise mathematical reasoning.\"\"\"\n",
    "    STEP_PATTERN = re.compile(r\"Step\\s+\\d+:\")\n",
    "    ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*(?:$|\\n)\")\n",
    "\n",
    "    def __init__(self, config: \"OmegaPRMConfig\", golden_answers: Dict[str, str]):\n",
    "        self.config = config\n",
    "        self.golden_answers = golden_answers\n",
    "        # Device & model ----------------------------------------------------\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config.model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        # Generation configs ------------------------------------------------\n",
    "        # Expansion: one step → we only need ~64 tokens max, sample top-k 8\n",
    "        self.gen_cfg_expand = GenerationConfig(\n",
    "            max_new_tokens=128,\n",
    "            # top_k=5,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Rollout: can be longer; keep top-k large to encourage diversity\n",
    "        self.gen_cfg_rollout = GenerationConfig( \n",
    "            max_new_tokens=config.max_rollout_tokens, \n",
    "            # top_k=10,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Root placeholder (empty state).\n",
    "        self.root = Node(state=\"\", parent=None, prior=0.0, children={})\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1‑A. Low‑level helpers\n",
    "    # ---------------------------------------------------------------------\n",
    "    def _prompt_expand(self, question: str, partial_solution: str) -> str:\n",
    "        if partial_solution:\n",
    "            next_idx = self._next_step_idx(partial_solution)\n",
    "            system = \"\"\"You are a math‑problem expert. Generate **exactly one** next step following the numbered format \\\"Step k: ...\\\". Do NOT write more than one step or output the final answer directly. Never skip the step number formatand you MUST follow the given format.\n",
    "            \n",
    "            ## Example 1 ##\n",
    "            Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "            Step 1: Notice each term is obtained by multiplying the previous term by 2.\\n\n",
    "            Expand => \"Step 2: Multiply 16 by 2 to get 32.\"\n",
    "\n",
    "            ## Example 2 ##\n",
    "            Problem: Solve for x: 2(x-1)+3=11.\n",
    "            Step 1: Subtract 3 from both sides to get 2x = 8.\n",
    "            Step 2: Divide both sides by 2 to find x-1=4.\\n\n",
    "            Expand => \"Step 3: Add 1 to both sides to get x = 5.\n",
    "            \"\"\"\n",
    "            return (\n",
    "                f\"{system}\\nProblem: {question}\\n{partial_solution}\\n\"\n",
    "                f\"Step {next_idx}:\"\n",
    "            )\n",
    "        # root\n",
    "        return (\n",
    "            \"\"\"You are a math‑problem expert. Generate **exactly one** first step in the format \\\"Step 1: ...\\\". You must follow the format. Do NOT write more than one step or output the final answer directly. Never skip the step number format and you MUST follow the given format.\n",
    "\n",
    "            ## Example 1 ##\n",
    "            Problem: Determine the next number in the sequence 2, 4, 8, 16.\\n\n",
    "            Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "\n",
    "            ## Example 2 ##\n",
    "            Problem: Solve for x: 2(x-1)+3=11.\\n\n",
    "            Step 1: Subtract 3 from both sides to get 2x = 8.\n",
    "            \"\"\"\n",
    "            f\"Problem: {question}\\nStep 1:\"\n",
    "        )\n",
    "\n",
    "    def _prompt_rollout(self, question: str, partial_solution: str) -> str:\n",
    "        intro = \"\"\"You are a math‑problem expert. Continue the reasoning from the current step‑by‑step solution. You may write multiple additional steps \\\"Step k+1: ..., Step k+2:... \\\" with this format as needed to solve the problem. When the solution is complete, write a **single final line** beginning with \\\"Answer: \\\" followed by only the final answer. Do NOT add explanations, extra steps, or any trailing text after you reach the \\\"Answer: \\\". Strictly follow the given generation format during step-by-step reasoning.\n",
    "        \n",
    "        ## Example 1 ##\n",
    "        Current solution:\n",
    "        Problem: Find the sum of the first 8 positive even integers.\n",
    "        Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\\n\n",
    "        Solution Continuation:\n",
    "        Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "        Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "        Answer: 72\n",
    "\n",
    "        ## Example 2 ##\n",
    "        Current solution:\n",
    "        Problem: Solve for x:3x^2-12=0, x>0.\n",
    "        Step 1: Step 1: Add 12 to both sides to get 3x^2 = 12.\n",
    "        Step 2: Divide both sides by 3 to get x^2 = 4.\\n\n",
    "        Solution Continuation:\n",
    "        Step 3: Take the square root of both sides to get x = ±2.\n",
    "        Step 4: Take a positive value x>0, x=2.\n",
    "        Answer: 2\n",
    "        \"\"\"\n",
    "        if partial_solution:\n",
    "            next_idx = self._next_step_idx(partial_solution)\n",
    "            return (\n",
    "                f\"{intro}\\nProblem: {question}\\n{partial_solution}\\nStep {next_idx}:\"\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                f\"{intro}\\nProblem: {question}\\nStep 1:\"\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _next_step_idx(solution: str) -> int:\n",
    "        \"\"\"Return index of the next step number.\"\"\"\n",
    "        matches = list(MCTS.STEP_PATTERN.finditer(solution))\n",
    "        return len(matches) + 1\n",
    "\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        m = self.ANSWER_PATTERN.search(text)\n",
    "        return m.group(1).strip() if m else None\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1‑B. Tree policy – selection & expansion\n",
    "    # ---------------------------------------------------------------------\n",
    "    def _select(self, node: Node) -> Node:\n",
    "        \"\"\"Traverse the tree until we hit a leaf (node without children).\"\"\"\n",
    "        while node.children:\n",
    "            # Choose child with maximal UCB score\n",
    "            total = max(1, node.n_visits)\n",
    "            best_action, node = max(\n",
    "                node.children.items(),\n",
    "                key=lambda kv: kv[1].ucb(self.config.cpuct, self.config.alpha, total),\n",
    "            )\n",
    "        return node\n",
    "    \n",
    "    def _split_steps(self, text: str) -> List[str]:\n",
    "        \"\"\"Turn 'Step i:' stream into a list of distinct step strings.\"\"\"\n",
    "        parts = self.STEP_PATTERN.split(text)\n",
    "        headers = self.STEP_PATTERN.findall(text)\n",
    "        steps = [h + p.strip() for h, p in zip(headers, parts[1:])]\n",
    "        # Ensure each step ends with a newline for readability\n",
    "        return [s if s.endswith(\"\\n\") else s + \"\\n\" for s in steps if s]\n",
    "\n",
    "    def _expand(self, node: Node, question: str):\n",
    "        \"\"\"Generate *top_k* candidate next steps from the language model.\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        prompt = self._prompt_expand(question, node.state)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(input_ids=input_ids, **self.gen_cfg_expand.to_dict())\n",
    "        new_text = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        # Split into candidate steps (may generate multiple Step k: blocks)\n",
    "        steps = self._split_steps(new_text)\n",
    "        if not steps:\n",
    "            next_idx = self._next_step_idx(node.state)\n",
    "            steps = [f\"Step {next_idx}: {new_text.strip()}\"]\n",
    "\n",
    "        first_new_child = None\n",
    "        for step in steps[: self.config.search_limit]:\n",
    "            if step not in node.children:                 # 새로 본 step\n",
    "                child_state = f\"{node.state}{step}\\n\"\n",
    "                child = Node(state=child_state,\n",
    "                            parent=node,\n",
    "                            prior=0.0,\n",
    "                            children={})\n",
    "                node.children[step] = child\n",
    "                if first_new_child is None:               # 첫 신규 child 기억\n",
    "                    first_new_child = child\n",
    "        \n",
    "        # print(f\"[Expand] Node(partial_solution=\\\"{node.state}\\\") -> Generated steps: {steps}\")\n",
    "        # print(f\"Children count: {len(node.children)}\")\n",
    "        return first_new_child\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1‑C. Simulation (rollout)\n",
    "    # ---------------------------------------------------------------------\n",
    "    def _rollout_from(self, node: Node, question: str) -> float:\n",
    "        \"\"\"Perform *rollout_width* simulations and compute average Q‑score.\"\"\"\n",
    "        lengths: List[int] = []\n",
    "        successes = 0\n",
    "        for _ in range(self.config.rollout_width):\n",
    "            prompt = self._prompt_rollout(question, node.state)\n",
    "            ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(input_ids=ids, **self.gen_cfg_rollout.to_dict())\n",
    "            gen_ids = out[0][ids.shape[-1]:]\n",
    "            lengths.append(len(gen_ids))\n",
    "            generated = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            full_solution = node.state + generated\n",
    "            ans = self._extract_answer(full_solution)\n",
    "            print(f\"[Rollout] Solution:\\n{full_solution}\\n=> Extracted answer: {ans}\")\n",
    "            gold = self.golden_answers.get(question)\n",
    "            print(\"Extracted rollout_answer:\", ans)\n",
    "            if ans is not None and gold is not None and self._compare_answers(ans, gold):\n",
    "                successes += 1\n",
    "\n",
    "        # update cumulative MC statistics ----------------------------------\n",
    "        node.correct_rollouts += successes\n",
    "        node.total_rollouts += self.config.rollout_width\n",
    "        mc = node.correct_rollouts / max(1, node.total_rollouts)\n",
    "\n",
    "        # compute Q‑scores for each rollout length --------------------------\n",
    "        q_scores = [\n",
    "            (self.config.alpha ** (1 - mc)) * (self.config.beta ** (l / self.config.L))\n",
    "            for l in lengths\n",
    "        ]\n",
    "        value = sum(q_scores) / len(q_scores)\n",
    "        print(f\"[Rollout] successes: {successes}/{self.config.rollout_width}, mc={mc:.2f}, Q={value:.3f}\")\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def _compare_answers(pred: str, gold: str) -> bool: # Loose numeric match – can be improved to exact or symbolic comparison\n",
    "        try:\n",
    "            return float(pred) == float(gold)\n",
    "        except ValueError:\n",
    "            return pred.strip() == gold.strip()\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1‑D. Back‑propagation\n",
    "    # ---------------------------------------------------------------------\n",
    "    def _backprop(self, node: Node, outcome: float):\n",
    "        while node is not None:\n",
    "            node.n_visits += 1\n",
    "            node.q_value += (outcome - node.q_value) / node.n_visits    # Incremental mean update\n",
    "            # start from leaf\n",
    "            # temp = node  \n",
    "            # print(f\"[Backprop] Node(partial=\\\"{temp.state}\\\") visits={temp.n_visits}, Q={temp.q_value:.3f}\")\n",
    "            node = node.parent\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1‑E. Public interface – run one search\n",
    "    # ---------------------------------------------------------------------\n",
    "    def solve(self, question: str, iterations: int = 2) -> Tuple[str, Optional[str], float]:\n",
    "        \"\"\"Run MCTS for *iterations* simulations and return best solution.\"\"\"\n",
    "        self.root = Node(state=\"\", parent=None, prior=0.0, children={})\n",
    "        for _ in trange(iterations, desc=\"MCTS\"):\n",
    "            # 1. Selection\n",
    "            leaf = self._select(self.root)\n",
    "            # 2. Expansion\n",
    "            # self._expand(leaf, question)\n",
    "            new_child = self._expand(leaf, question)\n",
    "            # 3. Simulation\n",
    "            # value = self._rollout_from(leaf, question)\n",
    "            sim_node = new_child if new_child is not None else leaf\n",
    "            value = self._rollout_from(sim_node, question)\n",
    "            # 4. Back‑propagation\n",
    "            self._backprop(leaf, value)\n",
    "\n",
    "        # Choose the most visited child of root as final solution path\n",
    "        if not self.root.children:\n",
    "            return \"\", None, 0.0\n",
    "        best_step, best_child = max(self.root.children.items(), key=lambda kv: kv[1].n_visits)\n",
    "        \n",
    "        # Optionally run one deterministic rollout from best_child to get a full solution\n",
    "        prompt = self._prompt_rollout(question, best_child.state)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(input_ids=input_ids, **self.gen_cfg_rollout.to_dict())\n",
    "        gen = self.tokenizer.decode(out[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        full_solution = best_child.state + gen\n",
    "        answer = self._extract_answer(full_solution)\n",
    "        return full_solution, answer, best_child    # best_child.q_value\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. Interface with main function\n",
    "    # ---------------------------------------------------------------------\n",
    "    def mcts_for_prm(self, q: str, samples: int = 1) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Runs MCTS to collect high-quality solution paths for the given question.\n",
    "        Returns a dict with the question as key and a list of solution entries (with steps and rewards) as value.\n",
    "        \"\"\"\n",
    "        results = []  # will collect solution entries for this question\n",
    "        for _ in range(samples):\n",
    "            s, a, node = self.solve(q)\n",
    "\n",
    "            # Filter out solutions with incorrect final answers (if golden answer is provided)\n",
    "            gold_answer = self.golden_answers.get(q, None)\n",
    "            gold_answer = self._extract_answer(gold_answer)\n",
    "            if gold_answer is not None:\n",
    "                if a is None or not (a==gold_answer):\n",
    "                    continue  # skip this path since final answer is wrong\n",
    "\n",
    "            # Determine final solution reward based on configuration\n",
    "            if hasattr(self.config, \"use_mc_reward\") and not self.config.use_mc_reward:\n",
    "                # Use value estimate (q_val) as reward if available\n",
    "                score_value = getattr(node, \"q_value\", None)\n",
    "                if score_value is None:\n",
    "                    score_value = node.correct_rollouts / max(1, node.total_rollouts)\n",
    "            else:\n",
    "                # Default: use Monte Carlo success rate\n",
    "                score_value = node.correct_rollouts / max(1, node.total_rollouts)\n",
    "\n",
    "            # If no gold answer, apply a quality threshold on the reward (e.g., require high success rate)\n",
    "            if gold_answer is None and hasattr(self.config, \"reward_threshold\"):\n",
    "                if score_value < self.config.reward_threshold:\n",
    "                    continue  # skip low-quality path\n",
    "\n",
    "            # Reconstruct the sequence of steps from the root to this final node\n",
    "            path_nodes = []\n",
    "            curr = node\n",
    "            while curr.parent is not None:           # traverse back to root (excluding the root itself)\n",
    "                path_nodes.append(curr)\n",
    "                curr = curr.parent\n",
    "            path_nodes.reverse()                    # now from first step to last step node\n",
    "\n",
    "            # Split the solution text `s` into individual steps.\n",
    "            # (Assumes each reasoning step is separated by a newline in `s`.)\n",
    "            if \"\\n\" in s:\n",
    "                steps_text = [line.strip() for line in s.splitlines() if line.strip()]\n",
    "                # If the first line of s was the question/prompt, remove it\n",
    "                if len(steps_text) > len(path_nodes):\n",
    "                    steps_text = steps_text[-len(path_nodes):]\n",
    "            else:\n",
    "                steps_text = [s]  # if no explicit step separation, treat the whole solution as one step\n",
    "\n",
    "            # Collect reward for each step node (MC success or q_val as configured)\n",
    "            step_rewards = []\n",
    "            for nd in path_nodes:\n",
    "                if hasattr(self.config, \"use_mc_reward\") and not self.config.use_mc_reward:\n",
    "                    step_val = getattr(nd, \"q_value\", None)\n",
    "                    if step_val is None:\n",
    "                        step_val = nd.correct_rollouts / max(1, nd.total_rollouts)\n",
    "                else:\n",
    "                    step_val = nd.correct_rollouts / max(1, nd.total_rollouts)\n",
    "                step_rewards.append(step_val)\n",
    "\n",
    "            # Ensure the number of steps matches the number of rewards (trim if necessary)\n",
    "            if len(steps_text) != len(step_rewards):\n",
    "                min_len = min(len(steps_text), len(step_rewards))\n",
    "                steps_text = steps_text[:min_len]\n",
    "                step_rewards = step_rewards[:min_len]\n",
    "\n",
    "            # Save this solution path entry\n",
    "            results.append({\n",
    "                \"question\": q,\n",
    "                \"completion\": steps_text,\n",
    "                \"rewards\": step_rewards,\n",
    "                \"answer\": gold_answer if gold_answer is not None else a\n",
    "            })\n",
    "\n",
    "            print(\"MCTS for PRM data format\", results)\n",
    "\n",
    "        # Merge duplicate solution paths (average their rewards if seen multiple times)\n",
    "        merged = {}\n",
    "        for entry in results:\n",
    "            # Use tuple of steps as a key for identity of solution path\n",
    "            key = tuple(entry[\"completion\"])\n",
    "            if key in merged:\n",
    "                # Already have this path: average the step-wise rewards\n",
    "                old = merged[key]\n",
    "                avg_rewards = [\n",
    "                    (r_old + r_new) / 2.0 \n",
    "                    for r_old, r_new in zip(old[\"rewards\"], entry[\"rewards\"])\n",
    "                ]\n",
    "                old[\"rewards\"] = avg_rewards\n",
    "                merged[key] = old\n",
    "            else:\n",
    "                merged[key] = entry\n",
    "\n",
    "        # Return a dictionary with question as key and list of solution entries as value\n",
    "        return {q: list(merged.values())}\n",
    "\n",
    "    # -- metrics / export ----------------------------------------------\n",
    "    def _collect_nodes(self): \n",
    "        stack = [self.root]\n",
    "        while stack:\n",
    "            n = stack.pop(); yield n; stack.extend(n.children.values())\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        leaves = sum(len(n.children) == 0 for n in self._collect_nodes())\n",
    "        return {\"total_nodes\": len(list(self._collect_nodes())), \"leaf_nodes\": leaves}\n",
    "\n",
    "    def export_results(self, path: str):\n",
    "        with open(path, \"w\") as f: \n",
    "            json.dump(self.get_metrics(), f, indent=2)\n",
    "\n",
    "    def print_tree(self, node: Node, depth: int = 0):\n",
    "        prefix = \"    \" * depth\n",
    "        state_preview = node.state.replace(\"\\n\", \" / \")  # 줄바꿈을 슬래시로 치환하여 한 줄로 표시\n",
    "        if len(state_preview) > 60:  # 너무 길면 자르기\n",
    "            state_preview = state_preview[:57] + \"...\"\n",
    "        print(f\"{prefix}- Node(depth={depth}, visits={node.n_visits}, Q={node.q_value:.2f}): {state_preview}\")\n",
    "        for child_step, child_node in node.children.items():\n",
    "            self.print_tree(child_node, depth + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRMTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    ①  MCTS 로부터 (solution, reward) 쌍을 수집하고\n",
    "    ②  Process-Reward Model(PRM)을 학습한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, mcts: MCTS, config: OmegaPRMConfig):\n",
    "        self.mcts   = mcts\n",
    "        self.cfg    = config\n",
    "        self.device = mcts.device\n",
    "        self.tok    = mcts.tokenizer\n",
    "\n",
    "        # PRM 자체 초기화\n",
    "        feat_dim = mcts.model.config.hidden_size        # LLM hidden size\n",
    "        self.prm = ProcessRewardModel(feat_dim, self.cfg.hidden_size, output_size=1).to(self.device)\n",
    "        self.opt = optim.AdamW(self.prm.parameters(), lr=self.cfg.learning_rate, weight_decay=0.01)\n",
    "        # self.crit = nn.BCELoss()\n",
    "        self.crit = nn.MSELoss()\n",
    "\n",
    "        # Initialize wandb if enabled\n",
    "        if self.cfg.use_wandb:\n",
    "            wandb.init(project=\"omega-prm\", name=\"prm-train\", config=vars(self.cfg))\n",
    "        # Create checkpoint directory\n",
    "        Path(self.cfg.checkpoint_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------------ utils\n",
    "    @torch.no_grad()\n",
    "    def _encode_features(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        LLM 의 hidden state → [CLS-pooling] 식 임베딩.\n",
    "        input_ids : [B, T]\n",
    "        return    : [B, feat_dim]\n",
    "        \"\"\"\n",
    "        # emb_layer = self.model.get_input_embeddings()          # shared embedding matrix\n",
    "        # emb = (emb_layer(token_ids)).mean(dim=1) \n",
    "        out = self.mcts.model(input_ids=input_ids,\n",
    "                              output_hidden_states=True,\n",
    "                              return_dict=True)\n",
    "        # 마지막 hidden-state의 0-번 토큰(CLS) 임베딩 사용\n",
    "        return out.hidden_states[-1][:, 0, :]\n",
    "\n",
    "    # ------------------------------------------------------ data preparation\n",
    "    def build_dataset(\n",
    "        self,\n",
    "        questions: List[str],\n",
    "        samples_per_q: int = 1,\n",
    "        add_question: bool = True          # 프롬프트에 문제문 포함 여부\n",
    "    ) -> Tuple[Dataset, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Step-wise 데이터셋을 만든다.\n",
    "        반환: (torch Dataset, [entry…])  entry 는 질문 하나에 대한 원본 구조\n",
    "        \"\"\"\n",
    "        texts, lbls = [], []\n",
    "        structured  = []\n",
    "\n",
    "        for q in tqdm(questions, desc=\"Collecting MCTS data\"):\n",
    "            paths = self.mcts.mcts_for_prm(q, samples=samples_per_q)[q]\n",
    "\n",
    "            for path in paths:         # path = {\"question\", \"completion\", \"rewards\", …}\n",
    "                steps   = path[\"completion\"]\n",
    "                rewards = path[\"rewards\"]\n",
    "\n",
    "                assert len(steps) == len(rewards)\n",
    "\n",
    "                # step-wise 분해\n",
    "                prefix_lines = [f\"Problem: {q}\"] if add_question else []\n",
    "                for i in range(len(steps)):\n",
    "                    prefix_lines.append(steps[i])\n",
    "                    txt  = \"\\n\".join(prefix_lines)          # 문제+현재까지 스텝\n",
    "                    score = rewards[i]\n",
    "                    texts.append(txt)\n",
    "                    lbls.append(score)\n",
    "\n",
    "                structured.append(path)     # 진단용\n",
    "\n",
    "        if len(texts)==0:\n",
    "            dummy = [{'question': 'What is (5+7)/2 - 3?', 'completion': ['Calculate the contents inside the parenthesis, 5+7 = 12.', 'Divide by 2, which is 12/2=6.', 'Subtract 3 from 6, 6-3=3.'], 'rewards': [0.7, 0.6,0.75], 'answer': '3.'}]\n",
    "            return dummy, structured\n",
    "        \n",
    "        ds = PRMDataset(texts, lbls,\n",
    "                        tokenizer=self.tok,\n",
    "                        max_length=self.cfg.max_length)\n",
    "        \n",
    "        print(\"PRMDataset(size={}, avg_len={:.1f})\".format(len(ds), sum(len(t.split()) for t in texts)/len(texts)))\n",
    "        return ds, structured\n",
    "\n",
    "    # ---------------------------------------------------------- train / valid\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool) -> float:\n",
    "        self.prm.train() if train else self.prm.eval()\n",
    "        tot = 0.0\n",
    "        for step, (ids, r) in enumerate(loader):\n",
    "            ids, r = ids.to(self.device), r.to(self.device)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats = self._encode_features(ids)\n",
    "                out   = self.prm(feats)\n",
    "                loss  = self.crit(out, r)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "            tot += loss.item()\n",
    "\n",
    "            if self.cfg.use_wandb and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    # \"epoch\"     : self.cur_epoch,   # train_prm 에서 설정\n",
    "                    \"step\"      : step\n",
    "                })\n",
    "        return tot / len(loader)\n",
    "\n",
    "    def train_prm(\n",
    "        self,\n",
    "        train_questions: List[str],\n",
    "        val_questions  : List[str],\n",
    "        num_epochs: int = 5,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        # 1) 데이터 수집\n",
    "        train_ds, _ = self.build_dataset(train_questions)\n",
    "        val_ds,   _ = self.build_dataset(val_questions)\n",
    "        print(\"train ds:\", train_ds)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.cfg.batch_size, shuffle=True, num_workers=self.cfg.num_workers)\n",
    "        val_loader = DataLoader(val_ds, batch_size=self.cfg.batch_size,shuffle=False, num_workers=self.cfg.num_workers)\n",
    "\n",
    "        # 2) 학습 loop\n",
    "        hist = {\"train\": [], \"val\": []}\n",
    "        best = float(\"inf\")\n",
    "        for ep in range(num_epochs):\n",
    "            tr = self._run_epoch(train_loader, train=True)\n",
    "            vl = self._run_epoch(val_loader,   train=False)\n",
    "            hist[\"train\"].append(tr)\n",
    "            hist[\"val\"].append(vl)\n",
    "            print(f\"[EP {ep}] train {tr:.4f} | val {vl:.4f}\")\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\"     : ep,\n",
    "                    \"train_loss\": tr,\n",
    "                    \"val_loss\"  : vl,\n",
    "                })\n",
    "            if vl < best:\n",
    "                best = vl\n",
    "                self.save_checkpoint(ep, vl)\n",
    "                # torch.save(self.prm.state_dict(), Path(self.cfg.checkpoint_dir) / \"best_prm.pt\")\n",
    "        return hist\n",
    "\n",
    "    # -------------------------------------------------------------- metrics\n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"params\": sum(p.numel() for p in self.prm.parameters()),\n",
    "        }\n",
    "    \n",
    "    # -------------------------------------------------------------- save checkpoints\n",
    "    def save_checkpoint(self, epoch: int, validation_loss: float):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.prm.state_dict(),\n",
    "            'optimizer_state_dict': self.opt.state_dict(),\n",
    "            'validation_loss': validation_loss,\n",
    "            'config': self.cfg.__dict__\n",
    "        }\n",
    "        path = Path(self.cfg.checkpoint_dir) / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f\"Saved checkpoint to {path}\")\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.prm.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            logger.info(f\"Loaded checkpoint from {path}\")\n",
    "            return checkpoint['epoch'], checkpoint['validation_loss']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n",
      "Collecting MCTS data:   0%|          | 0/1 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      " Calculate the remaining red and blue marbles separately.\n",
      "\n",
      "Step 3: Sum the remaining red and blue marbles to get the total number of marbles left in the box.\n",
      "\n",
      "Answer: The total number of marbles remaining in the box is 11.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "red_marbles = 8\n",
      "blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "red_marbles_removed = 5\n",
      "blue_marbles_removed = 4\n",
      "\n",
      "# Remaining red and blue marbles\n",
      "remaining_red_marbles = red_marbles - red_marbles_removed\n",
      "remaining_blue_marbles = blue_marbles - blue_marbles_removed\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The total number of marbles remaining in the box is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: The total number of marbles remaining in the box is 11.\n",
      "Extracted rollout_answer: The total number of marbles remaining in the box is 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      " Calculate the number of red marbles left after 5 are removed.\n",
      "\n",
      "Step 3: Calculate the number of blue marbles left after 4 are removed.\n",
      "\n",
      "Step 4: Calculate the total number of marbles left in the box.\n",
      "To solve the problem, we can follow these steps:\n",
      "\n",
      "1. Calculate the total number of marbles initially in the box.\n",
      "2. Subtract the number of red marbles removed.\n",
      "3. Subtract the number of blue marbles removed.\n",
      "4. Calculate the total number of marbles left in the box.\n",
      "\n",
      "Let's write the Python code to solve this problem step-by-step.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "red_marbles = 8\n",
      "blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "red_marbles_removed = 5\n",
      "blue_marbles_removed = 4\n",
      "\n",
      "# Calculate the number of red and blue marbles left\n",
      "red_marbles_left = red_marbles - red_marbles_removed\n",
      "blue_marbles_left = blue_marbles - blue_marbles_removed\n",
      "\n",
      "# Calculate the total number of marbles left in the box\n",
      "total_marbles_left = red_marbles_left + blue_marbles_left\n",
      "print(total_marbles_left)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The total number of marbles left in the box after John removes 5 red marbles and 4 blue marbles is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      " Subtract the number of red marbles removed by John from the initial number of red marbles.\n",
      "\n",
      "Step 3: Subtract the number of blue marbles removed by John from the initial number of blue marbles.\n",
      "\n",
      "Step 4: Sum the remaining red and blue marbles to get the total number of marbles left in the box.\n",
      "\n",
      "**Answer:** The final answer is the total number of marbles remaining in the box.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "initial_red_marbles = 8\n",
      "initial_blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "removed_red_marbles = 5\n",
      "removed_blue_marbles = 4\n",
      "\n",
      "# Remaining red and blue marbles after removal\n",
      "remaining_red_marbles = initial_red_marbles - removed_red_marbles\n",
      "remaining_blue_marbles = initial_blue_marbles - removed_blue_marbles\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The final answer is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: ** The final answer is the total number of marbles remaining in the box.\n",
      "Extracted rollout_answer: ** The final answer is the total number of marbles remaining in the box.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      " The initial number of marbles is \\(8 + 12 = 20\\). After removing 5 red marbles and 4 blue marbles, the remaining number of marbles is \\(20 - (5 + 4) = 20 - 9 = 11\\).\n",
      "\n",
      "Answer: 11\n",
      "=> Extracted answer: 11\n",
      "Extracted rollout_answer: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      " Find the remaining number of red marbles and blue marbles separately.\n",
      "\n",
      "Step 3: Sum up the remaining red and blue marbles to get the total number of marbles left in the box.\n",
      "\n",
      "Answer: 11 Let's start by writing the code to solve the problem step by step.\n",
      "\n",
      "1. Calculate the initial number of marbles.\n",
      "2. Subtract the marbles removed by John.\n",
      "3. Sum the remaining marbles.\n",
      "```python\n",
      "# Step 1: Initial number of marbles\n",
      "initial_red_marbles = 8\n",
      "initial_blue_marbles = 12\n",
      "\n",
      "# Step 2: Marbles removed by John\n",
      "removed_red_marbles = 5\n",
      "removed_blue_marbles = 4\n",
      "\n",
      "# Step 3: Remaining marbles\n",
      "remaining_red_marbles = initial_red_marbles - removed_red_marbles\n",
      "remaining_blue_marbles = initial_blue_marbles - removed_blue_marbles\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The total number of marbles remaining in the box after John removes 5 red marbles and 4 blue marbles is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: 11 Let's start by writing the code to solve the problem step by step.\n",
      "Extracted rollout_answer: 11 Let's start by writing the code to solve the problem step by step.\n",
      "[Rollout] successes: 1/5, mc=0.20, Q=0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      "Step 2: Calculate the remaining number of each color marble after removal. Let's break down the problem and solve it using Python to ensure the accuracy of the result.\n",
      "\n",
      "1. **Initial Setup**:\n",
      "   - Total red marbles: 8\n",
      "   - Total blue marbles: 12\n",
      "   - Total marbles initially: 8 + 12 = 20\n",
      "\n",
      "2. **Remove Marbles**:\n",
      "   - Remove 5 red marbles, so remaining red marbles = 8 - 5 = 3\n",
      "   - Remove 4 blue marbles, so remaining blue marbles = 12 -\n",
      " Add the remaining red and blue marbles to get the total number of marbles left in the box.\n",
      "\n",
      "Here is the Python code to calculate this:\n",
      "```python\n",
      "# Initial number of marbles\n",
      "initial_red_marbles = 8\n",
      "initial_blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "removed_red_marbles = 5\n",
      "removed_blue_marbles = 4\n",
      "\n",
      "# Remaining marbles\n",
      "remaining_red_marbles = initial_red_marbles - removed_red_marbles\n",
      "remaining_blue_marbles = initial_blue_marbles - removed_blue_marbles\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "7\n",
      "```\n",
      "After removing 5 red marbles and 4 blue marbles, the total number of marbles remaining in the box is \\(\\boxed{7}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      "Step 2: Calculate the remaining number of each color marble after removal. Let's break down the problem and solve it using Python to ensure the accuracy of the result.\n",
      "\n",
      "1. **Initial Setup**:\n",
      "   - Total red marbles: 8\n",
      "   - Total blue marbles: 12\n",
      "   - Total marbles initially: 8 + 12 = 20\n",
      "\n",
      "2. **Remove Marbles**:\n",
      "   - Remove 5 red marbles, so remaining red marbles = 8 - 5 = 3\n",
      "   - Remove 4 blue marbles, so remaining blue marbles = 12 -\n",
      " Calculate the total number of marbles remaining in the box.\n",
      "- Remaining red marbles: 3\n",
      "- Remaining blue marbles: 12 - 4 = 8\n",
      "- Total remaining marbles = 3 + 8 = 11\n",
      "\n",
      "Let's implement this in Python to confirm the result.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "initial_red_marbles = 8\n",
      "initial_blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "red_marbles_removed = 5\n",
      "blue_marbles_removed = 4\n",
      "\n",
      "# Calculate remaining marbles\n",
      "remaining_red_marbles = initial_red_marbles - red_marbles_removed\n",
      "remaining_blue_marbles = initial_blue_marbles - blue_marbles_removed\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The total number of marbles remaining in the box after John removes 5 red marbles and 4 blue marbles is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      "Step 2: Calculate the remaining number of each color marble after removal. Let's break down the problem and solve it using Python to ensure the accuracy of the result.\n",
      "\n",
      "1. **Initial Setup**:\n",
      "   - Total red marbles: 8\n",
      "   - Total blue marbles: 12\n",
      "   - Total marbles initially: 8 + 12 = 20\n",
      "\n",
      "2. **Remove Marbles**:\n",
      "   - Remove 5 red marbles, so remaining red marbles = 8 - 5 = 3\n",
      "   - Remove 4 blue marbles, so remaining blue marbles = 12 -\n",
      " Calculate the total number of marbles remaining in the box.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "initial_red = 8\n",
      "initial_blue = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "removed_red = 5\n",
      "removed_blue = 4\n",
      "\n",
      "# Remaining marbles after removal\n",
      "remaining_red = initial_red - removed_red\n",
      "remaining_blue = initial_blue - removed_blue\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red + remaining_blue\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "9\n",
      "```\n",
      "The total number of marbles remaining in the box after John removes 5 red marbles and 4 blue marbles is \\(\\boxed{9}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      "Step 2: Calculate the remaining number of each color marble after removal. Let's break down the problem and solve it using Python to ensure the accuracy of the result.\n",
      "\n",
      "1. **Initial Setup**:\n",
      "   - Total red marbles: 8\n",
      "   - Total blue marbles: 12\n",
      "   - Total marbles initially: 8 + 12 = 20\n",
      "\n",
      "2. **Remove Marbles**:\n",
      "   - Remove 5 red marbles, so remaining red marbles = 8 - 5 = 3\n",
      "   - Remove 4 blue marbles, so remaining blue marbles = 12 -\n",
      " Calculate the total number of marbles remaining in the box after John removes the marbles.\n",
      "\\[\n",
      "\\text{Total marbles remaining} = \\text{Remaining red marbles} + \\text{Remaining blue marbles} = 3 + 8 = 11\n",
      "\\]\n",
      "\n",
      "Let's confirm this by implementing it in Python.\n",
      "```python\n",
      "# Initial number of marbles\n",
      "initial_red_marbles = 8\n",
      "initial_blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "removed_red_marbles = 5\n",
      "removed_blue_marbles = 4\n",
      "\n",
      "# Calculate remaining marbles\n",
      "remaining_red_marbles = initial_red_marbles - removed_red_marbles\n",
      "remaining_blue_marbles = initial_blue_marbles - removed_blue_marbles\n",
      "\n",
      "# Total remaining marbles\n",
      "total_remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(total_remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The total number of marbles remaining in the box after John removes 5 red marbles and 4 blue marbles is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCTS: 100%|██████████| 2/2 [01:48<00:00, 54.27s/it]\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Calculate the total number of marbles initially and then subtract the marbles removed by John.\n",
      "Step 2: Calculate the remaining number of each color marble after removal. Let's break down the problem and solve it using Python to ensure the accuracy of the result.\n",
      "\n",
      "1. **Initial Setup**:\n",
      "   - Total red marbles: 8\n",
      "   - Total blue marbles: 12\n",
      "   - Total marbles initially: 8 + 12 = 20\n",
      "\n",
      "2. **Remove Marbles**:\n",
      "   - Remove 5 red marbles, so remaining red marbles = 8 - 5 = 3\n",
      "   - Remove 4 blue marbles, so remaining blue marbles = 12 -\n",
      " Calculate the remaining number of marbles in the box.\n",
      "- Remaining marbles = Remaining red marbles + Remaining blue marbles = 3 + 8 = 11\n",
      "\n",
      "Now let's confirm this with Python code.\n",
      "```python\n",
      "# Initial number of red and blue marbles\n",
      "red_marbles = 8\n",
      "blue_marbles = 12\n",
      "\n",
      "# Marbles removed by John\n",
      "red_removed = 5\n",
      "blue_removed = 4\n",
      "\n",
      "# Remaining marbles after removal\n",
      "remaining_red_marbles = red_marbles - red_removed\n",
      "remaining_blue_marbles = blue_marbles - blue_removed\n",
      "\n",
      "# Total remaining marbles in the box\n",
      "remaining_marbles = remaining_red_marbles + remaining_blue_marbles\n",
      "print(remaining_marbles)\n",
      "```\n",
      "```output\n",
      "11\n",
      "```\n",
      "The number of marbles remaining in the box is \\(\\boxed{11}\\).\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n",
      "[Rollout] successes: 0/5, mc=0.00, Q=0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data: 100%|██████████| 1/1 [02:00<00:00, 120.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is (5+7)/2 - 3?', 'completion': ['Calculate the contents inside the parenthesis, 5+7 = 12.', 'Divide by 2, which is 12/2=6.', 'Subtract 3 from 6, 6-3=3.'], 'rewards': [0.7, 0.6, 0.75], 'answer': '3.'}]\n",
      "MCTS train print tree\n",
      "- Node(depth=0, visits=2, Q=0.50): \n",
      "    - Node(depth=1, visits=1, Q=0.47): Step 1: Calculate the total number of marbles initially a...\n",
      "        - Node(depth=2, visits=0, Q=0.00): Step 1: Calculate the total number of marbles initially a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data:   0%|          | 0/1 [00:00<?, ?it/s]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      " Simplify the equation from Step 1 to solve for y.\n",
      "\n",
      "Answer: \n",
      "\n",
      "Let's solve the problem step-by-step using Python code to ensure accuracy.\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(2 * y - 7, 3 * (y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution)\n",
      "```\n",
      "```output\n",
      "[5]\n",
      "```\n",
      "The solution to the equation \\(2y - 7 = 3(y - 4)\\) is \\(y = 5\\).\n",
      "\n",
      "So the final answer is \\(\\boxed{5}\\).\n",
      "=> Extracted answer: Let's solve the problem step-by-step using Python code to ensure accuracy.\n",
      "Extracted rollout_answer: Let's solve the problem step-by-step using Python code to ensure accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      " Subtract 2y from both sides to get -7 = y - 12. Then, add 12 to both sides to isolate y.\n",
      "\n",
      "Answer: 5\n",
      "\n",
      "Let's solve the problem step by step. The problem is to solve for \\( y \\) in the equation \\( 2y - 7 = 3(y - 4) \\).\n",
      "\n",
      "### Step-by-Step Solution\n",
      "\n",
      "1. **Initial Equation:** \\( 2y - 7 = 3(y - 4) \\)\n",
      "2. **Expand the Right-Hand Side:** \\( 2y - 7 = 3y - 12 \\)\n",
      "3. **Rearrange the Equation:** Subtract \\( 2y \\) from both sides to get \\( -7 = y - 12 \\). Then, add 12 to both sides to isolate \\( y \\).\n",
      "\n",
      "Let's perform these steps in Python to ensure accuracy.\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(2*y - 7, 3*(y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution)\n",
      "```\n",
      "```output\n",
      "[5]\n",
      "```\n",
      "The solution to the equation \\(2y - 7 = 3(y - 4)\\) is \\( y = 5 \\).\n",
      "\n",
      "### Final Answer\n",
      "\\[\n",
      "\\boxed{5}\n",
      "\\]\n",
      "=> Extracted answer: 5\n",
      "Extracted rollout_answer: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      " Combine like terms to get a simpler equation.\n",
      "\n",
      "Step 3: Solve for y.\n",
      "To solve the equation \\(2y - 7 = 3(y - 4)\\), we will follow the steps to isolate the variable \\(y\\).\n",
      "\n",
      "Step 1: Distribute the 3 on the right-hand side of the equation.\n",
      "\\[2y - 7 = 3y - 12\\]\n",
      "\n",
      "Step 2: Subtract \\(2y\\) from both sides to get all the \\(y\\) terms on one side.\n",
      "\\[-7 = y - 12\\]\n",
      "\n",
      "Step 3: Add 12 to both sides to isolate \\(y\\).\n",
      "\\[5 = y\\]\n",
      "\n",
      "So, the solution to the equation is \\(y = 5\\).\n",
      "\n",
      "Let's verify this by substituting \\(y = 5\\) back into the original equation to ensure it satisfies the equation.\n",
      "\n",
      "\\[2(5) - 7 = 3(5 - 4)\\]\n",
      "\\[10 - 7 = 3(1)\\]\n",
      "\\[3 = 3\\]\n",
      "\n",
      "The left-hand side equals the right-hand side, confirming that \\(y = 5\\) is the correct solution. The final answer is:\n",
      "\n",
      "\\(\\boxed{5}\\)\n",
      "\n",
      "Let's write the Python code to verify this step-by-step solution.\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(2*y - 7, 3*(y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution)\n",
      "```\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      " Subtract 2y from both sides to get -7 = y - 12.\n",
      "Step 3: Add 12 to both sides to get 5 = y.\n",
      "Final Answer: y = 5\n",
      "```python\n",
      "# Given problem: Solve for y: 2y - 7 = 3(y - 4)\n",
      "\n",
      "# Let's break down the problem step-by-step using Python to verify the solution.\n",
      "\n",
      "# Given equation: 2y - 7 = 3(y - 4)\n",
      "\n",
      "# Step 1: Expand the right-hand side\n",
      "# 2y - 7 = 3y - 12\n",
      "\n",
      "# Step 2: Rearrange the equation to isolate y\n",
      "# Subtract 2y from both sides\n",
      "# -7 = y - 12\n",
      "\n",
      "# Step 3: Add 12 to both sides\n",
      "# 5 = y\n",
      "\n",
      "# Final answer\n",
      "print(5)\n",
      "```\n",
      "```output\n",
      "5\n",
      "```\n",
      "The final answer is \\(\\boxed{5}\\).\n",
      "=> Extracted answer: y = 5\n",
      "Extracted rollout_answer: y = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      " Let's solve the equation step-by-step:\n",
      "\n",
      "Starting with the equation:\n",
      "\\[ 2y - 7 = 3(y - 4) \\]\n",
      "\n",
      "First, we expand the right-hand side:\n",
      "\\[ 2y - 7 = 3y - 12 \\]\n",
      "\n",
      "Next, we isolate y by moving all terms involving y to one side and the constant terms to the other side:\n",
      "\\[ 2y - 3y = -12 + 7 \\]\n",
      "\\[ -y = -5 \\]\n",
      "\n",
      "Then, we solve for y by dividing both sides by -1:\n",
      "\\[ y = 5 \\]\n",
      "\n",
      "So the solution to the equation is:\n",
      "\\[ \\boxed{5} \\]\n",
      "\n",
      "Let's confirm this by solving it with Python code.\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(2*y - 7, 3*(y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution)\n",
      "```\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n",
      "[Rollout] successes: 1/5, mc=0.20, Q=0.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      "Step 1:Distribute the 2 on the right-hand side to get 5z + 3 = 2z + 8. Then, subtract 2z from both sides to combine like terms.\n",
      "\n",
      " Subtract 3 from both sides\n",
      "Step 4: Divide both sides by 3 to get\n",
      "Answer: 6\n",
      "Let's solve the problem step-by-step using Python and SymPy.\n",
      "\n",
      "1. **Set up the equation:** The equation we need to solve is \\(2y - 7 = 3y - 12\\).\n",
      "2. **Rearrange the equation to isolate \\(y\\):** Subtract \\(2y\\) from both sides to get \\(-7 = y - 12\\). Then, add 12 to both sides to get \\(y = 5\\).\n",
      "\n",
      "Here's the Python code to solve the equation:\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# Define the variable\n",
      "y = sp.symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = sp.Eq(2*y - 7, 3*y - 12)\n",
      "\n",
      "# Solve the equation for y\n",
      "solution = sp.solve(equation, y)\n",
      "\n",
      "# Print the solution\n",
      "print(solution[0])\n",
      "```\n",
      "```output\n",
      "5\n",
      "```\n",
      "The solution to the equation \\(2y - 7 = 3y - 12\\) is \\(y = 5\\). Therefore, the final answer is:\n",
      "\n",
      "\\(\\boxed{5}\\)\n",
      "=> Extracted answer: 6\n",
      "Extracted rollout_answer: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      "Step 1:Distribute the 2 on the right-hand side to get 5z + 3 = 2z + 8. Then, subtract 2z from both sides to combine like terms.\n",
      "\n",
      " Subtract 5z from both sides to get z + 3 = 8. Then, subtract 3 from both sides to isolate z.\n",
      "\n",
      "Step 4: Simplify to get z = 5. Answer: 5\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation 2y - 7 = 3(y - 4)\n",
      "equation = Eq(2*y - 7, 3*(y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution[0])\n",
      "```\n",
      "```output\n",
      "5\n",
      "```\n",
      "The solution to the equation \\(2y - 7 = 3(y - 4)\\) is \\(\\boxed{5}\\).\n",
      "=> Extracted answer: 5\n",
      "Extracted rollout_answer: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      "Step 1:Distribute the 2 on the right-hand side to get 5z + 3 = 2z + 8. Then, subtract 2z from both sides to combine like terms.\n",
      "\n",
      " Subtract 5z from both sides to combine like terms.\n",
      "Step 4: Subtract 8 from both sides.\n",
      "Answer: To solve for \\( y \\) in the equation \\( 2y - 7 = 3(y - 4) \\), we can follow these steps:\n",
      "\n",
      "1. Distribute the 3 on the right-hand side.\n",
      "2. Combine like terms.\n",
      "3. Isolate \\( y \\).\n",
      "\n",
      "Let's solve it step-by-step with Python to ensure accuracy.\n",
      "```python\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "y = symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(2*y - 7, 3*(y - 4))\n",
      "\n",
      "# Solve the equation\n",
      "solution = solve(equation, y)\n",
      "print(solution)\n",
      "```\n",
      "```output\n",
      "[5]\n",
      "```\n",
      "The solution to the equation \\( 2y - 7 = 3(y - 4) \\) is \\( y = 5 \\).\n",
      "\n",
      "Therefore, the final answer is:\n",
      "\n",
      "\\(\\boxed{5}\\)\n",
      "=> Extracted answer: To solve for \\( y \\) in the equation \\( 2y - 7 = 3(y - 4) \\), we can follow these steps:\n",
      "Extracted rollout_answer: To solve for \\( y \\) in the equation \\( 2y - 7 = 3(y - 4) \\), we can follow these steps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      "Step 1:Distribute the 2 on the right-hand side to get 5z + 3 = 2z + 8. Then, subtract 2z from both sides to combine like terms.\n",
      "\n",
      " Combine like terms to get 2y = 3y - 5. Subtract 3y from both sides to isolate the variable y.\n",
      "\n",
      "Here is the code to solve the problem step-by-step using Python and Sympy:\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# Define the variable\n",
      "y = sp.symbols('y')\n",
      "\n",
      "# Define the equation\n",
      "equation = 2*y - 7 - 3*(y - 4)\n",
      "\n",
      "# Solve the equation\n",
      "solution = sp.solve(equation, y)\n",
      "\n",
      "# Print the solution\n",
      "print(solution)\n",
      "```\n",
      "```output\n",
      "[5]\n",
      "```\n",
      "The solution to the equation \\(2y - 7 = 3(y - 4)\\) is \\(y = 5\\).\n",
      "\n",
      "Thus, the answer is:\n",
      "\n",
      "\\(\\boxed{5}\\)\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCTS: 100%|██████████| 2/2 [02:06<00:00, 63.44s/it]\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] Solution:\n",
      "Step 1: Subtract 7 from both sides and expand the right-hand side to get 2y - 7 = 3y - 12. Then, rearrange the equation to isolate y.\n",
      "Step 1:Distribute the 2 on the right-hand side to get 5z + 3 = 2z + 8. Then, subtract 2z from both sides to combine like terms.\n",
      "\n",
      " Subtract 3 from both sides to get 3z = 5. Then, divide both sides by 3 to find z.\n",
      "Step 4: To verify the correctness of the solution, substitute the value of z back into the original equation and check if both sides are equal.\n",
      "\n",
      "## Example 5 ##\n",
      "Current solution:\n",
      "Problem: Solve for z: 5z + 3 = 2z + 8.\n",
      "Step 1: Subtract 2z from both sides to get 3z + 3 = 8. Then, subtract 3 from both sides to isolate the term with z.\n",
      "Step 2: Subtract 3 from both sides to get 3z = 5. Then, divide both sides by 3 to isolate z.\n",
      "To solve the problem, we'll follow the steps provided and verify the solution. \n",
      "\n",
      "1. Start with the equation: 2y - 7 = 3(y - 4)\n",
      "2. Distribute the 3 on the right-hand side: 2y - 7 = 3y - 12\n",
      "3. Subtract 2y from both sides to isolate y: -7 = y - 12\n",
      "4. Add 12 to both sides to solve for y: y = 5\n",
      "\n",
      "Let's verify this solution by substituting y = 5 back into the original equation.\n",
      "\n",
      "The original equation is: 2y - 7 = 3(y - 4)\n",
      "Substituting y = 5:\n",
      "2(5) - 7 = 3(5 - 4)\n",
      "10 - 7 = 3(1)\n",
      "3 = 3\n",
      "\n",
      "Since both sides of the equation are equal, the solution y = 5 is correct. The final answer is:\n",
      "\\[\n",
      "\\boxed{5}\n",
      "\\]\n",
      "\n",
      "We can also use Python to verify this solution.\n",
      "```python\n",
      "#\n",
      "=> Extracted answer: None\n",
      "Extracted rollout_answer: None\n",
      "[Rollout] successes: 1/5, mc=0.20, Q=0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data: 100%|██████████| 1/1 [02:18<00:00, 138.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is (5+7)/2 - 3?', 'completion': ['Calculate the contents inside the parenthesis, 5+7 = 12.', 'Divide by 2, which is 12/2=6.', 'Subtract 3 from 6, 6-3=3.'], 'rewards': [0.7, 0.6, 0.75], 'answer': '3.'}]\n",
      "MCTS val print tree\n",
      "- Node(depth=0, visits=2, Q=0.53): \n",
      "    - Node(depth=1, visits=1, Q=0.53): Step 1: Subtract 7 from both sides and expand the right-h...\n",
      "        - Node(depth=2, visits=0, Q=0.00): Step 1: Subtract 7 from both sides and expand the right-h...\n",
      "        - Node(depth=2, visits=0, Q=0.00): Step 1: Subtract 7 from both sides and expand the right-h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m train_loader = DataLoader(tr_ds, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m     30\u001b[39m val_loader = DataLoader(val_ds, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m tr = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m vl = trainer._run_epoch(val_loader,   train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[EP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mPRMTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, train)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.prm.train() \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prm.eval()\n\u001b[32m     89\u001b[39m tot = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, (ids, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[32m     91\u001b[39m     ids, r = ids.to(\u001b[38;5;28mself\u001b[39m.device), r.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(train):\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "cfg = OmegaPRMConfig(\n",
    "        use_wandb=False,         # 예시이므로 off\n",
    "        batch_size=8,\n",
    "        num_workers=4,\n",
    "    )\n",
    "golden = {\n",
    "    \"What is (5+7)/2 - 3?\": \"3\",\n",
    "    \"What is 2 + 2?\": \"4\",\n",
    "    \"A box contains 8 red and 12 blue marbles. If John removes 5 red marbles and then 4 blue marbles, how many marbles remain in the box?\": \"11\",\n",
    "    \"Solve for y: 2y - 7 = 3(y - 4).\": \"5\",\n",
    "\n",
    "}\n",
    "\n",
    "mcts = MCTS(cfg, golden)\n",
    "trainer = PRMTrainer(mcts, cfg)\n",
    "\n",
    "train_q = [\"A box contains 8 red and 12 blue marbles. If John removes 5 red marbles and then 4 blue marbles, how many marbles remain in the box?\"]\n",
    "val_q = [\"Solve for y: 2y - 7 = 3(y - 4).\"]\n",
    "tr_ds, tr_st = trainer.build_dataset(train_q)\n",
    "print(tr_ds)\n",
    "print(\"MCTS train print tree\")\n",
    "mcts.print_tree(mcts.root)\n",
    "\n",
    "val_ds, val_st = trainer.build_dataset(val_q)\n",
    "print(val_ds)\n",
    "print(\"MCTS val print tree\")\n",
    "mcts.print_tree(mcts.root)\n",
    "\n",
    "train_loader = DataLoader(tr_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "tr = trainer._run_epoch(train_loader, train=True)\n",
    "vl = trainer._run_epoch(val_loader,   train=False)\n",
    "print(f\"[EP {1}] train {tr:.4f} | val {vl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.97it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleena12\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/leena/ccc_eval/wandb/run-20250619_112145-vni9no29</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leena12/omega-prm/runs/vni9no29' target=\"_blank\">prm-train</a></strong> to <a href='https://wandb.ai/leena12/omega-prm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leena12/omega-prm' target=\"_blank\">https://wandb.ai/leena12/omega-prm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leena12/omega-prm/runs/vni9no29' target=\"_blank\">https://wandb.ai/leena12/omega-prm/runs/vni9no29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data:   0%|          | 0/1 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] successes: 3/5, mc=0.60, Q=0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "MCTS: 100%|██████████| 2/2 [00:18<00:00,  9.16s/it]\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] successes: 1/5, mc=0.20, Q=0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data: 100%|██████████| 1/1 [00:19<00:00, 19.07s/it]\n",
      "Collecting MCTS data:   0%|          | 0/1 [00:00<?, ?it/s]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] successes: 2/5, mc=0.40, Q=0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "MCTS: 100%|██████████| 2/2 [02:25<00:00, 72.54s/it]\n",
      "Both `max_new_tokens` (=384) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rollout] successes: 0/5, mc=0.00, Q=0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting MCTS data: 100%|██████████| 1/1 [02:34<00:00, 154.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds: No dataset is collected.\n",
      "train loader: <torch.utils.data.dataloader.DataLoader object at 0x7de21c615010>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPRM metrics:\u001b[39m\u001b[33m\"\u001b[39m,  trainer.get_metrics())\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     20\u001b[39m train_q = [\u001b[33m\"\u001b[39m\u001b[33mWhat is 2 + 2?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m val_q   = [\u001b[33m\"\u001b[39m\u001b[33mWhat is (5+7)/2 - 3?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_prm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMCTS print tree\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m mcts.print_tree(mcts.root)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mPRMTrainer.train_prm\u001b[39m\u001b[34m(self, train_questions, val_questions, num_epochs)\u001b[39m\n\u001b[32m    174\u001b[39m best = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     vl = \u001b[38;5;28mself\u001b[39m._run_epoch(val_loader,   train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    178\u001b[39m     hist[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append(tr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mPRMTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, train)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.prm.train() \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prm.eval()\n\u001b[32m    135\u001b[39m tot = \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, (ids, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[32m    137\u001b[39m     ids, r = ids.to(\u001b[38;5;28mself\u001b[39m.device), r.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(train):\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cfg = OmegaPRMConfig(\n",
    "        use_wandb=True,         # 예시이므로 off\n",
    "        batch_size=12,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    golden = {\n",
    "        \"What is 2 + 2?\": \"4\",\n",
    "        \"What is (5+7)/2 - 3?\": \"3\",\n",
    "    }\n",
    "\n",
    "    # 1) MCTS 초기화\n",
    "    mcts = MCTS(cfg, golden)\n",
    "\n",
    "    # 2) PRM trainer\n",
    "    trainer = PRMTrainer(mcts, cfg)\n",
    "\n",
    "    # 3) 학습 파이프라인\n",
    "    train_q = [\"What is 2 + 2?\"]\n",
    "    val_q   = [\"What is (5+7)/2 - 3?\"]\n",
    "    trainer.train_prm(train_q, val_q, num_epochs=1)\n",
    "\n",
    "    print(\"MCTS print tree\")\n",
    "    mcts.print_tree(mcts.root)\n",
    "\n",
    "    # (선택) MCTS 메트릭·결과 저장\n",
    "    mcts.export_results(\"results.json\")\n",
    "    print(\"MCTS metrics:\", mcts.get_metrics())\n",
    "    print(\"PRM metrics:\",  trainer.get_metrics())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── main_gsm8k.py ──────────────────────────────────────────────────────────\n",
    "from datasets import load_dataset\n",
    "# from omega_cfg import OmegaPRMConfig\n",
    "# from mcts_module import MCTS                # 이미 작성된 클래스 import\n",
    "# from trainer_prm import PRMTrainer          # 개선된 trainer import\n",
    "# from dataset_prm import PRMDataset\n",
    "import random, json\n",
    "\n",
    "def build_golden_from_gsm(split=\"train\", n_samples=2000):\n",
    "    ds = load_dataset(\"gsm8k\", \"main\", split=split)\n",
    "    rows = random.sample(list(ds), n_samples)\n",
    "    # GSM8K 레코드: {\"question\": \"...\", \"answer\": \"#### 42\"} (42 가 정답)\n",
    "    golden = {}\n",
    "    for r in rows:\n",
    "        # answer 형식 \"#### 42\" → \"42\" 만 추출\n",
    "        ans = r[\"answer\"].split(\"####\")[-1].strip()\n",
    "        golden[r[\"question\"]] = ans\n",
    "    print(len(golden))\n",
    "    return golden\n",
    "\n",
    "def main():\n",
    "    cfg = OmegaPRMConfig(\n",
    "        use_wandb=True,\n",
    "        batch_size=16,\n",
    "        num_workers=4,\n",
    "        max_length=256,\n",
    "        rollout_width=10,            # GSM8K는 계산이 복잡하므로 살짝 축소\n",
    "    )\n",
    "\n",
    "    golden = build_golden_from_gsm(split=\"train\", n_samples=500)\n",
    "    questions = list(golden.keys())\n",
    "    random.shuffle(questions)\n",
    "    train_q, val_q = questions[:400], questions[400:450]\n",
    "\n",
    "    mcts = MCTS(cfg, golden)\n",
    "    trainer = PRMTrainer(mcts, cfg)\n",
    "\n",
    "    trainer.train_prm(train_q, val_q, num_epochs=3)\n",
    "\n",
    "    # 저장\n",
    "    mcts.export_results(\"gsm_metrics.json\")\n",
    "    with open(\"gsm_prm_stats.json\", \"w\") as f:\n",
    "        json.dump(trainer.get_metrics(), f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/peer/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    max_new_tokens: int = 386\n",
    "    num_rollouts: int = 5\n",
    "    reward_threshold: float = 0.2\n",
    "    samples_per_question: int = 2\n",
    "    # PRMTrainer config\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 5e-4\n",
    "    hidden_size: int = 256\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 2\n",
    "    # Misc config\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"mc_prm\"\n",
    "    run_name: str = \"test_0623\"\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                        UTILITY: ANSWER NORMALISATION                         #\n",
    "################################################################################\n",
    "import sympy as sp\n",
    "\n",
    "def _strip_markup(ans: str) -> str:\n",
    "    \"\"\"Remove common LaTeX/markup & variable tags.\"\"\"\n",
    "    # # Remove LaTeX inline math wrappers \\( … \\) or \\[ … \\]\n",
    "    # ans = re.sub(r\"\\\\[\\[(](.*?)[\\\\\\])]\", r\"\\1\", ans)\n",
    "    # # Remove \\boxed{…}\n",
    "    # ans = re.sub(r\"\\\\boxed\\{([^}]*)\\}\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\\\\\[.*?\\\\\\]\", \"\", ans)\n",
    "    ans = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", ans)\n",
    "    # Remove inline LaTeX: \\( ... \\) and $...$\n",
    "    ans = re.sub(r\"\\\\\\((.*?)\\\\\\)\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\$(.*?)\\$\", r\"\\1\", ans)\n",
    "    # Remove \\boxed{...}\n",
    "    ans = re.sub(r\"\\\\boxed\\s*{([^}]*)}\", r\"\\1\", ans)\n",
    "    # Remove LaTeX commands like \\text{...}, \\frac{...}, etc.\n",
    "    ans = re.sub(r\"\\\\[a-zA-Z]+\\s*(\\{[^{}]*\\})?\", \"\", ans)\n",
    "    # Remove variable assignments like \"y =\" or \"x=\" at start\n",
    "    ans = re.sub(r\"^[a-zA-Z]\\s*=\\s*\", \"\", ans)\n",
    "    # Trim outer $ … $ if present\n",
    "    ans = ans.strip()\n",
    "    if ans.startswith(\"$\") and ans.endswith(\"$\"):\n",
    "        ans = ans[1:-1]\n",
    "    return ans.strip()\n",
    "\n",
    "def _sanitize(text: str) -> str:\n",
    "    \"\"\"Normalise a candidate answer string for comparison.\"\"\"\n",
    "    text = _strip_markup(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\s\\.;:,]+$\", \"\", text)     # trailing punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)              # collapse spaces\n",
    "    return text\n",
    "\n",
    "def _to_float(expr: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(eval(expr.replace(\"^\", \"**\")))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _numeric_equiv(a: str, b: str) -> bool:\n",
    "    \"\"\"Return True if `a` and `b` are numerically equivalent or exact match.\"\"\"\n",
    "    a_clean, b_clean = map(_sanitize, (a, b))\n",
    "    if a_clean == b_clean:\n",
    "        return True\n",
    "\n",
    "    # Attempt simple numeric evaluation\n",
    "    a_val, b_val = _to_float(a_clean), _to_float(b_clean)\n",
    "    if a_val is not None and b_val is not None:\n",
    "        return math.isclose(a_val, b_val, rel_tol=1e-6)\n",
    "\n",
    "    if sp is not None:\n",
    "        try:\n",
    "            a_expr = sp.sympify(a_clean.replace(\"^\", \"**\"))\n",
    "            b_expr = sp.sympify(b_clean.replace(\"^\", \"**\"))\n",
    "            return sp.simplify(a_expr - b_expr) == 0\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def system_prompt(type):\n",
    "    prompt = \"\"\n",
    "    if type == \"sample\":\n",
    "        prompt = \"\"\"You are a math-problem expert. Your task is to complete the step-by-step solution for the problem provided. Write each reasoning step on its own line in the exact form \\\"Step k: [your reasoning step]\\n\\\", numbering start from Step 1. When the final answer is obtained, write exactly one final line, \\\"Answer: [Final answer]\\\". Do NOT add explanations, extra steps, or any text after the \"Answer:\" line.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Follow the FORMAT GUIDE structure exactly. Generate rationales step-by-step, not directly to the final answer. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    if type == \"rollout\":\n",
    "        prompt = \"\"\"You are a math problem-solving expert. Continue solving the given problem step by step, strictly following the required format. Each new step must begin with \\\"Step k+1: ...\\\", \\\"Step k+2:...\\\", and so on, continuing from the last given step number. When the final answer is reached, write only one final line starting with: \\\"Answer: [Final Answer]\\\". Do not add any explanations, extra commentary, or additional text after the \"Answer:\" line. Your output must follow this exact step-by-step format with no deviations.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "Continue and finish the solution:\n",
    "Step k+1: [Step k+1 reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Current solution steps:\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Continue and finish the solution:\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Current solution steps:\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Continue and finish the solution:\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Keep the reasoning steps precise and factual and complete the solution. Follow the FORMAT GUIDE structure exactly. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class MCReward:\n",
    "    # STEP_PATTERN = re.compile(r\"Step\\s+\\d+:\")\n",
    "    # ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*(?:$|\\n)\")\n",
    "    STEP_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Step\\s*              # word 'Step' (case-insensitive)\n",
    "        (\\d+)                # capture step number\n",
    "        \\s*[:.\\-]            # separator (: . or -)\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    ANSWER_PATTERN = re.compile(\n",
    "        r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "            Answer               # word 'Answer'\n",
    "            \\s*[:.\\-]\\s*         # separator\n",
    "            (.+?)\\s*$            # capture everything after\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def generate_solutions(self, question: str, sys_prompt: str, num_solutions: int):\n",
    "        prompt = f\"{sys_prompt}\\n\\n{question}\\n\"  # Prompt the model to start the step-by-step solution\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.model.device)\n",
    "        # Generate multiple solutions via sampling\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_solutions,\n",
    "            temperature=0.8,         # sampling temperature for diversity (adjust as needed)\n",
    "            top_p=0.8,               # top-p sampling for diversity\n",
    "            pad_token_id=self.tokenizer.eos_token_id  # pad token ID to avoid warning for some models\n",
    "        )\n",
    "        solutions = []\n",
    "        prompt_len = input_ids.shape[-1]\n",
    "        for i in range(num_solutions):\n",
    "            # Each output is the concatenation of the prompt and the generated completion.\n",
    "            generated_ids = outputs[i]\n",
    "            # Extract only the newly generated tokens (skip the prompt tokens).\n",
    "            gen_ids = generated_ids[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            solutions.append(text)\n",
    "            print(f\"{i}-th Sampled Solutions:\",text)\n",
    "        return solutions\n",
    "    \n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"\n",
    "        Split each step to start with 'Step X:' and the answer to start with 'Answer:'.\n",
    "        \"\"\"\n",
    "        steps = []\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # if line.startswith(\"Step\"):\n",
    "            #     steps.append(line)\n",
    "            if self.STEP_PATTERN.match(line):\n",
    "                cleaned = re.sub(r'^[\\s>#*\\-]+', '', line)\n",
    "                steps.append(cleaned)\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(steps[: i + 1]) + \"\\n\", return_tensors=\"pt\").to(self.device) # steps up to current step i (0-indexed)\n",
    "            # Decide how to prompt the next part:\n",
    "            if i < total_steps - 1:\n",
    "                next_label = f\"Step {i + 2}:\"\n",
    "            else:\n",
    "                next_label = \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            # Build full prefix ids (avoid Python concat inefficiency by cat)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "\n",
    "            # prefix_ids = self.tokenizer.encode(prefix_text, return_tensors='pt').to(self.model.device)\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1] \n",
    "            # Check each rollout's final answer against the gold answer\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                # if i == total_steps-1:\n",
    "                print(f\"[{i+1}-th Step, {idx}-th Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            reward = correct_count / float(self.config.num_rollouts)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets(self, problems: List):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        for problem in problems:\n",
    "            question = problem[\"question\"]\n",
    "            # gold_answer = problem[\"gold_answer\"]\n",
    "            gold_answer = _sanitize(problem[\"gold_answer\"])\n",
    "            # Generate one or more solutions for this question\n",
    "            sample_prompt = system_prompt(\"sample\")\n",
    "            rollout_prompt = system_prompt(\"rollout\")\n",
    "            solutions = self.generate_solutions(question, sys_prompt=sample_prompt, num_solutions=self.config.samples_per_question)\n",
    "            for sol_text in solutions:\n",
    "                steps, answer = self.parse_solution(sol_text)\n",
    "                # print(\"Parsed solution:\", steps, answer)\n",
    "                if answer is None: # If no answer was found in the solution (edge case), skip this solution\n",
    "                    continue\n",
    "                # Compute intermediate rewards for each step in this solution\n",
    "                rewards = self.compute_step_rewards(question, sys_prompt=rollout_prompt, steps=steps, gold_answer=gold_answer)\n",
    "                # print(steps, \"\\n\", rewards)\n",
    "                # Prepare the output entry\n",
    "                entry = {\n",
    "                    \"question\": question,\n",
    "                    \"completion\": steps,      # list of \"Step i: ...\" strings\n",
    "                    \"rewards\": rewards,       # list of reward values for each step\n",
    "                    \"answer\": answer,         # model's final answer from this solution\n",
    "                    \"gold_answer\": gold_answer\n",
    "                }\n",
    "                dataset.append(entry)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class StepwisePRMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    build_datasets() 가 반환한 entries(list[dict])를\n",
    "    (input_ids, scalar_reward) 샘플들로 변환한다.\n",
    "\n",
    "    한 entry = {question, completion[steps], rewards[float], …}\n",
    "    →  (Problem + Step1,   r1)\n",
    "        (Problem + Step1 \\nStep2,   r2) …\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        *,\n",
    "        cache_encodings: bool = True,\n",
    "        preprocess: bool = True,\n",
    "    ):\n",
    "        self.tokenizer   = tokenizer\n",
    "        self.max_length  = max_length\n",
    "        self.cache       = {} if cache_encodings else None\n",
    "        self.samples: List[Tuple[str, float]] = []\n",
    "\n",
    "        for e in entries:\n",
    "            q_txt   = e[\"question\"]\n",
    "            steps   = e[\"completion\"]\n",
    "            rewards = e[\"rewards\"]\n",
    "            assert len(steps) == len(rewards)\n",
    "\n",
    "            prefix_lines = [f\"Problem: {q_txt}\"]\n",
    "            for step_txt, r in zip(steps, rewards):\n",
    "                prefix_lines.append(step_txt)\n",
    "                full_txt = \"\\n\".join(prefix_lines)\n",
    "                if preprocess:\n",
    "                    full_txt = self._clean(full_txt)\n",
    "                self.samples.append((full_txt, float(r)))   # (text, reward)\n",
    "\n",
    "    # --------------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(txt: str) -> str:\n",
    "        \"\"\"whitespace normalize + 소문자화(선택적) 등 간단 전처리\"\"\"\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return txt\n",
    "\n",
    "    # --------------------------------------------------------------------- dunder\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, reward = self.samples[idx]\n",
    "\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            ids = self.cache[text]\n",
    "        else:\n",
    "            ids = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[text] = ids\n",
    "\n",
    "        return ids, torch.tensor(reward, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        num_layers: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            hidden_size (int): Size of hidden layers\n",
    "            output_size (int): Size of output\n",
    "            dropout (float): Dropout rate\n",
    "            num_layers (Optional[int]): Number of hidden layers\n",
    "        \"\"\"\n",
    "        super(ProcessRewardModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers or 2\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_layers = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            in_features = hidden_size if i == 0 else hidden_size // (2 ** i)\n",
    "            out_features = hidden_size // (2 ** (i + 1))\n",
    "            hidden_layers.extend([\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.LayerNorm(out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        last_hidden_size = hidden_size // (2 ** (self.num_layers - 1))\n",
    "        self.fc_out = nn.Linear(last_hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input layer\n",
    "        x = self.dropout(torch.relu(self.ln1(self.fc1(x))))\n",
    "        # Hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        # Output layer\n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        return x\n",
    "    \n",
    "    def get_complexity(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('omega_prm.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    (1) entries(list[dict]) → StepwisePRMDataset\n",
    "    (2) LLM encoder + PRM head fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PRMConfig, model, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "        # ----------------------------- Backbone model LLM (frozen or fine-tuned)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model  = model\n",
    "        self.model.eval()       # LLM은 feature extractor로 freeze\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        feat_dim = self.model.config.hidden_size\n",
    "        self.prm = ProcessRewardModel(feat_dim, hidden_size=cfg.hidden_size, output_size=1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.prm.to(self.device)\n",
    "\n",
    "        self.opt  = optim.AdamW(self.prm.parameters(), lr=cfg.learning_rate)\n",
    "        # self.crit = nn.MSELoss()\n",
    "        self.crit = nn.BCELoss()\n",
    "\n",
    "        Path(cfg.checkpoint_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.wandb_run = None\n",
    "        if cfg.use_wandb:                                  # <-- config에 플래그\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=cfg.wandb_project,                 # e.g. \"omega-prm\"\n",
    "                name=cfg.run_name,                         # e.g. \"qwen7b-prm\"\n",
    "                config=vars(cfg),                          # 모든 하이퍼파라미터 로깅\n",
    "                # reinit=True,\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------- features\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\n",
    "        \"\"\"\n",
    "        out = self.model(\n",
    "            input_ids=ids,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        return out.hidden_states[-1][:, 0, :]     # CLS embedding\n",
    "\n",
    "    # ----------------------------------------------------------------- loop util\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool, epoch_idx: int) -> float:\n",
    "        self.prm.train(train)\n",
    "        total = 0.0\n",
    "        for step, (ids, reward) in enumerate(loader):\n",
    "            ids, reward = ids.to(self.device), reward.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats  = self._encode(ids)\n",
    "                pred   = self.prm(feats).squeeze(-1)\n",
    "                loss   = self.crit(pred, reward)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "            # -------- minibatch logging --------\n",
    "            if self.wandb_run and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"epoch\": epoch_idx + step / len(loader),\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                    \"grad_norm\": sum(p.grad.data.norm(2).item()\n",
    "                                     for p in self.prm.parameters()\n",
    "                                     if p.grad is not None),\n",
    "                })\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ----------------------------------------------------------------- public\n",
    "    def fit(self, train_entries: List[dict], val_entries: List[dict]) -> Dict[str, List[float]]:\n",
    "        train_ds = StepwisePRMDataset(train_entries, self.tokenizer, self.cfg.max_new_tokens)\n",
    "        val_ds   = StepwisePRMDataset(val_entries,   self.tokenizer, self.cfg.max_new_tokens)\n",
    "        print(\"Train Dataset Example:\", train_ds[0], len(train_ds))\n",
    "        print(\"Validation Dataset Example:\", val_ds[0], len(val_ds))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "\n",
    "        history = {\"train\": [], \"val\": []}\n",
    "        best_val = float(\"inf\")\n",
    "\n",
    "        for ep in range(self.cfg.epochs):\n",
    "            tr_loss = self._run_epoch(train_loader, train=True,  epoch_idx=ep)\n",
    "            vl_loss = self._run_epoch(val_loader,   train=False, epoch_idx=ep)\n",
    "\n",
    "            # -------- epoch logging --------\n",
    "            if self.wandb_run:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": tr_loss,\n",
    "                    \"val_loss\": vl_loss,\n",
    "                    \"epoch\": ep,\n",
    "                })\n",
    "\n",
    "            history[\"train\"].append(tr_loss)\n",
    "            history[\"val\"].append(vl_loss)\n",
    "            print(f\"[Epoch {ep+1}/{self.cfg.epochs}] train={tr_loss:.4f}  val={vl_loss:.4f}\")\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if vl_loss < best_val:\n",
    "                best_val = vl_loss\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": ep,\n",
    "                        \"prm_state\": self.prm.state_dict(),\n",
    "                        \"val_loss\": vl_loss,\n",
    "                    },\n",
    "                    Path(self.cfg.checkpoint_dir) / \"best_prm.pt\",\n",
    "                )\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load model and config!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-7B\"      # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "cfg = PRMConfig()\n",
    "print(\"Finish load model and config!\")\n",
    "\n",
    "problems = [\n",
    "    {\"question\": \"Each notebook costs $5. Sarah buys 4 notebooks and pays with a $50 bill. How much change does she get?\", \"gold_answer\": \"30\"},\n",
    "    {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "    # Add more problems as needed...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th Sampled Solutions: To determine how much change Sarah gets, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total cost of the notebooks.\n",
      "2. Subtract the total cost from the amount of money Sarah paid.\n",
      "\n",
      "First, let's find the total cost of the notebooks. Each notebook costs $5 and Sarah buys 4 notebooks. So, the total cost is:\n",
      "\\[ 5 \\text{ dollars/notebook} \\times 4 \\text{ notebooks} = 20 \\text{ dollars} \\]\n",
      "\n",
      "Next, we subtract the total cost from the amount of money Sarah paid. Sarah paid with a $50 bill, so the change she gets is:\n",
      "\\[ 50 \\text{ dollars} - 20 \\text{ dollars} = 30 \\text{ dollars} \\]\n",
      "\n",
      "Therefore, the change Sarah gets is \\(\\boxed{30}\\).\n",
      "\n",
      "Find the quotient when the polynomial $-\\frac{25 x^5}{3}+\\frac{19 x^4}{3}+\\frac{16 x^3}{3}-\\frac{20 x^2}{3}+\\frac{25 x}{3}+\\frac{10}{3}$ is divided by $\\frac{17 x^4}{3}+\\frac{28 x^3}{3}-\\frac{28 x^2}{3}-\\frac{22 x}{3}+\\frac{23}{3}$.\n",
      "To find the quotient when the polynomial \\( P(x) = -\\frac{25 x^5}{3} + \\frac{19 x^4}{3} + \\frac{16 x^3}{3} - \\frac{20 x^2}{3} + \\frac{25 x}{3} + \\frac{10}{3} \\) is divided by \\( D(x)\n",
      "0-th Sampled Solutions: Step 1: Expand the right side of the equation using the distributive property.\n",
      "Step 2: Simplify both sides of the equation.\n",
      "Step 3: Isolate the variable term by subtracting 3y from both sides.\n",
      "Step 4: Isolate the variable by adding 7 to both sides.\n",
      "Step 5: Divide both sides by -1 to solve for y.\n",
      "\n",
      "Answer: y = -5\n",
      "[0-th Step, 0-th Rollout]  Simplify the equation by combining like terms.\n",
      "Step 3: Isolate the variable y by subtracting 2y from both sides and adding 7 to both sides.\n",
      "Step 4: Divide both sides by the coefficient of y to solve for y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[0-th Step, 1-th Rollout]  Distribute the 3 on the right side.\n",
      "Step 3: Simplify the equation by combining like terms.\n",
      "Step 4: Isolate the variable y on one side of the equation.\n",
      "Step 5: Add 7 to both sides of the equation.\n",
      "Step 6: Simplify the equation.\n",
      "Step 7: Divide both sides of the equation by 1.\n",
      "Step 8: Simplify the equation to get the final answer.\n",
      "Answer: y = 11 Pred Answer 11\n",
      "[0-th Step, 2-th Rollout]  Simplify the equation by combining like terms.\n",
      "Step 3: Isolate the variable y on one side of the equation.\n",
      "Step 4: Solve for y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[0-th Step, 3-th Rollout]  Simplify the equation by combining like terms.\n",
      "Step 3: Isolate the variable term by subtracting 3y from both sides.\n",
      "Step 4: Simplify the equation further.\n",
      "Step 5: Isolate the constant term by adding 7 to both sides.\n",
      "Step 6: Solve for y by dividing both sides by the coefficient of y.\n",
      "Answer: y = -5 Pred Answer -5\n",
      "[0-th Step, 4-th Rollout]  Simplify the equation by combining like terms.\n",
      "Step 3: Isolate the variable y on one side of the equation.\n",
      "Step 4: Solve for y by dividing both sides by the coefficient of y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[1-th Step, 0-th Rollout]  Isolate the variable y by moving terms.\n",
      "Step 4: Solve for y.\n",
      "Step 5: Check the solution by substituting it back into the original equation.\n",
      "\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[1-th Step, 1-th Rollout]  Isolate the variable y by subtracting 3y from both sides.\n",
      "Step 4: Simplify the equation.\n",
      "Step 5: Divide both sides by -1 to solve for y.\n",
      "Step 6: Simplify the fraction.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[1-th Step, 2-th Rollout]  Isolate the variable y.\n",
      "Step 4: Solve for y.\n",
      "\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[1-th Step, 3-th Rollout]  Isolate the variable term by adding 7 to both sides.\n",
      "Step 4: Isolate the variable y by dividing both sides by the coefficient of y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[1-th Step, 4-th Rollout]  Isolate the variable term on one side.\n",
      "Step 4: Solve for y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[2-th Step, 0-th Rollout]  Combine like terms.\n",
      "Step 5: Isolate the variable y by adding 7 to both sides.\n",
      "Step 6: Simplify to find the value of y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[2-th Step, 1-th Rollout]  Isolate the constant term by adding 7 to both sides.\n",
      "Step 5: Divide both sides by -1 to solve for y.\n",
      "Answer: -1 Pred Answer -1\n",
      "[2-th Step, 2-th Rollout]  Simplify the equation.\n",
      "Step 5: Solve for y by dividing both sides by -1.\n",
      "Step 6: Write the final answer.\n",
      "Answer: y = -5 Pred Answer -5\n",
      "[2-th Step, 3-th Rollout]  Combine like terms.\n",
      "Step 5: Solve for y by dividing both sides by -1.\n",
      "\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[2-th Step, 4-th Rollout]  Isolate the constant term by adding 7 to both sides.\n",
      "Step 5: Divide both sides by the coefficient of y to solve for y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[3-th Step, 0-th Rollout]  Solve for y by dividing both sides by -1.\n",
      "Answer: y = -1. Pred Answer -1\n",
      "[3-th Step, 1-th Rollout]  Divide both sides by the coefficient of y.\n",
      "Answer: y = 5 Pred Answer 5\n",
      "[3-th Step, 2-th Rollout]  Divide both sides by -1 to solve for y.\n",
      "Answer: y = -5 Pred Answer -5\n",
      "[3-th Step, 3-th Rollout]  Solve for y by dividing both sides by -1.\n",
      "\n",
      "Answer: y = -1. Pred Answer -1\n",
      "[3-th Step, 4-th Rollout]  Solve for y by dividing both sides by -1.\n",
      "Answer: y = -1\n",
      "\n",
      "Step 1: Expand the right side of the equation using the distributive property.\n",
      "Step 2: Simplify both sides of the equation.\n",
      "Step 3: Isolate the variable term by subtracting 3y from both sides.\n",
      "Step 4: Isolate the variable by adding 7 to both sides.\n",
      "Step 5: Solve for y by dividing both sides by -1.\n",
      "Answer: y = -1 Pred Answer -1\n",
      "[4-th Step, 0-th Rollout]  y = 5 Pred Answer 5\n",
      "[4-th Step, 1-th Rollout]  y = -5 Pred Answer -5\n",
      "[4-th Step, 2-th Rollout]  y = -5 Pred Answer -5\n",
      "[4-th Step, 3-th Rollout]  y = 1 Pred Answer 1\n",
      "[4-th Step, 4-th Rollout]  y = 5 Pred Answer 5\n",
      "{'question': 'Each notebook costs $5. Sarah buys 4 notebooks and pays with a $50 bill. How much change does she get?', 'completion': [], 'rewards': [], 'answer': 'To find the quotient when the polynomial P(x) = -{3} + {3} + {3} - {3} + {3} + {3} is divided by \\\\( D(x)', 'gold_answer': '30'}\n",
      "--------------------------------------------------------------------------------\n",
      "{'question': 'Solve for y: 2y - 7 = 3(y - 4).', 'completion': ['Step 1: Expand the right side of the equation using the distributive property.', 'Step 2: Simplify both sides of the equation.', 'Step 3: Isolate the variable term by subtracting 3y from both sides.', 'Step 4: Isolate the variable by adding 7 to both sides.', 'Step 5: Divide both sides by -1 to solve for y.'], 'rewards': [0.6, 1.0, 0.6, 0.2, 0.4], 'answer': '-5', 'gold_answer': '5'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_0623</strong> at: <a href='https://wandb.ai/leena12/mc_prm/runs/yrh74day' target=\"_blank\">https://wandb.ai/leena12/mc_prm/runs/yrh74day</a><br> View project at: <a href='https://wandb.ai/leena12/mc_prm' target=\"_blank\">https://wandb.ai/leena12/mc_prm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250623_162437-yrh74day/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/leena/ccc_eval/mcts_prm/PRMDataset/wandb/run-20250623_162837-y83zv2ry</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leena12/mc_prm/runs/y83zv2ry' target=\"_blank\">test_0623</a></strong> to <a href='https://wandb.ai/leena12/mc_prm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leena12/mc_prm' target=\"_blank\">https://wandb.ai/leena12/mc_prm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leena12/mc_prm/runs/y83zv2ry' target=\"_blank\">https://wandb.ai/leena12/mc_prm/runs/y83zv2ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m val_entries     = entries_raw[split_idx:] \u001b[38;5;129;01mor\u001b[39;00m entries_raw[:\u001b[32m1\u001b[39m]   \u001b[38;5;66;03m# 최소 1개 확보\u001b[39;00m\n\u001b[32m     15\u001b[39m trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_entries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Loss history:\u001b[39m\u001b[33m\"\u001b[39m, history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mPRMTrainer.fit\u001b[39m\u001b[34m(self, train_entries, val_entries)\u001b[39m\n\u001b[32m    105\u001b[39m train_ds = StepwisePRMDataset(train_entries, \u001b[38;5;28mself\u001b[39m.tokenizer, \u001b[38;5;28mself\u001b[39m.cfg.max_new_tokens)\n\u001b[32m    106\u001b[39m val_ds   = StepwisePRMDataset(val_entries,   \u001b[38;5;28mself\u001b[39m.tokenizer, \u001b[38;5;28mself\u001b[39m.cfg.max_new_tokens)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain Dataset Example:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_ds))\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation Dataset Example:\u001b[39m\u001b[33m\"\u001b[39m, val_ds[\u001b[32m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(val_ds))\n\u001b[32m    110\u001b[39m train_loader = DataLoader(\n\u001b[32m    111\u001b[39m     train_ds, batch_size=\u001b[38;5;28mself\u001b[39m.cfg.batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    112\u001b[39m     num_workers=\u001b[38;5;28mself\u001b[39m.cfg.num_workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    113\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mStepwisePRMDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     text, reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache:\n\u001b[32m     60\u001b[39m         ids = \u001b[38;5;28mself\u001b[39m.cache[text]\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "# Print or inspect the dataset\n",
    "for entry in entries_raw:\n",
    "    print(entry)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "random.shuffle(entries_raw)\n",
    "# split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "split_idx = 1\n",
    "train_entries   = entries_raw[:split_idx]\n",
    "val_entries     = entries_raw[split_idx:] or entries_raw[:1]   # 최소 1개 확보\n",
    "\n",
    "trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "history = trainer.fit(train_entries, val_entries)\n",
    "print(\"Training complete. Loss history:\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load model and config!\n",
      "0-th solutions To find the average speed of the train, we need to determine the total distance traveled and the total time taken, and then use the formula for average speed:\n",
      "\n",
      "\\[\n",
      "\\text{Average speed} = \\frac{\\text{Total distance}}{\\text{Total time}}\n",
      "\\]\n",
      "\n",
      "First, let's calculate the total distance traveled by the train. The train travels 120 km in the first part of the journey and 180 km in the second part. So, the total distance is:\n",
      "\n",
      "\\[\n",
      "120 \\text{ km} + 180 \\text{ km} = 300 \\text{ km}\n",
      "\\]\n",
      "\n",
      "Next, let's calculate the total time taken for the journey. The train takes 2 hours for the first part and 3 hours for the second part. So, the total time is:\n",
      "\n",
      "\\[\n",
      "2 \\text{ hours} + 3 \\text{ hours} = 5 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Now, we can find the average speed by dividing the total distance by the total time:\n",
      "\n",
      "\\[\n",
      "\\text{Average speed} = \\frac{300 \\text{ km}}{5 \\text{ hours}} = 60 \\text{ km/h}\n",
      "\\]\n",
      "\n",
      "Therefore, the average speed of the train is \\boxed{60} km/h.\n",
      "\n",
      "In a science experiment, a group of students is studying the behavior of a special type of gas called a Bose-Einstein Condensate (BEC). They use a simplified model where the gas is described by two types of particles: Type A and Type B. Initially, there are 50 Type A particles and 30 Type B particles in the container. During the experiment, the particles can transform into each other according to the following rule: for every 2 Type A particles that transform, 3 Type B particles are created. If the\n",
      "[] \n",
      " []\n",
      "0-th solutions Step 1: Distribute the 3 on the right side: 2y - 7 = 3y - 12.\n",
      "Step 2: Move all terms involving y to one side by subtracting 2y from both sides: -7 = y - 12.\n",
      "Step 3: Add 12 to both sides to isolate y: 5 = y.\n",
      "Answer: 5\n",
      "To solve the equation \\(2y - 7 = 3(y - 4)\\) for \\(y\\), we will follow a step-by-step approach:\n",
      "\n",
      "**Step 1: Distribute the 3 on the right side of the equation.**\n",
      "\\[2y - 7 = 3(y - 4)\\]\n",
      "\\[2y - 7 = 3y - 12\\]\n",
      "\n",
      "**Step 2: Move all terms involving \\(y\\) to one side of the equation by subtracting \\(2y\\) from both sides.**\n",
      "\\[2y - 7 - 2y = 3y - 12 - 2y\\]\n",
      "\\[-7 = y - 12\\]\n",
      "\n",
      "**Step 3: Add 12 to both sides to isolate \\(y\\).**\n",
      "\\[-7 + 12 = y - 12 + 12\\]\n",
      "\\[5 = y\\]\n",
      "\n",
      "**Step 4: Write the final answer.**\n",
      "\\[y = 5\\]\n",
      "\n",
      "So, the solution is \\(\\boxed{5}\\).\n",
      "['Step 1: Distribute the 3 on the right side: 2y - 7 = 3y - 12.', 'Step 2: Move all terms involving y to one side by subtracting 2y from both sides: -7 = y - 12.', 'Step 3: Add 12 to both sides to isolate y: 5 = y.', 'Step 1: Distribute the 3 on the right side of the equation.**', 'Step 2: Move all terms involving \\\\(y\\\\) to one side of the equation by subtracting \\\\(2y\\\\) from both sides.**', 'Step 3: Add 12 to both sides to isolate \\\\(y\\\\).**', 'Step 4: Write the final answer.**'] \n",
      " [0.75, 0.75, 0.5, 0.5, 0.125, 0.0, 0.0]\n",
      "{'question': 'A train travels 120 km in 2 hours and then 180 km in 3 hours. What is the average speed of the train?', 'completion': [], 'rewards': [], 'answer': 'In a science experiment, a group of students is studying the behavior of a special type of gas called a Bose-Einstein Condensate (BEC). They use a simplified model where the gas is described by two types of particles: Type A and Type B. Initially, there are 50 Type A particles and 30 Type B particles in the container. During the experiment, the particles can transform into each other according to the following rule: for every 2 Type A particles that transform, 3 Type B particles are created. If the', 'gold_answer': '60'}\n",
      "--------------------------------------------------------------------------------\n",
      "{'question': 'Solve for y: 2y - 7 = 3(y - 4).', 'completion': ['Step 1: Distribute the 3 on the right side: 2y - 7 = 3y - 12.', 'Step 2: Move all terms involving y to one side by subtracting 2y from both sides: -7 = y - 12.', 'Step 3: Add 12 to both sides to isolate y: 5 = y.', 'Step 1: Distribute the 3 on the right side of the equation.**', 'Step 2: Move all terms involving \\\\(y\\\\) to one side of the equation by subtracting \\\\(2y\\\\) from both sides.**', 'Step 3: Add 12 to both sides to isolate \\\\(y\\\\).**', 'Step 4: Write the final answer.**'], 'rewards': [0.75, 0.75, 0.5, 0.5, 0.125, 0.0, 0.0], 'answer': '5', 'gold_answer': '5'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PRMTrainer' object has no attribute 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Loss history:\u001b[39m\u001b[33m\"\u001b[39m, history)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m val_entries     = entries_raw[split_idx:] \u001b[38;5;129;01mor\u001b[39;00m entries_raw[:\u001b[32m1\u001b[39m]   \u001b[38;5;66;03m# 최소 1개 확보\u001b[39;00m\n\u001b[32m     31\u001b[39m trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_entries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Loss history:\u001b[39m\u001b[33m\"\u001b[39m, history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mPRMTrainer.fit\u001b[39m\u001b[34m(self, train_entries, val_entries)\u001b[39m\n\u001b[32m     97\u001b[39m best_val = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.cfg.epochs):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     tr_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     vl_loss = \u001b[38;5;28mself\u001b[39m._run_epoch(val_loader,   train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    103\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append(tr_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mPRMTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, train)\u001b[39m\n\u001b[32m     67\u001b[39m ids, reward = ids.to(\u001b[38;5;28mself\u001b[39m.device), reward.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(train):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     feats  = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     pred   = \u001b[38;5;28mself\u001b[39m.prm(feats).squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     72\u001b[39m     loss   = \u001b[38;5;28mself\u001b[39m.crit(pred, reward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/peer/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mPRMTrainer._encode\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids: torch.Tensor) -> torch.Tensor:\n\u001b[32m     52\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m(\n\u001b[32m     56\u001b[39m         input_ids=ids,\n\u001b[32m     57\u001b[39m         return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     58\u001b[39m         output_hidden_states=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     59\u001b[39m     )\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.hidden_states[-\u001b[32m1\u001b[39m][:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[31mAttributeError\u001b[39m: 'PRMTrainer' object has no attribute 'backbone'"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    model_name =  \"Qwen/Qwen2.5-Math-7B\" # \"Qwen/Qwen2.5-Math-7B-Instruct\"  #\"Qwen/Qwen2.5-Math-7B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    cfg = PRMConfig()\n",
    "    print(\"Finish load model and config!\")\n",
    "\n",
    "    problems = [\n",
    "        {\"question\": \"A train travels 120 km in 2 hours and then 180 km in 3 hours. What is the average speed of the train?\", \"gold_answer\": \"60\"},\n",
    "        {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "        # Add more problems as needed...\n",
    "    ]\n",
    "    \n",
    "    mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "    entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "    # Print or inspect the dataset\n",
    "    for entry in entries_raw:\n",
    "        print(entry)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    random.shuffle(entries_raw)\n",
    "    # split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "    split_idx = 1\n",
    "    train_entries   = entries_raw[:split_idx]\n",
    "    val_entries     = entries_raw[split_idx:] or entries_raw[:1]   # 최소 1개 확보\n",
    "\n",
    "    trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "    history = trainer.fit(train_entries, val_entries)\n",
    "    print(\"Training complete. Loss history:\", history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

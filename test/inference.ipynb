{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46e074b",
   "metadata": {},
   "source": [
    "# Import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cd026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a67d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    model_name:             str = \"Qwen/Qwen2.5-Math-7B\"    # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "    max_new_tokens:         int = 512\n",
    "    num_rollouts:           int = 8      \n",
    "    samples_per_question:   int = 1\n",
    "    use_llm:                bool = True  # Use llm for masking\n",
    "    reward_type:            str = \"contri\"  # ori, contri, mi, naive, norm\n",
    "    # PRM Model config \n",
    "    hidden_size:        int = 512      # 256-1024 범위에서 적절\n",
    "    num_layers:         int = 3        # 2-4 범위에서 적절\n",
    "    dropout:            float = 0.2    # 0.1-0.3 범위에서 적절\n",
    "    # PRMTrainer config \n",
    "    batch_size:         int = 16       # 12 → 16으로 증가 (더 안정적)\n",
    "    learning_rate:      float = 3e-4   # 5e-4 → 3e-4로 감소 (더 안정적)\n",
    "    num_workers:        int = 4        # 적절\n",
    "    weight_decay:       float = 1e-2   # 적절\n",
    "    lr_scheduler:       str = \"cosine\" # 적절\n",
    "    dataset_size:       int = 0\n",
    "    warmup_steps:       int = 40       # 22 → 50으로 증가 (더 안정적)\n",
    "    grad_clip:          float = 1.0    # 적절\n",
    "    epochs:             int = 20       # 25 → 15로 감소 (early stopping 고려)\n",
    "    # Misc config\n",
    "    use_wandb:          bool = True\n",
    "    wandb_project:      str = \"mc_prm\"\n",
    "    run_name:           str = \"test_400_0715\"\n",
    "    checkpoint_dir:     str = \"./checkpoints/0715/contri\"\n",
    "    seed:               int = 42\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(self, input_size: int, cfg: \"PRMConfig\"):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size : CLS-embedding dim of the frozen LLM backbone\n",
    "            cfg        : PRMConfig instance (hidden_size, num_layers, dropout …)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        # self.output_size = cfg.output_size\n",
    "        h = cfg.hidden_size\n",
    "        p_drop = cfg.dropout\n",
    "        n_layers = cfg.num_layers\n",
    "        act_fn     = nn.GELU()\n",
    "\n",
    "         # ── first projection ────────────────────────────────────────────\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, h),\n",
    "            nn.LayerNorm(h),\n",
    "            act_fn,\n",
    "            nn.Dropout(p_drop),\n",
    "        )\n",
    "\n",
    "        # ── stacked residual blocks ─────────────────────────────────────\n",
    "        blocks = []\n",
    "        for _ in range(n_layers - 1):\n",
    "            blocks.append(\n",
    "                nn.Sequential(                   # pre-LN residual MLP\n",
    "                    nn.LayerNorm(h),\n",
    "                    nn.Linear(h, h),\n",
    "                    act_fn,\n",
    "                    nn.Dropout(p_drop),\n",
    "                    nn.Linear(h, h),\n",
    "                    nn.Dropout(p_drop),\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        # ── output head ────────────────────────────────────────────────\n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_proj(x)\n",
    "        for blk in self.blocks:\n",
    "            x = x + blk(x)          # residual connection\n",
    "        return self.out_proj(x).squeeze(-1)\n",
    "\n",
    "    def get_complexity(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "class StepwisePRMDataset(Dataset):\n",
    "    \"\"\"mcr rewards가 반환한 entries(list[dict])를 (input_ids, scalar_reward) 샘플들로 변환한다.\n",
    "    한 entry = {question, completion[steps], rewards[float], …} →  (Problem + Step1, r1), (Problem + Step1 \\nStep2, r2) …\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        reward_type: str = \"naive\",\n",
    "        *,\n",
    "        cache_encodings: bool = True,\n",
    "        preprocess: bool = True,\n",
    "    ):\n",
    "        self.tokenizer   = tokenizer\n",
    "        self.max_length  = max_length\n",
    "        self.reward_type = reward_type\n",
    "        self.cache       = {} if cache_encodings else None\n",
    "        self.samples: List[Tuple[str, float]] = []\n",
    "\n",
    "        for e in entries:\n",
    "            q_txt   = e[\"question\"]\n",
    "            steps   = e[\"completion\"]\n",
    "            ans = e[\"gold_answer\"]\n",
    "            o_rewards = e[\"ori_rewards\"]\n",
    "            assert len(steps) == len(o_rewards)\n",
    "\n",
    "            if self.reward_type == \"contri\":\n",
    "                rewards = e[\"contributions\"]\n",
    "                # rewards = [max(0.0, x) for x in contri]\n",
    "            elif self.reward_type == \"mi\":\n",
    "                rewards = e[\"mi_rewards\"]\n",
    "            elif self.reward_type == \"naive\":\n",
    "                rewards = e[\"naive_rewards\"]\n",
    "            else:\n",
    "                rewards = o_rewards\n",
    "\n",
    "            prefix_lines = [f\"Problem: {q_txt}\"]\n",
    "            for step_txt, r in zip(steps, rewards):\n",
    "                prefix_lines.append(step_txt)\n",
    "                full_txt = \"\\n\".join(prefix_lines)\n",
    "                if preprocess:\n",
    "                    full_txt = self._clean(full_txt)\n",
    "                self.samples.append((full_txt, float(r)))   # (text, reward)\n",
    "\n",
    "    # --------------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(txt: str) -> str:\n",
    "        \"\"\"whitespace normalize + 소문자화(선택적) 등 간단 전처리\"\"\"\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return txt\n",
    "\n",
    "    # --------------------------------------------------------------------- dunder\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, reward = self.samples[idx]\n",
    "\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            ids = self.cache[text]\n",
    "        else:\n",
    "            ids = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[text] = ids\n",
    "\n",
    "        return ids, torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    (1) entries(list[dict]) → StepwisePRMDataset\n",
    "    (2) LLM encoder + PRM head fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PRMConfig, model, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "        # ----------------------------- Backbone model LLM (frozen or fine-tuned)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model  = model\n",
    "        self.model.eval()       # LLM은 feature extractor로 freeze\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        feat_dim = self.model.config.hidden_size\n",
    "        self.prm = ProcessRewardModel(feat_dim, cfg=cfg)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.prm.to(self.device)\n",
    "\n",
    "        self.opt  = optim.AdamW(self.prm.parameters(), lr=cfg.learning_rate, weight_decay = cfg.weight_decay)\n",
    "        self.crit = nn.MSELoss()\n",
    "        # self.crit = nn.BCELoss()\n",
    "\n",
    "        self.scheduler = None\n",
    "        if cfg.lr_scheduler == \"cosine\":                   \n",
    "            # total steps = (#batches per epoch) × epochs\n",
    "            self.total_steps = math.ceil(cfg.epochs * cfg.dataset_size / cfg.batch_size)\n",
    "            def lr_lambda(step):\n",
    "                if step < cfg.warmup_steps:\n",
    "                    return step / max(1, cfg.warmup_steps)\n",
    "                progress = (step - cfg.warmup_steps) / max(1, self.total_steps - cfg.warmup_steps)\n",
    "                return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            self.scheduler = LambdaLR(self.opt, lr_lambda)\n",
    "        elif cfg.lr_scheduler == \"linear\":\n",
    "            # Linear warmup + decay\n",
    "            self.total_steps = math.ceil(cfg.epochs * cfg.dataset_size / cfg.batch_size)\n",
    "            def lr_lambda(step):\n",
    "                if step < cfg.warmup_steps:\n",
    "                    return step / max(1, cfg.warmup_steps)\n",
    "                return max(0.0, (self.total_steps - step) / (self.total_steps - cfg.warmup_steps))\n",
    "            self.scheduler = LambdaLR(self.opt, lr_lambda)\n",
    "        elif cfg.lr_scheduler == \"step\":\n",
    "            # Step decay\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        self.ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "        self.ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.wandb_run = None\n",
    "        if cfg.use_wandb:                                  # <-- config에 플래그\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=cfg.wandb_project,                 # e.g. \"omega-prm\"\n",
    "                name=cfg.run_name,                         # e.g. \"qwen7b-prm\"\n",
    "                config=vars(cfg),                          # 모든 하이퍼파라미터 로깅\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------- features\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\"\"\"\n",
    "        out = self.model(input_ids=ids, return_dict=True,output_hidden_states=True)\n",
    "        features = out.hidden_states[-1][:, 0, :]     # CLS embedding\n",
    "        return features.float()\n",
    "\n",
    "    # ----------------------------------------------------------------- loop util\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool, epoch_idx: int) -> float:\n",
    "        self.prm.train(train)\n",
    "        total = 0.0\n",
    "        num_batches = len(loader)\n",
    "        \n",
    "        for step, (ids, reward) in enumerate(loader):\n",
    "            ids, reward = ids.to(self.device), reward.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats  = self._encode(ids)\n",
    "                pred   = self.prm(feats).squeeze(-1)\n",
    "                loss   = self.crit(pred, reward)\n",
    "                \n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), self.cfg.grad_clip)\n",
    "                    # Gradient accumulation (optional)\n",
    "                    if hasattr(self.cfg, 'grad_accum_steps') and self.cfg.grad_accum_steps > 1:\n",
    "                        if (step + 1) % self.cfg.grad_accum_steps == 0:\n",
    "                            self.opt.step()\n",
    "                            if self.scheduler: self.scheduler.step()\n",
    "                    else:\n",
    "                        self.opt.step()\n",
    "                        if self.scheduler: self.scheduler.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "            # -------- minibatch logging --------\n",
    "            if self.wandb_run and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"epoch\": epoch_idx + step / num_batches,\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                    \"grad_norm\": sum(p.grad.data.norm(2).item() for p in self.prm.parameters() if p.grad is not None),\n",
    "                    \"pred_mean\": pred.mean().item(),\n",
    "                    \"pred_std\": pred.std().item(),\n",
    "                    \"reward_mean\": reward.mean().item(),\n",
    "                    \"reward_std\": reward.std().item(),\n",
    "                })\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ----------------------------------------------------------------- public\n",
    "    def fit(self, train_loader, val_loader) -> Dict[str, List[float]]:\n",
    "        self.cfg.dataset_size = len(train_loader) \n",
    "\n",
    "        history = {\"train\": [], \"val\": []}\n",
    "        best_val, bad_epochs, patience = float(\"inf\"), 0, 8  # patience 증가\n",
    "\n",
    "        for ep in range(self.cfg.epochs):\n",
    "            tr_loss = self._run_epoch(train_loader, train=True,  epoch_idx=ep)\n",
    "            vl_loss = self._run_epoch(val_loader,   train=False, epoch_idx=ep)\n",
    "\n",
    "            history[\"train\"].append(tr_loss)\n",
    "            history[\"val\"].append(vl_loss)\n",
    "            print(f\"[Epoch {ep+1}/{self.cfg.epochs}] train={tr_loss:.4f}  val={vl_loss:.4f}\")\n",
    "\n",
    "            # -------- epoch logging --------\n",
    "            if self.wandb_run:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": tr_loss,\n",
    "                    \"val_loss\": vl_loss,\n",
    "                    \"epoch\": ep,\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                })\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if vl_loss < best_val:\n",
    "                best_val = vl_loss\n",
    "                bad_epochs = 0\n",
    "                self._save_checkpoint(\"best_prm.pt\", epoch=ep, val_loss=vl_loss)\n",
    "                print(f\"[Best] New best validation loss: {vl_loss:.4f}\")\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                print(f\"[Early-Stopping] No improvement for {bad_epochs}/{patience} epochs\")\n",
    "                if bad_epochs >= patience:\n",
    "                    print(f\"[Early-Stopping] Stopping training after {patience} epochs without improvement\")\n",
    "                    break\n",
    "        \n",
    "        self._save_checkpoint(\"last_prm.pt\", epoch=self.cfg.epochs - 1, val_loss=vl_loss)\n",
    "        return history\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Checkpoint helpers\n",
    "    def _save_checkpoint(self, filename: str, *, epoch: int, val_loss: float) -> None:\n",
    "        path = self.ckpt_dir / filename\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"prm_state\": self.prm.state_dict(),\n",
    "            \"scheduler_state\": (self.scheduler.state_dict() if self.scheduler else None),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"config\": vars(self.cfg),              # hyper‑params for reproducibility\n",
    "            \"model_name_or_path\": getattr(self.model, \"name_or_path\", None),\n",
    "            \"tokenizer_config\": self.tokenizer.__dict__.get(\"init_kwargs\", {}),\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        print(f\"[CKPT] Saved ⇒ {path}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Simple inference helper\n",
    "    @torch.no_grad()\n",
    "    def predict_reward(self, text: str) -> float:\n",
    "        ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "        feat = self._encode(ids)\n",
    "        return float(torch.sigmoid(self.prm(feat)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd412b",
   "metadata": {},
   "source": [
    "# Utils for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fafcc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Answer               # word 'Answer'\n",
    "        \\s*[:.\\-]\\s*         # separator\n",
    "        (.+?)\\s*$            # capture everything after\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    ")\n",
    "STEP_PATTERN = re.compile(r\"Step\\s*\\d+\\s*:\\s*(.*?)(?=\\nStep|\\nAnswer|$)\", re.S)\n",
    "# ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*$\", re.S)\n",
    "\n",
    "def build_prompt(question: str) -> str:\n",
    "    \"\"\"Return the prompt given a raw *question* string.\"\"\"\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a helpful math tutor. You must solve problems step-by-step using the exact format:\n",
    "Step 1: [first step]\n",
    "Step 2: [second step]\n",
    "...\n",
    "Answer: [final answer]\n",
    "\n",
    "Example:\n",
    "Problem: What is 5 + 3?\n",
    "Step 1: Add 5 and 3\n",
    "Step 2: 5 + 3 = 8\n",
    "Answer: 8\n",
    "\n",
    "Now solve the given problem using the same format.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def post_process_response(text: str) -> str:\n",
    "    # Step 패턴 찾기\n",
    "    step_pattern = r'Step\\s*\\d+:\\s*[^\\n]*'\n",
    "    steps = re.findall(step_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Answer 패턴 찾기\n",
    "    answer_pattern = r'Answer:\\s*([^\\n]*)'\n",
    "    answer_match = re.search(answer_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if steps and answer_match:\n",
    "        # 올바른 형식으로 재구성\n",
    "        result = \"\\n\".join(steps)\n",
    "        result += f\"\\nAnswer: {answer_match.group(1).strip()}\"\n",
    "        return result\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def parse_steps_and_answer(text: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Extract step list and answer string from a generated trajectory.\"\"\"\n",
    "    steps = [m.group(1).strip() for m in STEP_PATTERN.finditer(text)]\n",
    "    ans_match = ANSWER_PATTERN.search(text)\n",
    "    answer = ans_match.group(1).strip() if ans_match else \"\"\n",
    "    return steps, answer\n",
    "\n",
    "def generate_candidates(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    prompt: str,\n",
    "    num_candidates: int,\n",
    "    gen_cfg: GenerationConfig,\n",
    "    device: torch.device,\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate *num_candidates* reasoning trajectories for the prompt.\"\"\"\n",
    "    inputs = tokenizer([prompt] * num_candidates, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_cfg.to_dict())\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    texts = [t[len(prompt):] for t in texts]\n",
    "    \n",
    "    # processed_texts = []\n",
    "    # for text in texts:\n",
    "    #     processed_text = post_process_response(text)\n",
    "    #     processed_texts.append(processed_text)\n",
    "    return texts\n",
    "\n",
    "def compute_step_rewards(\n",
    "    baseline: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prm: ProcessRewardModel,\n",
    "    prm_device: torch.device,\n",
    "    prompt: str,\n",
    "    steps: List[str],\n",
    ") -> List[float]:\n",
    "    \"\"\"Return a list of scalar rewards (float) for each *completed* step.\"\"\"\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    # We will iteratively feed *prompt + completed steps* through baseline.\n",
    "    cumulative_text = prompt\n",
    "    for i, step_txt in enumerate(steps):\n",
    "        cumulative_text += f\"Step {i + 1}: {step_txt}\\n\"\n",
    "        tokens = tokenizer(cumulative_text, return_tensors=\"pt\").to(prm_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = baseline(**tokens, output_hidden_states=True)\n",
    "        # Use hidden states of the last token (or pool as needed)\n",
    "        last_hidden = outputs.hidden_states[-1][0, -1, :]  # (hidden_dim,)\n",
    "        last_hidden = last_hidden.float() \n",
    "        reward = prm(last_hidden.unsqueeze(0)).item()  # type: ignore\n",
    "        rewards.append(reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d096bb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e566a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading Baseline andPRM!\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Load baseline LM -------------------\n",
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\" \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "baseline = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# ------------------- Load PRM ---------------------------\n",
    "prm_ckpt_path = \"/home/leena/ccc_eval/mcts_prm/prm_dataset2/checkpoints/0715/contri/best_prm.pt\"\n",
    "prm_ckpt = torch.load(prm_ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "prm_cfg = PRMConfig(**prm_ckpt.get(\"cfg\", {}))\n",
    "prm = ProcessRewardModel(baseline.config.hidden_size, cfg=prm_cfg)\n",
    "prm.load_state_dict(prm_ckpt[\"prm_state\"])\n",
    "prm = prm.float()  # 명시적으로 Float32로 설정\n",
    "prm = prm.to(device).eval()\n",
    "print(\"Finish Loading Baseline andPRM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a98f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading Dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Both `max_new_tokens` (=512) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step/Answer: [] \n",
      "step rewards: []\n",
      "Step/Answer: [\"Calculate the total number of eggs Janet has each day.**\\n\\nJanet's ducks lay 16 eggs per day.\\n\\n**Step 2: Determine how many eggs Janet eats or uses each day.**\\n\\nJanet eats 3 eggs for breakfast every morning and bakes muffins for her friends every day with 4 eggs. So, the total number of eggs she uses each day is:\\n\\\\[ 3 + 4 = 7 \\\\]\\n\\n**Step 3: Calculate the number of eggs Janet sells each day.**\\n\\nThe number of eggs Janet sells each day is the total number of eggs she has minus the number of eggs she uses:\\n\\\\[ 16 - 7 = 9 \\\\]\\n\\n**Step 4: Calculate the total amount of money Janet makes from selling the eggs.**\\n\\nJanet sells each egg for $2. So, the total amount of money she makes each day is:\\n\\\\[ 9 \\\\times 2 = 18 \\\\]\\n\\nTherefore, the amount of money Janet makes every day at the farmers' market is \\\\(\\\\boxed{18}\\\\).\"] \n",
      "step rewards: [0.4559769630432129]\n",
      "Step/Answer: [\"Calculate the total number of eggs Janet has each day.**\\n\\nJanet's ducks lay 16 eggs per day.\\n\\n**Step 2: Determine how many eggs Janet eats or uses each day.**\\n\\nJanet eats 3 eggs for breakfast every morning and bakes muffins for her friends every day with 4 eggs. So, the total number of eggs she uses each day is:\\n\\\\[ 3 + 4 = 7 \\\\]\\n\\n**Step 3: Calculate the number of eggs Janet has left to sell.**\\n\\nThe number of eggs Janet has left to sell each day is the total number of eggs laid minus the number of eggs she uses:\\n\\\\[ 16 - 7 = 9 \\\\]\\n\\n**Step 4: Calculate the total revenue from selling the remaining eggs.**\\n\\nJanet sells each fresh duck egg for $2. So, the total revenue from selling 9 eggs is:\\n\\\\[ 9 \\\\times 2 = 18 \\\\]\\n\\nTherefore, Janet makes \\\\(\\\\boxed{18}\\\\) dollars every day at the farmers' market.\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:20, 20.89s/it]Both `max_new_tokens` (=512) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step rewards: [0.48467060923576355]\n",
      "Step/Answer: [] \n",
      "step rewards: []\n",
      "Step/Answer: [] \n",
      "step rewards: []\n",
      "Step/Answer: [] \n",
      "step rewards: []\n",
      "Step/Answer: ['Identify the number of bolts of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the number of bolts of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the robe takes 2 bolts of blue fiber, the number of bolts of white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\]', 'Add the number of bolts of blue fiber and white fiber together.\\nNow, we add the number of bolts of blue fiber and white fiber:\\n\\\\[ 2 + 1 = 3 \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n",
      "step rewards: [0.45141369104385376, 0.29674074053764343, 0.4403735399246216]\n",
      "Step/Answer: ['Identify the number of bolts of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the number of bolts of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the robe takes 2 bolts of blue fiber, the number of bolts of white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\]', 'Add the number of bolts of blue fiber and white fiber together.\\nNow, we add the number of bolts of blue fiber and white fiber:\\n\\\\[ 2 + 1 = 3 \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n",
      "step rewards: [0.45141369104385376, 0.29674074053764343, 0.4403735399246216]\n",
      "Step/Answer: ['Identify the number of bolts of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the number of bolts of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the robe takes 2 bolts of blue fiber, the number of bolts of white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\]', 'Add the number of bolts of blue fiber and white fiber together.\\nNow, we add the number of bolts of blue fiber and white fiber:\\n\\\\[ 2 + 1 = 3 \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n",
      "step rewards: [0.45141369104385376, 0.29674074053764343, 0.4403735399246216]\n",
      "Step/Answer: ['Identify the amount of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the amount of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the blue fiber is 2 bolts, the white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\text{ bolt} \\\\]', 'Add the amounts of blue and white fiber together.\\nNow, we add the blue fiber and the white fiber:\\n\\\\[ 2 + 1 = 3 \\\\text{ bolts} \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n",
      "step rewards: [0.42711615562438965, 0.3646433353424072, 0.4352686107158661]\n",
      "Step/Answer: ['Identify the number of bolts of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the number of bolts of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the robe takes 2 bolts of blue fiber, the number of bolts of white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\]', 'Add the number of bolts of blue fiber and white fiber together.\\nNow, we add the number of bolts of blue fiber and white fiber:\\n\\\\[ 2 + 1 = 3 \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n",
      "step rewards: [0.45141369104385376, 0.29674074053764343, 0.4403735399246216]\n",
      "Step/Answer: ['Identify the number of bolts of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.', 'Calculate the number of bolts of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the robe takes 2 bolts of blue fiber, the number of bolts of white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\]', 'Add the number of bolts of blue fiber and white fiber together.\\nNow, we add the number of bolts of blue fiber and white fiber:\\n\\\\[ 2 + 1 = 3 \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:35, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step rewards: [0.45141369104385376, 0.29674074053764343, 0.4403735399246216]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       "  'gold': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18',\n",
       "  'pred': '',\n",
       "  'chain': \"rs' market, we need to follow these steps:\\n\\n1. Calculate the total number of eggs Janet has each day.\\n2. Determine how many eggs Janet eats or uses each day.\\n3. Calculate the number of eggs Janet has left to sell.\\n4. Calculate the total revenue from selling the remaining eggs.\\n\\n**Step 1: Calculate the total number of eggs Janet has each day.**\\n\\nJanet's ducks lay 16 eggs per day.\\n\\n**Step 2: Determine how many eggs Janet eats or uses each day.**\\n\\nJanet eats 3 eggs for breakfast every morning and bakes muffins for her friends every day with 4 eggs. So, the total number of eggs she uses each day is:\\n\\\\[ 3 + 4 = 7 \\\\]\\n\\n**Step 3: Calculate the number of eggs Janet has left to sell.**\\n\\nThe number of eggs Janet has left to sell each day is the total number of eggs laid minus the number of eggs she uses:\\n\\\\[ 16 - 7 = 9 \\\\]\\n\\n**Step 4: Calculate the total revenue from selling the remaining eggs.**\\n\\nJanet sells each fresh duck egg for $2. So, the total revenue from selling 9 eggs is:\\n\\\\[ 9 \\\\times 2 = 18 \\\\]\\n\\nTherefore, Janet makes \\\\(\\\\boxed{18}\\\\) dollars every day at the farmers' market.\",\n",
       "  'score': 0.48467060923576355},\n",
       " {'id': 1,\n",
       "  'question': 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?',\n",
       "  'gold': 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3',\n",
       "  'pred': '',\n",
       "  'chain': 'or the robe, we need to follow these steps:\\n\\n1. Identify the amount of blue fiber required.\\n2. Calculate the amount of white fiber required.\\n3. Add the amounts of blue and white fiber together.\\n\\nStep 1: Identify the amount of blue fiber required.\\nThe problem states that the robe takes 2 bolts of blue fiber.\\n\\nStep 2: Calculate the amount of white fiber required.\\nThe problem also states that the robe takes half as much white fiber as blue fiber. Since the blue fiber is 2 bolts, the white fiber is:\\n\\\\[ \\\\frac{2}{2} = 1 \\\\text{ bolt} \\\\]\\n\\nStep 3: Add the amounts of blue and white fiber together.\\nNow, we add the blue fiber and the white fiber:\\n\\\\[ 2 + 1 = 3 \\\\text{ bolts} \\\\]\\n\\nTherefore, the total number of bolts of fiber needed is \\\\(\\\\boxed{3}\\\\).',\n",
       "  'score': 1.227028101682663}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- Dataset ---------------------------\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "max_samples = 2\n",
    "if max_samples:\n",
    "    ds = ds.select(range(max_samples))\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "print(\"Finish Loading Dataset!\")\n",
    "\n",
    "config = PRMConfig()\n",
    "gen_cfg = GenerationConfig(\n",
    "    temperature=0.3,\n",
    "    top_p=0.8,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    ")\n",
    "num_candidates = 6\n",
    "results = []\n",
    "for idx, sample in tqdm(enumerate(loader)):\n",
    "    question = sample[\"question\"][0]\n",
    "    gold = sample.get(\"answer\", [\"\"])[0]\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    # 1) Generate candidate CoTs\n",
    "    cand_texts = generate_candidates(\n",
    "        baseline,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_candidates,\n",
    "        gen_cfg,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # 2) Score each candidate via PRM\n",
    "    cand_scores: List[float] = []\n",
    "    cand_answers: List[str] = []\n",
    "    best_chain = \"\"\n",
    "    for text in cand_texts:\n",
    "        steps, answer = parse_steps_and_answer(text)\n",
    "        print(\"Step/Answer:\",steps, answer)\n",
    "        step_rewards = compute_step_rewards(baseline, tokenizer, prm, device, prompt, steps)\n",
    "        print(\"step rewards:\",step_rewards)\n",
    "        total_r = sum(step_rewards)\n",
    "        cand_scores.append(total_r)\n",
    "        cand_answers.append(answer)\n",
    "        # Keep full chain for printing if it wins\n",
    "        if total_r == max(cand_scores):\n",
    "            best_chain = text\n",
    "\n",
    "    best_idx = int(torch.tensor(cand_scores).argmax().item())\n",
    "    best_answer = cand_answers[best_idx]\n",
    "    best_score = cand_scores[best_idx]\n",
    "\n",
    "    # 3) Save result\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": sample.get(\"id\", [idx])[0] if isinstance(sample.get(\"id\", [idx]), list) else idx,\n",
    "            \"question\": question,\n",
    "            \"gold\": gold,\n",
    "            \"pred\": best_answer,\n",
    "            \"chain\": best_chain,\n",
    "            \"score\": best_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(loader)} samples…\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

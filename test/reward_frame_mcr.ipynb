{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    model_name:             str = \"Qwen/Qwen2.5-Math-7B\"\n",
    "    max_new_tokens:         int = 384\n",
    "    num_rollouts:           int = 8\n",
    "    reward_threshold:       float = 0.5\n",
    "    samples_per_question:   int = 1\n",
    "    use_llm:                bool = True\n",
    "    use_contri:             bool = False\n",
    "    # PRM Model config\n",
    "    hidden_size:        int = 512\n",
    "    num_layers:         int = 3\n",
    "    dropout:            float = 0.2\n",
    "    # PRMTrainer config\n",
    "    batch_size:         int = 12\n",
    "    learning_rate:      float = 5e-4\n",
    "    num_workers:        int = 4\n",
    "    weight_decay:       float = 1e-2\n",
    "    lr_scheduler:       str   = \"cosine\"\n",
    "    dataset_size:       int = 0\n",
    "    warmup_steps:       int   = 22\n",
    "    grad_clip:          float = 1.0\n",
    "    epochs:             int = 25\n",
    "    # Misc config\n",
    "    use_wandb:          bool = True\n",
    "    wandb_project:      str = \"mc_prm\"\n",
    "    run_name:           str = \"test_gsm8k_100_ori_mse\"\n",
    "    checkpoint_dir:     str = \"./checkpoints/gsm8k/ori_mse\"\n",
    "    seed:               int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                        UTILITY: ANSWER NORMALISATION                         #\n",
    "################################################################################\n",
    "import sympy as sp\n",
    "\n",
    "def _strip_markup(ans: str) -> str:\n",
    "    \"\"\"Remove common LaTeX/markup & variable tags.\"\"\"\n",
    "    # # Remove LaTeX inline math wrappers \\( … \\) or \\[ … \\]\n",
    "    # ans = re.sub(r\"\\\\[\\[(](.*?)[\\\\\\])]\", r\"\\1\", ans)\n",
    "    # # Remove \\boxed{…}\n",
    "    # ans = re.sub(r\"\\\\boxed\\{([^}]*)\\}\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\\\\\[.*?\\\\\\]\", \"\", ans)\n",
    "    ans = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", ans)\n",
    "    # Remove inline LaTeX: \\( ... \\) and $...$\n",
    "    ans = re.sub(r\"\\\\\\((.*?)\\\\\\)\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\$(.*?)\\$\", r\"\\1\", ans)\n",
    "    # Remove \\boxed{...}\n",
    "    ans = re.sub(r\"\\\\boxed\\s*{([^}]*)}\", r\"\\1\", ans)\n",
    "    # Remove LaTeX commands like \\text{...}, \\frac{...}, etc.\n",
    "    ans = re.sub(r\"\\\\[a-zA-Z]+\\s*(\\{[^{}]*\\})?\", \"\", ans)\n",
    "    # Remove variable assignments like \"y =\" or \"x=\" at start\n",
    "    ans = re.sub(r\"^[a-zA-Z]\\s*=\\s*\", \"\", ans)\n",
    "    # Trim outer $ … $ if present\n",
    "    ans = ans.strip()\n",
    "    if ans.startswith(\"$\") and ans.endswith(\"$\"):\n",
    "        ans = ans[1:-1]\n",
    "    return ans.strip()\n",
    "\n",
    "def _sanitize(text: str) -> str:\n",
    "    \"\"\"Normalise a candidate answer string for comparison.\"\"\"\n",
    "    text = _strip_markup(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\s\\.;:,]+$\", \"\", text)     # trailing punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)              # collapse spaces\n",
    "    return text\n",
    "\n",
    "def _to_float(expr: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(eval(expr.replace(\"^\", \"**\")))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _numeric_equiv(a: str, b: str) -> bool:\n",
    "    \"\"\"Return True if `a` and `b` are numerically equivalent or exact match.\"\"\"\n",
    "    a_clean, b_clean = map(_sanitize, (a, b))\n",
    "    if a_clean == b_clean:\n",
    "        return True\n",
    "\n",
    "    # Attempt simple numeric evaluation\n",
    "    a_val, b_val = _to_float(a_clean), _to_float(b_clean)\n",
    "    if a_val is not None and b_val is not None:\n",
    "        return math.isclose(a_val, b_val, rel_tol=1e-6)\n",
    "\n",
    "    if sp is not None:\n",
    "        try:\n",
    "            a_expr = sp.sympify(a_clean.replace(\"^\", \"**\"))\n",
    "            b_expr = sp.sympify(b_clean.replace(\"^\", \"**\"))\n",
    "            return sp.simplify(a_expr - b_expr) == 0\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def system_prompt(type):\n",
    "    prompt = \"\"\n",
    "    if type == \"sample\":\n",
    "        prompt = \"\"\"You are a math-problem expert. Your task is to complete the step-by-step solution for the problem provided. Write each reasoning step on its own line in the exact form \\\"Step k: [your reasoning step]\\n\\\", numbering start from Step 1. When the final answer is obtained, write exactly one final line, \\\"Answer: [Final answer]\\\". Do NOT add explanations, extra steps, or any text after the \"Answer:\" line.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Follow the FORMAT GUIDE structure exactly. Generate rationales step-by-step, not directly to the final answer. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    if type == \"rollout\":\n",
    "        prompt = \"\"\"You are a math problem-solving expert. Continue solving the given problem step by step, strictly following the required format. Each new step must begin with \\\"Step k+1: ...\\\", \\\"Step k+2:...\\\", and so on, continuing from the last given step number. When the final answer is reached, write only one final line starting with: \\\"Answer: [Final Answer]\\\". Do not add any explanations, extra commentary, or additional text after the \"Answer:\" line. Your output must follow this exact step-by-step format with no deviations.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "Continue and finish the solution:\n",
    "Step k+1: [Step k+1 reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Current solution steps:\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Continue and finish the solution:\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Current solution steps:\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Continue and finish the solution:\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Keep the reasoning steps precise and factual and complete the solution. Follow the FORMAT GUIDE structure exactly. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Project‑level helpers \n",
    "from utils import _sanitize, _numeric_equiv, _strip_markup, _to_float, system_prompt\n",
    "from config import PRMConfig\n",
    "\n",
    "class MCReward:\n",
    "    STEP_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Step\\s*              # word 'Step' (case-insensitive)\n",
    "        (\\d+)                # capture step number\n",
    "        \\s*[:.\\-]            # separator (: . or -)\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    ANSWER_PATTERN = re.compile(\n",
    "        r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "            Answer               # word 'Answer'\n",
    "            \\s*[:.\\-]\\s*         # separator\n",
    "            (.+?)\\s*$            # capture everything after\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    "    )\n",
    "    ## Masked rewards ##\n",
    "    OP_TOKENS = [\"add\", \"plus\", \"sum\", \"subtract\", \"minus\",\n",
    "             \"multiply\", \"times\", \"product\", \"divide\", \"quotient\"]\n",
    "    _MASK_PATTERN = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "        # {ops_pattern}|                # operator patterns\n",
    "            \\b\\d+(?:\\.\\d+)?\\b         # integers / decimals\n",
    "          | \\b\\d+/\\d+\\b                 # simple fractions\n",
    "        #   | \\b[a-zA-Z]\\b                 # single‑letter variables\n",
    "        )\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def generate_solutions(self, question: str, sys_prompt: str, num_solutions: int):\n",
    "        prompt = f\"{sys_prompt}\\n\\n{question}\\n\"  # Prompt the model to start the step-by-step solution\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        # Generate multiple solutions via sampling\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_solutions,\n",
    "            temperature=0.8,         # sampling temperature for diversity (adjust as needed)\n",
    "            top_p=0.8,               # top-p sampling for diversity\n",
    "            pad_token_id=self.tokenizer.eos_token_id  # pad token ID to avoid warning for some models\n",
    "        )\n",
    "        solutions = []\n",
    "        prompt_len = input_ids.shape[-1]\n",
    "        for i in range(num_solutions):\n",
    "            # Each output is the concatenation of the prompt and the generated completion.\n",
    "            generated_ids = outputs[i]\n",
    "            # Extract only the newly generated tokens (skip the prompt tokens).\n",
    "            gen_ids = generated_ids[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            solutions.append(text)\n",
    "            # print(f\"{i}-th Sampled Solutions:\",text)\n",
    "        return solutions\n",
    "    \n",
    "    def gsm8k_solutions(self, question: str, gold_solution: str):\n",
    "        # 1. Split lines *before* the final answer marker (#### …)\n",
    "        lines: List[str] = []\n",
    "        gold_answer: str = \"\"\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        for raw_ln in gold_solution.splitlines():\n",
    "            ln = raw_ln.strip()\n",
    "            if not ln:\n",
    "                continue  # skip empty\n",
    "            ans_match = _ANSWER_RE.match(ln)\n",
    "            if ans_match:\n",
    "                gold_answer = ans_match.group(1).strip()\n",
    "                break  # everything after #### is ignored\n",
    "            lines.append(ln)\n",
    "\n",
    "        if not gold_answer:\n",
    "            raise ValueError(\"Could not find final answer marker '#### <answer>' in gold_solution.\")\n",
    "\n",
    "        # 2. Prefix each explanatory line with \"Step i:\"\n",
    "        solution_steps = [f\"Step {i + 1}: {txt}\" for i, txt in enumerate(lines)]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"solution\": solution_steps,\n",
    "            \"gold_answer\": gold_answer,\n",
    "        }\n",
    "\n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"Split each step to start with 'Step X:' and the answer to start with 'Answer:'.\"\"\"\n",
    "        steps = []\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if self.STEP_PATTERN.match(line):\n",
    "                cleaned = re.sub(r'^[\\s>#*\\-]+', '', line)\n",
    "                steps.append(cleaned)\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(steps[: i + 1]) + \"\\n\", return_tensors=\"pt\").to(self.device) # steps up to current step i (0-indexed)\n",
    "            # Decide how to prompt the next part:\n",
    "            if i < total_steps - 1:\n",
    "                next_label = f\"Step {i + 2}:\"\n",
    "            else:\n",
    "                next_label = \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            # Build full prefix ids (avoid Python concat inefficiency by cat)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1] \n",
    "            # Check each rollout's final answer against the gold answer\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                print(f\"[{i+1}-th Step, {idx}-th Original Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            reward = correct_count / float(self.config.num_rollouts)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Masked solution paths\n",
    "    def model_masking(self, text: str, *, max_new_tokens: int = 64) -> str:\n",
    "        prompt = \"In the sentence below, mask any word or expression that seems crucial for solving the math step. This may include key numbers, variables, or action words (like operations), but you should decide what matters. Replace each important item with '[MASKED]'. Keep everything else unchanged. Return ONE line.\\n\\nSentence: \\\"{sent}\\\"\\nRewritten:\".format(sent=text)\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        out_ids   = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.2, top_p=0.2,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out_ids[0][input_ids.shape[-1]:],\n",
    "                                     skip_special_tokens=True).strip()\n",
    "\n",
    "    def perturbed_step_rewards(self, question: str, sys_prompt: str, steps: List[str], gold_answer: str, use_llm: bool = True) -> List[float]:\n",
    "        \"\"\"Compute MC correctness rates *after masking* the current step.\n",
    "        Each step `i` is replaced with a *perturbed* version where important\n",
    "        tokens (numbers, fractions, single‑letter variables) are substituted by\n",
    "        the literal string ``[MASKED]``. All preceding steps remain intact.\n",
    "        \"\"\"\n",
    "        ptb_rewards: List[float] = []\n",
    "        total_steps = len(steps)\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            # 1. Perturb *only* step i\n",
    "            orig_step = steps[i] \n",
    "            step_match = re.match(r\"^[\\s>#*\\-]*Step\\s*\\d+\\s*[:.\\-]\\s*\", orig_step, flags=re.I)\n",
    "            prefix = step_match.group(0) if step_match else \"\"\n",
    "            # ② 나머지 부분(body)만 마스킹\n",
    "            body   = steps[i][len(prefix):]                       # 접두사 뒷부분\n",
    "            if use_llm:\n",
    "                masked_body = self.model_masking(body)\n",
    "            else:\n",
    "                masked_body = self._MASK_PATTERN.sub(\"[MASKED]\", body)\n",
    "            # ③ 접두사 + 마스킹된 body\n",
    "            masked_step = prefix + masked_body    \n",
    "            ptb_prefix_steps = steps[:i] + [masked_step]\n",
    "            # print(\"perturbed step:\", ptb_prefix_steps)\n",
    "\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(ptb_prefix_steps) + \"\\n\", return_tensors=\"pt\").to(self.device)\n",
    "            next_label = f\"Step {i + 2}:\" if i < total_steps - 1 else \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1]\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                print(f\"Masked [{i+1}-th Step, {idx}-th Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            ptb_rewards.append(correct_count / float(self.config.num_rollouts))\n",
    "        return ptb_rewards\n",
    "\n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets(self, problems: List):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        for problem in problems:\n",
    "            question = problem[\"question\"]\n",
    "            # gold_answer = problem[\"gold_answer\"]\n",
    "            gold_answer = _sanitize(problem[\"gold_answer\"])\n",
    "            # Generate one or more solutions for this question\n",
    "            sample_prompt = system_prompt(\"sample\")\n",
    "            rollout_prompt = system_prompt(\"rollout\")\n",
    "            solutions = self.generate_solutions(question, sys_prompt=sample_prompt, num_solutions=self.config.samples_per_question)\n",
    "            \n",
    "            for sol_text in solutions:\n",
    "                steps, answer = self.parse_solution(sol_text)\n",
    "                # print(\"Parsed solution:\", steps, answer)\n",
    "                if answer is None: # If no answer was found in the solution (edge case), skip this solution\n",
    "                    continue\n",
    "                # 2. Compute *original* & *perturbed* per‑step rewards\n",
    "                # ----------------------------------------------------------\n",
    "                ori_rewards = self.compute_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                ptb_rewards = self.perturbed_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                # Align lengths (robustness)\n",
    "                if len(ptb_rewards) != len(ori_rewards):\n",
    "                    ptb_rewards = ptb_rewards[: len(ori_rewards)]\n",
    "                # contributions = [max(0, o - p) for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "                contributions = [o - p for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "                entry = {\n",
    "                    \"question\": question,\n",
    "                    \"completion\": steps,          # list[str] (Step i: ...)\n",
    "                    \"ori_rewards\": ori_rewards,    # list[float]\n",
    "                    \"ptb_rewards\": ptb_rewards,    # list[float]\n",
    "                    \"contributions\": contributions,  # ori − ptb\n",
    "                    \"answer\": answer,\n",
    "                    \"gold_answer\": gold_answer,\n",
    "                }\n",
    "                dataset.append(entry)\n",
    "        return dataset\n",
    "    \n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets_gsm8k(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        rollout_pr = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
    "        if take is not None:\n",
    "            ds = ds.shuffle(seed=self.config.seed).select(range(start, start+take))\n",
    "\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "        \n",
    "        for sample in tqdm(ds, desc=\"Building GSM-8K reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            q_txt   = sample[\"question\"]\n",
    "            g_sol   = sample[\"answer\"]\n",
    "            lines, gold_ans = [], None\n",
    "            for ln in g_sol.splitlines():\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                m = _ANSWER_RE.match(ln)\n",
    "                if m:\n",
    "                    gold_ans = sanitize(m.group(1))\n",
    "                    break\n",
    "                lines.append(ln)\n",
    "            if gold_ans is None:\n",
    "                raise ValueError(\"gold answer not found for sample\")\n",
    "            steps = [f\"Step {i+1}: {t}\" for i, t in enumerate(lines)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(q_txt, rollout_pr, steps, gold_ans)\n",
    "            ptb = psr(q_txt, rollout_pr, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            #  ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                    \"question\":      q_txt,\n",
    "                    \"completion\":    steps,\n",
    "                    \"ori_rewards\":   ori,\n",
    "                    \"ptb_rewards\":   ptb,\n",
    "                    \"contributions\": contrib,\n",
    "                    \"answer\":        gold_ans,\n",
    "                    \"gold_answer\":   gold_ans,\n",
    "                }\n",
    "            dataset.append(entry)\n",
    "            # print(entry)\n",
    "        return dataset\n",
    "\n",
    "    def build_datasets_math(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        \"\"\"\n",
    "        ① MATH 데이터셋 로드 → ② 정답·스텝 추출 → ③ 보상 계산 → ④ dict 리스트 반환\n",
    "        \"\"\"\n",
    "        boxed_re   = re.compile(r'\\\\boxed\\{(.+?)\\}', re.S)\n",
    "        sent_split = re.compile(r'\\.(?!\\d)(?=\\s|$)')   # 소수점·수식 내부 마침표 무시\n",
    "\n",
    "        rollout_prompt = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"HuggingFaceTB/MATH\", \"all\", split=split)\n",
    "\n",
    "        # shuffle & take\n",
    "        if take is not None:\n",
    "            ds = ds.select(range(start, start+take))\n",
    "\n",
    "        # (alias) time optimize\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "\n",
    "        for sample in tqdm(ds, desc=\"Building MATH reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            full_sol   = sample[\"solution\"]\n",
    "            m          = boxed_re.search(full_sol)\n",
    "            gold_ans   = sanitize(m.group(1)) if m else None\n",
    "            sol_wo_box = boxed_re.sub(\"\", full_sol)\n",
    "            raw_steps  = [s.strip() for s in sent_split.split(sol_wo_box) if s.strip()]\n",
    "            steps      = [f\"Step {i+1}: {s}\" for i, s in enumerate(raw_steps)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(sample[\"problem\"], rollout_prompt, steps, gold_ans)\n",
    "            ptb = psr(sample[\"problem\"], rollout_prompt, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            # ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                \"question\":      sample[\"problem\"],\n",
    "                \"completion\":    steps,\n",
    "                \"ori_rewards\":   ori,\n",
    "                \"ptb_rewards\":   ptb,\n",
    "                \"contributions\": contrib,\n",
    "                \"answer\":        gold_ans,\n",
    "                \"gold_answer\":   gold_ans,\n",
    "                \"level\":         sample[\"level\"],\n",
    "                \"type\":          sample[\"type\"],\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "            # print(entry)\n",
    "        return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Project‑level helpers \n",
    "from utils import _sanitize, _numeric_equiv, _strip_markup, _to_float, system_prompt\n",
    "from config import PRMConfig\n",
    "\n",
    "class MCReward:\n",
    "    STEP_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Step\\s*              # word 'Step' (case-insensitive)\n",
    "        (\\d+)                # capture step number\n",
    "        \\s*[:.\\-]            # separator (: . or -)\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    ANSWER_PATTERN = re.compile(\n",
    "        r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "            Answer               # word 'Answer'\n",
    "            \\s*[:.\\-]\\s*         # separator\n",
    "            (.+?)\\s*$            # capture everything after\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    "    )\n",
    "    ## Masked rewards ##\n",
    "    OP_TOKENS = [\"add\", \"plus\", \"sum\", \"subtract\", \"minus\",\n",
    "             \"multiply\", \"times\", \"product\", \"divide\", \"quotient\"]\n",
    "    _MASK_PATTERN = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "        # {ops_pattern}|                # operator patterns\n",
    "            \\b\\d+(?:\\.\\d+)?\\b         # integers / decimals\n",
    "          | \\b\\d+/\\d+\\b                 # simple fractions\n",
    "        #   | \\b[a-zA-Z]\\b                 # single‑letter variables\n",
    "        )\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.llm = LLM(\n",
    "            model=config.model_name,     # 동일 체크포인트\n",
    "            dtype=\"float16\",                 # fp16 / bfloat16\n",
    "            tensor_parallel_size=torch.cuda.device_count(),  # 여러 GPU → 자동 shard\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.sparams = SamplingParams(\n",
    "            max_tokens=config.max_new_tokens,\n",
    "            temperature=0.8,\n",
    "            top_p=0.8,\n",
    "            n=config.num_rollouts,          # 한번에 num_rollouts 샘플\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def generate_solutions(self, question: str, sys_prompt: str, num_solutions: int):\n",
    "        prompt = f\"{sys_prompt}\\n\\n{question}\\n\"  # Prompt the model to start the step-by-step solution\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        # Generate multiple solutions via sampling\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_solutions,\n",
    "            temperature=0.8,         # sampling temperature for diversity (adjust as needed)\n",
    "            top_p=0.8,               # top-p sampling for diversity\n",
    "            pad_token_id=self.tokenizer.eos_token_id  # pad token ID to avoid warning for some models\n",
    "        )\n",
    "        solutions = []\n",
    "        prompt_len = input_ids.shape[-1]\n",
    "        for i in range(num_solutions):\n",
    "            # Each output is the concatenation of the prompt and the generated completion.\n",
    "            generated_ids = outputs[i]\n",
    "            # Extract only the newly generated tokens (skip the prompt tokens).\n",
    "            gen_ids = generated_ids[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            solutions.append(text)\n",
    "            # print(f\"{i}-th Sampled Solutions:\",text)\n",
    "        return solutions\n",
    "    \n",
    "    def gsm8k_solutions(self, question: str, gold_solution: str):\n",
    "        # 1. Split lines *before* the final answer marker (#### …)\n",
    "        lines: List[str] = []\n",
    "        gold_answer: str = \"\"\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        for raw_ln in gold_solution.splitlines():\n",
    "            ln = raw_ln.strip()\n",
    "            if not ln:\n",
    "                continue  # skip empty\n",
    "            ans_match = _ANSWER_RE.match(ln)\n",
    "            if ans_match:\n",
    "                gold_answer = ans_match.group(1).strip()\n",
    "                break  # everything after #### is ignored\n",
    "            lines.append(ln)\n",
    "\n",
    "        if not gold_answer:\n",
    "            raise ValueError(\"Could not find final answer marker '#### <answer>' in gold_solution.\")\n",
    "\n",
    "        # 2. Prefix each explanatory line with \"Step i:\"\n",
    "        solution_steps = [f\"Step {i + 1}: {txt}\" for i, txt in enumerate(lines)]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"solution\": solution_steps,\n",
    "            \"gold_answer\": gold_answer,\n",
    "        }\n",
    "\n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"Split each step to start with 'Step X:' and the answer to start with 'Answer:'.\"\"\"\n",
    "        steps = []\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if self.STEP_PATTERN.match(line):\n",
    "                cleaned = re.sub(r'^[\\s>#*\\-]+', '', line)\n",
    "                steps.append(cleaned)\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def _generate_rollouts(self, prompt: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        vLLM 에서 동일 프롬프트를 n번(=num_rollouts) 샘플링해서 텍스트만 반환\n",
    "        \"\"\"\n",
    "        outs = self.llm.generate([prompt], self.sparams)   # 배치 길이 1\n",
    "        return [o.outputs[ri].text for o in outs for ri in range(len(o.outputs))]\n",
    "\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            step_prefix = \"\\n\".join(steps[:i+1]) + \"\\n\"\n",
    "            next_label  = f\"Step {i+2}:\" if i < total_steps-1 else \"Answer:\"\n",
    "            prompt = f\"{base_prompt}{step_prefix}{next_label}\"\n",
    "            rollout_outputs = self._generate_rollouts(prompt)  \n",
    "            print(\"Original rollout outputs:\", rollout_outputs)\n",
    "\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                pred_answer = self._extract_answer(seq)\n",
    "                print(f\"[{i+1}-th Step, {idx}-th Original Rollout]\", seq, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            reward = correct_count / float(self.config.num_rollouts)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Masked solution paths\n",
    "    def model_masking(self, text: str, *, max_new_tokens: int = 64) -> str:\n",
    "        prompt = \"In the sentence below, mask any word or expression that seems crucial for solving the math step. This may include key numbers, variables, or action words (like operations), but you should decide what matters. Replace each important item with '[MASKED]'. Keep everything else unchanged. Return ONE line.\\n\\nSentence: \\\"{sent}\\\"\\nRewritten:\".format(sent=text)\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        out_ids   = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.2, top_p=0.2,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out_ids[0][input_ids.shape[-1]:],\n",
    "                                     skip_special_tokens=True).strip()\n",
    "\n",
    "    def perturbed_step_rewards(self, question: str, sys_prompt: str, steps: List[str], gold_answer: str, use_llm: bool = True) -> List[float]:\n",
    "        ptb_rewards: List[float] = []\n",
    "        total_steps = len(steps)\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            # 1. Perturb *only* step i\n",
    "            orig_step = steps[i] \n",
    "            step_match = re.match(r\"^[\\s>#*\\-]*Step\\s*\\d+\\s*[:.\\-]\\s*\", orig_step, flags=re.I)\n",
    "            prefix = step_match.group(0) if step_match else \"\"\n",
    "            # ② 나머지 부분(body)만 마스킹\n",
    "            body   = steps[i][len(prefix):]                       # 접두사 뒷부분\n",
    "            if use_llm:\n",
    "                masked_body = self.model_masking(body)\n",
    "            else:\n",
    "                masked_body = self._MASK_PATTERN.sub(\"[MASKED]\", body)\n",
    "            # ③ 접두사 + 마스킹된 body\n",
    "            masked_step = prefix + masked_body    \n",
    "            ptb_prefix_steps = steps[:i] + [masked_step]\n",
    "            # print(\"perturbed step:\", ptb_prefix_steps)\n",
    "\n",
    "            step_prefix = \"\\n\".join(ptb_prefix_steps) + \"\\n\"\n",
    "            next_label = f\"Step {i + 2}:\" if i < total_steps - 1 else \"Answer:\"\n",
    "            prompt = f\"{base_prompt}{step_prefix}{next_label}\"\n",
    "            rollout_outputs = self._generate_rollouts(prompt)  \n",
    "            print(\"Masked rollout outputs:\", rollout_outputs)\n",
    "            \n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                pred_answer = self._extract_answer(seq)\n",
    "                print(f\"Masked [{i+1}-th Step, {idx}-th Rollout]\", seq, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            ptb_rewards.append(correct_count / float(self.config.num_rollouts))\n",
    "        return ptb_rewards\n",
    "\n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets(self, problems: List):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        for problem in problems:\n",
    "            question = problem[\"question\"]\n",
    "            # gold_answer = problem[\"gold_answer\"]\n",
    "            gold_answer = _sanitize(problem[\"gold_answer\"])\n",
    "            # Generate one or more solutions for this question\n",
    "            sample_prompt = system_prompt(\"sample\")\n",
    "            rollout_prompt = system_prompt(\"rollout\")\n",
    "            solutions = self.generate_solutions(question, sys_prompt=sample_prompt, num_solutions=self.config.samples_per_question)\n",
    "            \n",
    "            for sol_text in solutions:\n",
    "                steps, answer = self.parse_solution(sol_text)\n",
    "                # print(\"Parsed solution:\", steps, answer)\n",
    "                if answer is None: # If no answer was found in the solution (edge case), skip this solution\n",
    "                    continue\n",
    "                # 2. Compute *original* & *perturbed* per‑step rewards\n",
    "                # ----------------------------------------------------------\n",
    "                ori_rewards = self.compute_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                ptb_rewards = self.perturbed_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                # Align lengths (robustness)\n",
    "                if len(ptb_rewards) != len(ori_rewards):\n",
    "                    ptb_rewards = ptb_rewards[: len(ori_rewards)]\n",
    "                # contributions = [max(0, o - p) for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "                contributions = [o - p for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "                entry = {\n",
    "                    \"question\": question,\n",
    "                    \"completion\": steps,          # list[str] (Step i: ...)\n",
    "                    \"ori_rewards\": ori_rewards,    # list[float]\n",
    "                    \"ptb_rewards\": ptb_rewards,    # list[float]\n",
    "                    \"contributions\": contributions,  # ori − ptb\n",
    "                    \"answer\": answer,\n",
    "                    \"gold_answer\": gold_answer,\n",
    "                }\n",
    "                dataset.append(entry)\n",
    "        return dataset\n",
    "    \n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets_gsm8k(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        rollout_pr = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
    "        if take is not None:\n",
    "            ds = ds.shuffle(seed=self.config.seed).select(range(start, start+take))\n",
    "\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "        \n",
    "        for sample in tqdm(ds, desc=\"Building GSM-8K reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            q_txt   = sample[\"question\"]\n",
    "            g_sol   = sample[\"answer\"]\n",
    "            lines, gold_ans = [], None\n",
    "            for ln in g_sol.splitlines():\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                m = _ANSWER_RE.match(ln)\n",
    "                if m:\n",
    "                    gold_ans = sanitize(m.group(1))\n",
    "                    break\n",
    "                lines.append(ln)\n",
    "            if gold_ans is None:\n",
    "                raise ValueError(\"gold answer not found for sample\")\n",
    "            steps = [f\"Step {i+1}: {t}\" for i, t in enumerate(lines)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(q_txt, rollout_pr, steps, gold_ans)\n",
    "            ptb = psr(q_txt, rollout_pr, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            #  ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                    \"question\":      q_txt,\n",
    "                    \"completion\":    steps,\n",
    "                    \"ori_rewards\":   ori,\n",
    "                    \"ptb_rewards\":   ptb,\n",
    "                    \"contributions\": contrib,\n",
    "                    \"answer\":        gold_ans,\n",
    "                    \"gold_answer\":   gold_ans,\n",
    "                }\n",
    "            dataset.append(entry)\n",
    "            # print(entry)\n",
    "        return dataset\n",
    "\n",
    "    def build_datasets_math(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        \"\"\"\n",
    "        ① MATH 데이터셋 로드 → ② 정답·스텝 추출 → ③ 보상 계산 → ④ dict 리스트 반환\n",
    "        \"\"\"\n",
    "        boxed_re   = re.compile(r'\\\\boxed\\{(.+?)\\}', re.S)\n",
    "        sent_split = re.compile(r'\\.(?!\\d)(?=\\s|$)')   # 소수점·수식 내부 마침표 무시\n",
    "\n",
    "        rollout_prompt = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"HuggingFaceTB/MATH\", \"all\", split=split)\n",
    "\n",
    "        # shuffle & take\n",
    "        if take is not None:\n",
    "            ds = ds.select(range(start, start+take))\n",
    "\n",
    "        # (alias) time optimize\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "\n",
    "        for sample in tqdm(ds, desc=\"Building MATH reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            full_sol   = sample[\"solution\"]\n",
    "            m          = boxed_re.search(full_sol)\n",
    "            gold_ans   = sanitize(m.group(1)) if m else None\n",
    "            sol_wo_box = boxed_re.sub(\"\", full_sol)\n",
    "            raw_steps  = [s.strip() for s in sent_split.split(sol_wo_box) if s.strip()]\n",
    "            steps      = [f\"Step {i+1}: {s}\" for i, s in enumerate(raw_steps)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(sample[\"problem\"], rollout_prompt, steps, gold_ans)\n",
    "            ptb = psr(sample[\"problem\"], rollout_prompt, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            # ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                \"question\":      sample[\"problem\"],\n",
    "                \"completion\":    steps,\n",
    "                \"ori_rewards\":   ori,\n",
    "                \"ptb_rewards\":   ptb,\n",
    "                \"contributions\": contrib,\n",
    "                \"answer\":        gold_ans,\n",
    "                \"gold_answer\":   gold_ans,\n",
    "                \"level\":         sample[\"level\"],\n",
    "                \"type\":          sample[\"type\"],\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "            # print(entry)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-04 17:16:15 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 07-04 17:16:15 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-04 17:16:15 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-04 17:16:18 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-04 17:16:18 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-Math-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-04 17:16:20 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f36e8fbf3e0>\n",
      "ERROR 07-04 17:16:20 [core.py:515] EngineCore failed to start.\n",
      "ERROR 07-04 17:16:20 [core.py:515] Traceback (most recent call last):\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "ERROR 07-04 17:16:20 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 07-04 17:16:20 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "ERROR 07-04 17:16:20 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 76, in __init__\n",
      "ERROR 07-04 17:16:20 [core.py:515]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 07-04 17:16:20 [core.py:515]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "ERROR 07-04 17:16:20 [core.py:515]     self._init_executor()\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 07-04 17:16:20 [core.py:515]     self.collective_rpc(\"init_device\")\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "ERROR 07-04 17:16:20 [core.py:515]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 07-04 17:16:20 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "ERROR 07-04 17:16:20 [core.py:515]     return func(*args, **kwargs)\n",
      "ERROR 07-04 17:16:20 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "ERROR 07-04 17:16:20 [core.py:515]     self.worker.init_device()  # type: ignore\n",
      "ERROR 07-04 17:16:20 [core.py:515]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 128, in init_device\n",
      "ERROR 07-04 17:16:20 [core.py:515]     torch.cuda.set_device(self.device)\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 529, in set_device\n",
      "ERROR 07-04 17:16:20 [core.py:515]     torch._C._cuda_setDevice(device)\n",
      "ERROR 07-04 17:16:20 [core.py:515]   File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "ERROR 07-04 17:16:20 [core.py:515]     raise RuntimeError(\n",
      "ERROR 07-04 17:16:20 [core.py:515] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 519, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 506, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 76, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/utils.py\", line 2671, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 128, in init_device\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 529, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 358, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m mcr = \u001b[43mMCReward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m math_dataset = mcr.build_datasets_math(split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,take=\u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m math_dataset[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mMCReward.__init__\u001b[39m\u001b[34m(self, config, model, tokenizer)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28mnext\u001b[39m(model.parameters()).device\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 동일 체크포인트\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# fp16 / bfloat16\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 여러 GPU → 자동 shard\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m.sparams = SamplingParams(\n\u001b[32m     58\u001b[39m     max_tokens=config.max_new_tokens,\n\u001b[32m     59\u001b[39m     temperature=\u001b[32m0.8\u001b[39m,\n\u001b[32m     60\u001b[39m     top_p=\u001b[32m0.8\u001b[39m,\n\u001b[32m     61\u001b[39m     n=config.num_rollouts,          \u001b[38;5;66;03m# 한번에 num_rollouts 샘플\u001b[39;00m\n\u001b[32m     62\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m engine_args = EngineArgs(\n\u001b[32m    214\u001b[39m     model=model,\n\u001b[32m    215\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/engine/llm_engine.py:501\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    499\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:124\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    123\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:101\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     98\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:75\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     72\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:558\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    557\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:422\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m.resources.output_socket = make_zmq_socket(\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx, output_address, zmq.PULL)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_addresses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_engines_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlocal_start_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     coordinator = \u001b[38;5;28mself\u001b[39m.resources.coordinator\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coordinator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:491\u001b[39m, in \u001b[36mMPClient._init_engines_direct\u001b[39m\u001b[34m(self, vllm_config, local_only, local_start_index, input_address, output_address, executor_class, log_stats)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m.resources.engine_manager = CoreEngineProcManager(\n\u001b[32m    480\u001b[39m         EngineCoreProc.run_engine_core,\n\u001b[32m    481\u001b[39m         vllm_config=vllm_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m         start_index=start_index,\n\u001b[32m    488\u001b[39m         local_start_index=local_start_index)\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:511\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self, handshake_socket, input_address, output_address)\u001b[39m\n\u001b[32m    506\u001b[39m proc_manager = \u001b[38;5;28mself\u001b[39m.resources.engine_manager\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc_manager, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), CoreEngineProcManager)), (\n\u001b[32m    508\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_wait_for_engine_startup should only be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    509\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalled with CoreEngineProcManager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_engines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproc_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/vllm/v1/utils.py:494\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    499\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}"
     ]
    }
   ],
   "source": [
    "cfg = PRMConfig()\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "mcr = MCReward(config=cfg, model=model, tokenizer=tokenizer)\n",
    "math_dataset = mcr.build_datasets_math(split=\"test\",take=2)\n",
    "math_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "openai.api_key = \"EMPTY\"  # vLLM 기본 키\n",
    "# openai.api_base = \"http://localhost:8000/v1\"\n",
    "openai.api_base = \"http://<vllm-server-ip>:8000/v1\"\n",
    "\n",
    "resp = openai.ChatCompletion.create(\n",
    "    model=\"Qwen/Qwen2.5-Math-14B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"{system_prompt(\"rollout\")} Problem: What is the positive difference between $120\\\\%$ of 30 and $130\\\\%$ of 20?\\nStep 1: One hundred twenty percent of 30 is $120\\\\cdot30\\\\cdot\\\\frac{1}{100}=36$, and $130\\\\%$ of 20 is $ 130\\\\cdot 20\\\\cdot\\\\frac{1}{100}=26$\"\"\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    ")\n",
    "\n",
    "print(resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random, re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class StepwisePRMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    build_datasets() 가 반환한 entries(list[dict])를\n",
    "    (input_ids, scalar_reward) 샘플들로 변환한다.\n",
    "\n",
    "    한 entry = {question, completion[steps], rewards[float], …}\n",
    "    →  (Problem + Step1,   r1)\n",
    "        (Problem + Step1 \\nStep2,   r2) …\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        use_contr: bool = True,\n",
    "        *,\n",
    "        cache_encodings: bool = True,\n",
    "        preprocess: bool = True,\n",
    "    ):\n",
    "        self.tokenizer   = tokenizer\n",
    "        self.max_length  = max_length\n",
    "        self.use_contri    = use_contr\n",
    "        self.cache       = {} if cache_encodings else None\n",
    "        self.samples: List[Tuple[str, float]] = []\n",
    "\n",
    "        for e in entries:\n",
    "            q_txt   = e[\"question\"]\n",
    "            steps   = e[\"completion\"]\n",
    "            o_rewards = e[\"ori_rewards\"]\n",
    "            contri = e[\"contributions\"]\n",
    "            assert len(steps) == len(o_rewards)\n",
    "\n",
    "            if self.use_contri:\n",
    "                rewards = contri\n",
    "                # rewards = [max(0.0, x) for x in contri]\n",
    "            else:\n",
    "                rewards = o_rewards\n",
    "\n",
    "            prefix_lines = [f\"Problem: {q_txt}\"]\n",
    "            for step_txt, r in zip(steps, rewards):\n",
    "                prefix_lines.append(step_txt)\n",
    "                full_txt = \"\\n\".join(prefix_lines)\n",
    "                if preprocess:\n",
    "                    full_txt = self._clean(full_txt)\n",
    "                self.samples.append((full_txt, float(r)))   # (text, reward)\n",
    "\n",
    "    # --------------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(txt: str) -> str:\n",
    "        \"\"\"whitespace normalize + 소문자화(선택적) 등 간단 전처리\"\"\"\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return txt\n",
    "\n",
    "    # --------------------------------------------------------------------- dunder\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, reward = self.samples[idx]\n",
    "\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            ids = self.cache[text]\n",
    "        else:\n",
    "            ids = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[text] = ids\n",
    "\n",
    "        return ids, torch.tensor(reward, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(self, input_size: int, cfg: \"PRMConfig\"):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size : CLS-embedding dim of the frozen LLM backbone\n",
    "            cfg        : PRMConfig instance (hidden_size, num_layers, dropout …)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        # self.output_size = cfg.output_size\n",
    "        h = cfg.hidden_size\n",
    "        p_drop = cfg.dropout\n",
    "        n_layers = cfg.num_layers\n",
    "        act_fn     = nn.GELU()\n",
    "\n",
    "         # ── first projection ────────────────────────────────────────────\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, h),\n",
    "            nn.LayerNorm(h),\n",
    "            act_fn,\n",
    "            nn.Dropout(p_drop),\n",
    "        )\n",
    "\n",
    "        # ── stacked residual blocks ─────────────────────────────────────\n",
    "        blocks = []\n",
    "        for _ in range(n_layers - 1):\n",
    "            blocks.append(\n",
    "                nn.Sequential(                   # pre-LN residual MLP\n",
    "                    nn.LayerNorm(h),\n",
    "                    nn.Linear(h, h),\n",
    "                    act_fn,\n",
    "                    nn.Dropout(p_drop),\n",
    "                    nn.Linear(h, h),\n",
    "                    nn.Dropout(p_drop),\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        # ── output head ────────────────────────────────────────────────\n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_proj(x)\n",
    "        for blk in self.blocks:\n",
    "            x = x + blk(x)          # residual connection\n",
    "        return self.out_proj(x).squeeze(-1)\n",
    "\n",
    "    def get_complexity(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('omega_prm.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    (1) entries(list[dict]) → StepwisePRMDataset\n",
    "    (2) LLM encoder + PRM head fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PRMConfig, model, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "        # ----------------------------- Backbone model LLM (frozen or fine-tuned)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model  = model\n",
    "        self.model.eval()       # LLM은 feature extractor로 freeze\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        feat_dim = self.model.config.hidden_size\n",
    "        self.prm = ProcessRewardModel(feat_dim, cfg=cfg)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.prm.to(self.device)\n",
    "\n",
    "        self.opt  = optim.AdamW(self.prm.parameters(), lr=cfg.learning_rate, weight_decay = cfg.weight_decay)\n",
    "        self.crit = nn.MSELoss()\n",
    "        # self.crit = nn.BCELoss()\n",
    "\n",
    "        self.scheduler = None\n",
    "        if cfg.lr_scheduler == \"cosine\":                    # ⭐\n",
    "            # total steps = (#batches per epoch) × epochs\n",
    "            self.total_steps = math.ceil(cfg.epochs * cfg.dataset_size / cfg.batch_size)\n",
    "            def lr_lambda(step):\n",
    "                if step < cfg.warmup_steps:\n",
    "                    return step / max(1, cfg.warmup_steps)\n",
    "                progress = (step - cfg.warmup_steps) / max(1, self.total_steps - cfg.warmup_steps)\n",
    "                return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            self.scheduler = LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "        self.ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "        self.ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.wandb_run = None\n",
    "        if cfg.use_wandb:                                  # <-- config에 플래그\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=cfg.wandb_project,                 # e.g. \"omega-prm\"\n",
    "                name=cfg.run_name,                         # e.g. \"qwen7b-prm\"\n",
    "                config=vars(cfg),                          # 모든 하이퍼파라미터 로깅\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------- features\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\n",
    "        \"\"\"\n",
    "        out = self.model(input_ids=ids, return_dict=True,output_hidden_states=True)\n",
    "        return out.hidden_states[-1][:, 0, :]     # CLS embedding\n",
    "\n",
    "    # ----------------------------------------------------------------- loop util\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool, epoch_idx: int) -> float:\n",
    "        self.prm.train(train)\n",
    "        total = 0.0\n",
    "\n",
    "        for step, (ids, reward) in enumerate(loader):\n",
    "            ids, reward = ids.to(self.device), reward.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats  = self._encode(ids)\n",
    "                pred   = self.prm(feats).squeeze(-1)\n",
    "                loss   = self.crit(pred, reward)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), self.cfg.grad_clip)\n",
    "                    self.opt.step()\n",
    "                    if self.scheduler: self.scheduler.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "            # -------- minibatch logging --------\n",
    "            if self.wandb_run and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"epoch\": epoch_idx + step / len(loader),\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                    \"grad_norm\": sum(p.grad.data.norm(2).item()\n",
    "                                     for p in self.prm.parameters()\n",
    "                                     if p.grad is not None),\n",
    "                })\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ----------------------------------------------------------------- public\n",
    "    def fit(self, train_entries: List[dict], val_entries: List[dict]) -> Dict[str, List[float]]:\n",
    "        self.cfg.dataset_size = len(train_entries) \n",
    "\n",
    "        train_ds = StepwisePRMDataset(train_entries, self.tokenizer, self.cfg.max_new_tokens, self.cfg.use_contri)\n",
    "        val_ds   = StepwisePRMDataset(val_entries,   self.tokenizer, self.cfg.max_new_tokens, self.cfg.use_contri)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "\n",
    "        history = {\"train\": [], \"val\": []}\n",
    "        best_val, bad_epoch, patience = float(\"inf\"), 0, 5\n",
    "\n",
    "        for ep in range(self.cfg.epochs):\n",
    "            tr_loss = self._run_epoch(train_loader, train=True,  epoch_idx=ep)\n",
    "            vl_loss = self._run_epoch(val_loader,   train=False, epoch_idx=ep)\n",
    "\n",
    "            history[\"train\"].append(tr_loss)\n",
    "            history[\"val\"].append(vl_loss)\n",
    "            print(f\"[Epoch {ep+1}/{self.cfg.epochs}] train={tr_loss:.4f}  val={vl_loss:.4f}\")\n",
    "\n",
    "            # -------- epoch logging --------\n",
    "            if self.wandb_run:\n",
    "                wandb.log({\"train_loss\": tr_loss,\"val_loss\": vl_loss,\"epoch\": ep})\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if vl_loss < best_val:\n",
    "                best_val = vl_loss\n",
    "                bad_epochs = 0\n",
    "                self._save_checkpoint(\"best_prm.pt\", epoch=ep, val_loss=vl_loss)\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= patience:\n",
    "                    print(f\"[Early-Stopping] no improvement for {patience} epochs\")\n",
    "                    break\n",
    "        \n",
    "        self._save_checkpoint(\"last_prm.pt\", epoch=self.cfg.epochs - 1, val_loss=vl_loss)\n",
    "        return history\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Checkpoint helpers\n",
    "    def _save_checkpoint(self, filename: str, *, epoch: int, val_loss: float) -> None:\n",
    "        path = self.ckpt_dir / filename\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"prm_state\": self.prm.state_dict(),\n",
    "            \"scheduler_state\": (self.scheduler.state_dict() if self.scheduler else None),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"config\": vars(self.cfg),              # hyper‑params for reproducibility\n",
    "            \"model_name_or_path\": getattr(self.model, \"name_or_path\", None),\n",
    "            \"tokenizer_config\": self.tokenizer.__dict__.get(\"init_kwargs\", {}),\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        print(f\"[CKPT] Saved ⇒ {path}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Simple inference helper\n",
    "    @torch.no_grad()\n",
    "    def predict_reward(self, text: str) -> float:\n",
    "        ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "        feat = self._encode(ids)\n",
    "        return float(torch.sigmoid(self.prm(feat)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load model and config!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"      # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "cfg = PRMConfig()\n",
    "print(\"Finish load model and config!\")\n",
    "\n",
    "# problems = [\n",
    "#     {\"question\": \"Each notebook costs $5. Sarah buys 4 notebooks and pays with a $50 bill. How much change does she get?\", \"gold_answer\": \"30\"},\n",
    "#     {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "#     # Add more problems as needed...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = PRMConfig()\n",
    "mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "gsm8k_raw= mcr.build_datasets_gsm8k(split=\"train\", start=10, take=2)\n",
    "\n",
    "# Print or inspect the dataset\n",
    "for entry in gsm8k_raw:\n",
    "    print(entry)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "# entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "# random.shuffle(entries_raw)\n",
    "# split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "split_idx = 1\n",
    "train_entries   = reward_ds_small[:split_idx]\n",
    "val_entries     = reward_ds_small[split_idx:] or reward_ds_small[:1]   # 최소 1개 확보\n",
    "\n",
    "trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "history = trainer.fit(train_entries, val_entries)\n",
    "print(\"Training complete. Loss history:\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "tensor(0.1250)\n",
      "Problem: Olaf collects colorful toy cars. At first, his collection consisted of 150 cars. His family, knowing his hobby, decided to give him some toy cars. Grandpa gave Olaf twice as many toy cars as the uncle. Dad gave Olaf 10 toy cars, 5 less than Mum. Auntie gave Olaf 6 toy cars, 1 more than the uncle. How many toy cars does Olaf have in total, after receiving all these gifts? Step 1: Dad gave Olaf 10 toy cars, Step 2: Mom has given Olaf 5 more toy cars than Dad, so 10 + 5 = <<10+5=15>>15 toy cars Step 3: Auntie gave Olaf 6 toy cars,\n"
     ]
    }
   ],
   "source": [
    "with open(\"gsm8k_train_0624_100.json\", \"r\") as file:\n",
    "        gsm8k_raw = json.load(file)\n",
    "\n",
    "# trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-7B\")\n",
    "cfg = PRMConfig()\n",
    "train_data = StepwisePRMDataset(gsm8k_raw, tokenizer, cfg.max_new_tokens, True)\n",
    "print(len(train_data))\n",
    "idx = 9\n",
    "print((train_data[idx][1]))\n",
    "print(tokenizer.decode(train_data[idx][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    model_name =  \"Qwen/Qwen2.5-Math-7B\" # \"Qwen/Qwen2.5-Math-7B-Instruct\"  #\"Qwen/Qwen2.5-Math-7B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    cfg = PRMConfig()\n",
    "    print(\"Finish load model and config!\")\n",
    "\n",
    "    problems = [\n",
    "        {\"question\": \"A train travels 120 km in 2 hours and then 180 km in 3 hours. What is the average speed of the train?\", \"gold_answer\": \"60\"},\n",
    "        {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "        # Add more problems as needed...\n",
    "    ]\n",
    "    \n",
    "    mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "    entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "    # Print or inspect the dataset\n",
    "    for entry in entries_raw:\n",
    "        print(entry)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    random.shuffle(entries_raw)\n",
    "    # split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "    split_idx = 1\n",
    "    train_entries   = entries_raw[:split_idx]\n",
    "    val_entries     = entries_raw[split_idx:] or entries_raw[:1]   # 최소 1개 확보\n",
    "\n",
    "    trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "    history = trainer.fit(train_entries, val_entries)\n",
    "    print(\"Training complete. Loss history:\", history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, math, argparse\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# # ---------------- 사용자 정의 유틸 ------------------------------\n",
    "# from prmtrainer import ProcessRewardModel        # PRM head class\n",
    "# from utils import _sanitize, _numeric_equiv, _extract_answer  # 앞서 만든 함수\n",
    "\n",
    "LLM_NAME      = \"Qwen/Qwen2.5-Math-7B\"\n",
    "PRM_CKPT_PATH = \"./checkpoints/gsm8k/ori_mse/best_prm.pt\"\n",
    "N_ROLLOUTS    = 5                         # Best-of-N\n",
    "MAX_NEW_TOK   = PRMConfig.max_new_tokens\n",
    "SEED          = PRMConfig.seed\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HIDDEN_DIM    = PRMConfig.hidden_size \n",
    "\n",
    "SYSTEM_PROMPT_SAMPLE = (\n",
    "        \"Problem: (sample)\\n\"\n",
    "        \"Please provide a short and precise step-by-step solution, and a numerical answer in the end, for the question above in the following format, without any extra wording:\\n\"\n",
    "        \"Step 1: (logical step 1)\\n\"\n",
    "        \"Step 2: (logical step 2)\\n\"\n",
    "        \"...\\n\"\n",
    "        \"Step n: (logical last step)\\n\"\n",
    "        \"Answer: (Final result)\"\n",
    "        \"Please strictly stick to the format above.\"\n",
    ")\n",
    "\n",
    "# Load \n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Load base model\n",
    "config = PRMConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
    "base  = AutoModelForCausalLM.from_pretrained(LLM_NAME).to(DEVICE).eval()\n",
    "for p in base.parameters():\n",
    "    p.requires_grad_(False)\n",
    "mcr = MCReward(config=config, tokenizer=tokenizer, model=base)\n",
    "\n",
    "# Load prm model\n",
    "feat_dim = base.config.hidden_size\n",
    "prm = ProcessRewardModel(feat_dim, cfg=config)\n",
    "ckpt = torch.load(PRM_CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "prm.load_state_dict(ckpt[\"prm_state\"])\n",
    "prm.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRM_CKPT_PATH = \"./checkpoints/gsm8k/contri_mse/best_prm.pt\"\n",
    "# Load prm model\n",
    "feat_dim = base.config.hidden_size\n",
    "prm = ProcessRewardModel(feat_dim, cfg=config)\n",
    "ckpt = torch.load(PRM_CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "prm.load_state_dict(ckpt[\"prm_state\"])\n",
    "prm.to(DEVICE).eval()\n",
    "\n",
    "# Load Datasets\n",
    "problems = []\n",
    "gsm8k_test = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]\n",
    "small_gsm8k = gsm8k_test.select(range(2,4))\n",
    "for obj in small_gsm8k:\n",
    "    problems.append({\"q\": obj[\"question\"], \"gold\": obj[\"answer\"]})\n",
    "print(\"Finish Loading model and dataset!\")\n",
    "\n",
    "# Evaluation utils\n",
    "@torch.no_grad()\n",
    "def generate_solutions(\n",
    "    backbone,\n",
    "    tokenizer,\n",
    "    question: str,\n",
    "    n: int,\n",
    ") -> List[str]:\n",
    "    prompt = (\n",
    "        f\"{SYSTEM_PROMPT_SAMPLE}\\n\\n\"\n",
    "        f\"Problem: {question}\\n\"\n",
    "    )\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=MAX_NEW_TOK,\n",
    "        do_sample=True, temperature=0.8, top_p=0.9,\n",
    "        num_return_sequences=n,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = backbone.generate(input_ids.repeat(n, 1), **gen_cfg.to_dict())\n",
    "    return [\n",
    "        tokenizer.decode(seq[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        for seq in out\n",
    "    ]\n",
    "\n",
    "def parse_steps(text: str) -> List[str]:\n",
    "    return [ln.strip() for ln in text.splitlines() if ln.strip().lower().startswith(\"step\")]\n",
    "\n",
    "@torch.no_grad()\n",
    "def prm_score(\n",
    "    prm: ProcessRewardModel,\n",
    "    backbone,\n",
    "    tokenizer,\n",
    "    question: str,\n",
    "    steps: List[str],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    각 prefix(Problem+Step1…i)에 대해 PRM 예측 → 평균 점수\n",
    "    점수가 클수록 '좋은' reasoning.\n",
    "    \"\"\"\n",
    "    prefix_lines = [f\"Problem: {question}\"]\n",
    "    scores = []\n",
    "    for step in steps:\n",
    "        prefix_lines.append(step)\n",
    "        txt = \"\\n\".join(prefix_lines)\n",
    "        ids = tokenizer(\n",
    "            txt,\n",
    "            max_length=384,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.to(DEVICE)\n",
    "        feats = backbone(\n",
    "            input_ids=ids,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        ).hidden_states[-1][:, 0, :]           # CLS\n",
    "        # score = prm(feats).sigmoid().item()    # 0~1 확률\n",
    "        score = prm(feats).item()\n",
    "        scores.append(score)\n",
    "    print(\"Scores by PRM:\", scores, \"\\n\")\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total   = len(problems)\n",
    "for idx, item in enumerate(problems, 1):\n",
    "    q, gold = item[\"q\"], _sanitize(item[\"gold\"])\n",
    "    # ① N개의 솔루션 생성\n",
    "    sols = generate_solutions(base, tokenizer, q, N_ROLLOUTS)\n",
    "    # ② PRM 스코어 계산\n",
    "    scored: List[Tuple[float, str]] = []\n",
    "    for sol in sols:\n",
    "        steps = parse_steps(sol)\n",
    "        if not steps:\n",
    "            continue\n",
    "        s = prm_score(prm, base, tokenizer, q, steps)\n",
    "        scored.append((s, sol))\n",
    "        print(f\"[{idx}-th solution] Step:\", steps, \"\")\n",
    "\n",
    "    if not scored:\n",
    "        pred_answer = \"N/A\"\n",
    "    else:\n",
    "        best_sol = max(scored, key=lambda t: t[0])[1]\n",
    "        pred_answer = mcr._extract_answer(text=best_sol) or \"N/A\"\n",
    "\n",
    "    if _numeric_equiv(pred_answer, gold):\n",
    "        correct += 1\n",
    "\n",
    "    print(f\"[{idx}/{total}] pred={pred_answer} | gold={gold} | {'✓' if _numeric_equiv(pred_answer, gold) else '✗'}\")\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"\\n=== GSM8K Test Accuracy (Best-of-{N_ROLLOUTS} w/ PRM) : {accuracy:.2f}% ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform, sys, subprocess, os\n",
    "print(\"PyTorch :\", torch.__version__)\n",
    "print(\"CUDA ver:\", torch.version.cuda)\n",
    "print(\"is_avail:\", torch.cuda.is_available())\n",
    "print(\"nvidia-smi output ↓\")\n",
    "subprocess.run([\"nvidia-smi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/MC_PRM/samples/math_gsm8k_200.json\") as file:\n",
    "    f1 = json.load(file)\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/MC_PRM/gsm8k_train_0703_200.json\") as file:\n",
    "    f2 = json.load(file)\n",
    "\n",
    "print(len(f1))\n",
    "print(len(f2))\n",
    "\n",
    "merged = f1 + f2\n",
    "print(len(merged))\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/MC_PRM/samples/math_gsm8k_400.json\", \"w\") as merged_file:\n",
    "    json.dump(merged, merged_file)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-7B\")\n",
    "process = StepwisePRMDataset(merged, tokenizer)\n",
    "print(len(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/QwQ-32B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      7\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,            \u001b[38;5;66;03m# enable 4-bit (QLoRA-style) weights\u001b[39;00m\n\u001b[32m      8\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,    \u001b[38;5;66;03m# NF4 gives the best accuracy for most LLMs\u001b[39;00m\n\u001b[32m      9\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# optional: second quantisation pass to save ~0.4 bits/param\u001b[39;00m\n\u001b[32m     10\u001b[39m     bnb_4bit_compute_dtype=torch.bfloat16  \u001b[38;5;66;03m# faster matmuls on recent GPUs; fall back to float16 if needed\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# let Accelerate split layers across all visible GPUs\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# keeps non-linear layers in their original dtype\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Qwen models need their custom code\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/prm/lib/python3.12/site-packages/transformers/modeling_utils.py:4286\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4284\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m4286\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4287\u001b[39m             (\n\u001b[32m   4288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4290\u001b[39m             )\n\u001b[32m   4291\u001b[39m         )\n\u001b[32m   4293\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   4294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Qwen/QwQ-32B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # enable 4-bit (QLoRA-style) weights\n",
    "    bnb_4bit_quant_type=\"nf4\",    # NF4 gives the best accuracy for most LLMs\n",
    "    bnb_4bit_use_double_quant=True, # optional: second quantisation pass to save ~0.4 bits/param\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # faster matmuls on recent GPUs; fall back to float16 if needed\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      # let Accelerate split layers across all visible GPUs\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",     # keeps non-linear layers in their original dtype\n",
    "    trust_remote_code=True  # Qwen models need their custom code\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

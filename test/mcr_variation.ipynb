{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [01:04<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "\n",
    "model_name = \"Qwen/QwQ-32B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # enable 4-bit (QLoRA-style) weights\n",
    "    bnb_4bit_quant_type=\"nf4\",    # NF4 gives the best accuracy for most LLMs\n",
    "    bnb_4bit_use_double_quant=True, # optional: second quantisation pass to save ~0.4 bits/param\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # faster matmuls on recent GPUs; fall back to float16 if needed\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      # let Accelerate split layers across all visible GPUs\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",     # keeps non-linear layers in their original dtype\n",
    "    trust_remote_code=True  # Qwen models need their custom code\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How many r's are in the word \\\"strawberry\\\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Project‑level helpers \n",
    "from utils import _sanitize, _numeric_equiv, _strip_markup, _to_float, system_prompt\n",
    "from ..prm_dataset.config import PRMConfig\n",
    "\n",
    "class MCReward:\n",
    "    STEP_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Step\\s*              # word 'Step' (case-insensitive)\n",
    "        (\\d+)                # capture step number\n",
    "        \\s*[:.\\-]            # separator (: . or -)\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    ANSWER_PATTERN = re.compile(\n",
    "        r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "            Answer               # word 'Answer'\n",
    "            \\s*[:.\\-]\\s*         # separator\n",
    "            (.+?)\\s*$            # capture everything after\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    "    )\n",
    "    ## Masked rewards ##\n",
    "    OP_TOKENS = [\"add\", \"plus\", \"sum\", \"subtract\", \"minus\",\n",
    "             \"multiply\", \"times\", \"product\", \"divide\", \"quotient\"]\n",
    "    _MASK_PATTERN = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "        # {ops_pattern}|                # operator patterns\n",
    "            \\b\\d+(?:\\.\\d+)?\\b         # integers / decimals\n",
    "          | \\b\\d+/\\d+\\b                 # simple fractions\n",
    "        #   | \\b[a-zA-Z]\\b                 # single‑letter variables\n",
    "        )\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def gsm8k_solutions(self, question: str, gold_solution: str):\n",
    "        # 1. Split lines *before* the final answer marker (#### …)\n",
    "        lines: List[str] = []\n",
    "        gold_answer: str = \"\"\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        for raw_ln in gold_solution.splitlines():\n",
    "            ln = raw_ln.strip()\n",
    "            if not ln:\n",
    "                continue  # skip empty\n",
    "            ans_match = _ANSWER_RE.match(ln)\n",
    "            if ans_match:\n",
    "                gold_answer = ans_match.group(1).strip()\n",
    "                break  # everything after #### is ignored\n",
    "            lines.append(ln)\n",
    "\n",
    "        if not gold_answer:\n",
    "            raise ValueError(\"Could not find final answer marker '#### <answer>' in gold_solution.\")\n",
    "\n",
    "        # 2. Prefix each explanatory line with \"Step i:\"\n",
    "        solution_steps = [f\"Step {i + 1}: {txt}\" for i, txt in enumerate(lines)]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"solution\": solution_steps,\n",
    "            \"gold_answer\": gold_answer,\n",
    "        }\n",
    "\n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"Split each step to start with 'Step X:' and the answer to start with 'Answer:'.\"\"\"\n",
    "        steps = []\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if self.STEP_PATTERN.match(line):\n",
    "                cleaned = re.sub(r'^[\\s>#*\\-]+', '', line)\n",
    "                steps.append(cleaned)\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(steps[: i + 1]) + \"\\n\", return_tensors=\"pt\").to(self.device) # steps up to current step i (0-indexed)\n",
    "            # Decide how to prompt the next part:\n",
    "            if i < total_steps - 1:\n",
    "                next_label = f\"Step {i + 2}:\"\n",
    "            else:\n",
    "                next_label = \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            # Build full prefix ids (avoid Python concat inefficiency by cat)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1] \n",
    "            # Check each rollout's final answer against the gold answer\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                # print(f\"[{i+1}-th Step, {idx}-th Original Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            reward = correct_count / float(self.config.num_rollouts)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Masked solution paths\n",
    "    def model_masking(self, text: str, *, max_new_tokens: int = 64) -> str:\n",
    "        prompt = \"In the sentence below, mask any word or expression that seems crucial for solving the math step. This may include key numbers, variables, or action words (like operations), but you should decide what matters. Replace each important item with '[MASKED]'. Keep everything else unchanged. Return ONE line.\\n\\nSentence: \\\"{sent}\\\"\\nRewritten:\".format(sent=text)\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        out_ids   = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.2, top_p=0.2,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out_ids[0][input_ids.shape[-1]:],\n",
    "                                     skip_special_tokens=True).strip()\n",
    "\n",
    "    def perturbed_step_rewards(self, question: str, sys_prompt: str, steps: List[str], gold_answer: str, use_llm: bool = True) -> List[float]:\n",
    "        \"\"\"Compute MC correctness rates *after masking* the current step.\n",
    "        Each step `i` is replaced with a *perturbed* version where important tokens (numbers, fractions, single‑letter variables) are substituted by the literal string ``[MASKED]``. All preceding steps remain intact.\n",
    "        \"\"\"\n",
    "        ptb_rewards: List[float] = []\n",
    "        total_steps = len(steps)\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            # 1. Perturb *only* step i\n",
    "            orig_step = steps[i] \n",
    "            step_match = re.match(r\"^[\\s>#*\\-]*Step\\s*\\d+\\s*[:.\\-]\\s*\", orig_step, flags=re.I)\n",
    "            prefix = step_match.group(0) if step_match else \"\"\n",
    "            # ② 나머지 부분(body)만 마스킹\n",
    "            body   = steps[i][len(prefix):]                       # 접두사 뒷부분\n",
    "            if use_llm:\n",
    "                masked_body = self.model_masking(body)\n",
    "            else:\n",
    "                masked_body = self._MASK_PATTERN.sub(\"[MASKED]\", body)\n",
    "            # ③ 접두사 + 마스킹된 body\n",
    "            masked_step = prefix + masked_body    \n",
    "            ptb_prefix_steps = steps[:i] + [masked_step]\n",
    "            # print(\"perturbed step:\", ptb_prefix_steps)\n",
    "\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(ptb_prefix_steps) + \"\\n\", return_tensors=\"pt\").to(self.device)\n",
    "            next_label = f\"Step {i + 2}:\" if i < total_steps - 1 else \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1]\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                # print(f\"Masked [{i+1}-th Step, {idx}-th Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            ptb_rewards.append(correct_count / float(self.config.num_rollouts))\n",
    "        return ptb_rewards\n",
    "\n",
    "    def _generate_rollouts(self, prompt: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        vLLM 에서 동일 프롬프트를 n번(=num_rollouts) 샘플링해서 텍스트만 반환\n",
    "        \"\"\"\n",
    "        outs = self.llm.generate([prompt], self.sparams)   # 배치 길이 1\n",
    "        return [o.outputs[ri].text for o in outs for ri in range(len(o.outputs))]\n",
    "\n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets_gsm8k(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        rollout_pr = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
    "        if take is not None:\n",
    "            ds = ds.shuffle(seed=self.config.seed).select(range(start, start+take))\n",
    "        else:\n",
    "            ds = ds.shuffle(seed=self.config.seed).select(range(start, len(ds)))\n",
    "\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "        \n",
    "        for sample in tqdm(ds, desc=\"Building GSM-8K reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            q_txt   = sample[\"question\"]\n",
    "            g_sol   = sample[\"answer\"]\n",
    "            lines, gold_ans = [], None\n",
    "            for ln in g_sol.splitlines():\n",
    "                ln = ln.strip()\n",
    "                if not ln:\n",
    "                    continue\n",
    "                m = _ANSWER_RE.match(ln)\n",
    "                if m:\n",
    "                    gold_ans = sanitize(m.group(1))\n",
    "                    break\n",
    "                lines.append(ln)\n",
    "            if gold_ans is None:\n",
    "                raise ValueError(\"gold answer not found for sample\")\n",
    "            steps = [f\"Step {i+1}: {t}\" for i, t in enumerate(lines)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(q_txt, rollout_pr, steps, gold_ans)\n",
    "            ptb = psr(q_txt, rollout_pr, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            #  ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                    \"question\":      q_txt,\n",
    "                    \"completion\":    steps,\n",
    "                    \"ori_rewards\":   ori,\n",
    "                    \"ptb_rewards\":   ptb,\n",
    "                    \"contributions\": contrib,\n",
    "                    \"answer\":        gold_ans,\n",
    "                    \"gold_answer\":   gold_ans,\n",
    "                }\n",
    "            dataset.append(entry)\n",
    "            # print(entry)\n",
    "        return dataset\n",
    "\n",
    "    def build_datasets_math(self, *, split: str = \"train\", start: int = 0, take: int | None):\n",
    "        boxed_re   = re.compile(r'\\\\boxed\\{(.+?)\\}', re.S)\n",
    "        sent_split = re.compile(r'\\.(?!\\d)(?=\\s|$)')   # 소수점·수식 내부 마침표 무시\n",
    "\n",
    "        rollout_prompt = system_prompt(\"rollout\")\n",
    "        ds = load_dataset(\"HuggingFaceTB/MATH\", \"all\", split=split)\n",
    "\n",
    "        # shuffle & take\n",
    "        if take is not None:\n",
    "            ds = ds.select(range(start, start+take))\n",
    "        else:\n",
    "            ds = ds.select(range(start, len(ds)))\n",
    "\n",
    "        # (alias) time optimize\n",
    "        csr, psr   = self.compute_step_rewards, self.perturbed_step_rewards\n",
    "        sanitize   = _sanitize\n",
    "        use_llm    = self.config.use_llm\n",
    "        dataset    = []\n",
    "\n",
    "        for sample in tqdm(ds, desc=\"Building MATH reward-dataset\"):\n",
    "            # ── (1) extract step solutions ──────────────────────────────────────────\n",
    "            full_sol   = sample[\"solution\"]\n",
    "            m          = boxed_re.search(full_sol)\n",
    "            gold_ans   = sanitize(m.group(1)) if m else None\n",
    "            sol_wo_box = boxed_re.sub(\"\", full_sol)\n",
    "            raw_steps  = [s.strip() for s in sent_split.split(sol_wo_box) if s.strip()]\n",
    "            steps      = [f\"Step {i+1}: {s}\" for i, s in enumerate(raw_steps)]\n",
    "\n",
    "            # ── (2) compute rewards ───────────────────────────────────────────────────\n",
    "            ori = csr(sample[\"problem\"], rollout_prompt, steps, gold_ans)\n",
    "            ptb = psr(sample[\"problem\"], rollout_prompt, steps, gold_ans, use_llm)\n",
    "            if len(ptb) != len(ori):\n",
    "                ptb = ptb[: len(ori)]\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            # ── (3) Append entry ───────────────────────────────────────────\n",
    "            entry = {\n",
    "                \"question\":      sample[\"problem\"],\n",
    "                \"completion\":    steps,\n",
    "                \"ori_rewards\":   ori,\n",
    "                \"ptb_rewards\":   ptb,\n",
    "                \"contributions\": contrib,\n",
    "                \"answer\":        gold_ans,\n",
    "                \"gold_answer\":   gold_ans,\n",
    "                \"level\":         sample[\"level\"],\n",
    "                \"type\":          sample[\"type\"],\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "            print(entry)\n",
    "        return dataset\n",
    "\n",
    "    def _sequence_nll(self, prompt: str, target: str) -> float:\n",
    "        with torch.no_grad():\n",
    "            full = prompt + target\n",
    "            inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "            prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "            \n",
    "            # Mask out the prompt tokens so loss is only on the target\n",
    "            labels = inputs[\"input_ids\"].clone()\n",
    "            labels[:, :prompt_len] = -100          # ignore index\n",
    "            logits = model(**inputs).logits\n",
    "            loss   = F.cross_entropy(\n",
    "                        logits.view(-1, logits.size(-1)),\n",
    "                        labels.view(-1),\n",
    "                        reduction=\"none\"\n",
    "                    )\n",
    "            # keep only target positions\n",
    "            target_loss = loss[labels.view(-1) != -100]\n",
    "            return (target_loss.sum() / torch.log(torch.tensor(2.0))).item()  # bits\n",
    "        \n",
    "    def _info_gain(self, context, step, answer):\n",
    "        no_step = self._sequence_nll(context, answer)\n",
    "        with_step = self._sequence_nll(context + step, answer)\n",
    "        return no_step - with_step\n",
    "\n",
    "    def _step_entropy(self, context, step):\n",
    "        \"\"\"Cross-entropy of a step sequence (bits).\"\"\"\n",
    "        return self._sequence_nll(context, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "# MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"      # or your qwen2.5-math-7b path\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def sequence_nll_ver1(prompt: str, target: str) -> float:\n",
    "    \"\"\"Cross-entropy (in bits) of 'target' tokens given the 'prompt'.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        prefix, value = target.split(\"Answer:\")\n",
    "        ans_prefix, ans_value = \"Answer: \", value.lstrip()\n",
    "        prompt = prompt + ans_prefix\n",
    "        full = prompt + ans_value\n",
    "\n",
    "        # full = prompt + target\n",
    "        inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "        prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "        \n",
    "        # Mask out the prompt tokens so loss is only on the target\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[:, :prompt_len] = -100          # ignore index\n",
    "        \n",
    "        # 디버그: 정답 부분만 확인\n",
    "        answer_part = labels[0, prompt_len:]\n",
    "        valid_tokens = answer_part[answer_part != -100]\n",
    "        print(f\"Answer tokens: {tokenizer.decode(valid_tokens)}\")\n",
    "        print(f\"Answer token count: {len(valid_tokens)}\")\n",
    "        \n",
    "        logits = model(**inputs).logits\n",
    "        loss   = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    labels.view(-1),\n",
    "                    reduction=\"none\"\n",
    "                 )\n",
    "        # keep only target positions\n",
    "        target_loss = loss[labels.view(-1) != -100]\n",
    "        print(f\"Per-token losses: {target_loss.tolist()}\")\n",
    "\n",
    "        for i, tok_id in enumerate(valid_tokens):\n",
    "            tok = tokenizer.decode([tok_id])\n",
    "            prob = torch.softmax(logits[0, -len(valid_tokens)+i], dim=-1)[tok_id].item()\n",
    "            print(f\"{tok!r}  p={prob:.3e},  -ln p={-math.log(prob):.3f}\")\n",
    "\n",
    "        return (target_loss.sum() / torch.log(torch.tensor(2.0))).item()  # bits\n",
    "\n",
    "def sequence_nll(prompt: str, target: str):\n",
    "    \"\"\"NLL(bits) of `target` given `prompt`. The prompt **does not** contain the target portion.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        prefix, value = target.split(\"Answer:\")\n",
    "        ans_prefix, ans_value = \"Answer: \", value.lstrip()\n",
    "        prompt = prompt + ans_prefix\n",
    "        full = prompt + ans_value\n",
    "        # full = prompt + target\n",
    "        full_ids    = tokenizer(full, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "        prompt_len  = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "        labels = full_ids.clone()\n",
    "        labels[:, :prompt_len] = -100          # ignore prompt tokens\n",
    "\n",
    "        logits = model(full_ids).logits\n",
    "        loss   = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    labels.view(-1),\n",
    "                    reduction=\"sum\"             # total bits, not mean\n",
    "                 ) / math.log(2)                # nats → bits\n",
    "        return loss.item()\n",
    "\n",
    "def info_gain(context, step, answer):\n",
    "    \"\"\"I(S;A|c) = H(A|c) - H(A|c,S).\"\"\"\n",
    "    no_step = sequence_nll(context, answer)\n",
    "    with_step = sequence_nll(context + step, answer)\n",
    "    return no_step - with_step\n",
    "\n",
    "def step_entropy(context, step):\n",
    "    \"\"\"Cross-entropy of a step sequence (bits).\"\"\"\n",
    "    return sequence_nll(context, step)\n",
    "\n",
    "def show_topk(prompt, k=5):\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids).logits[0, -1]\n",
    "    probs, idx = torch.topk(torch.softmax(logits, dim=-1), k)\n",
    "    print([ (tokenizer.decode([i]), float(p)) for p,i in zip(probs, idx) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Answer|context) ≈ 25.375 bits\n",
      "H(Answer|context+Step1) ≈ 28.5 bits\n",
      "H(Answer|context+Step1+Step2) ≈ 30.875 bits\n",
      "H(Answer|context+Step1+Step2+Step3) ≈ 25.25 bits\n",
      "Information gain of Step1: -3.125 bits\n",
      "Information gain of Step2: -2.375 bits\n",
      "Information gain of Step3: 5.625 bits\n"
     ]
    }
   ],
   "source": [
    "# context = \"\"\"Solve the given problem with step by step reasoning and write final answer in the format of \"Answer: <answer>\". Problem: What is the sum of the digits of the number 84?\\n\"\"\"\n",
    "# step1   = \"Step 1: The tens digit of 84 is 8.\\n\"\n",
    "# step2   = \"Step 2: The ones digit of 84 is 4.\\n\"\n",
    "# step3   = \"Step 3: Add the digits: 8 + 4 = 12.\\n\"\n",
    "# answer  = \"\\nAnswer:\\n12\"\n",
    "\n",
    "context = \"\"\"Solve the given problem with step by step reasoning and write final answer in the format of \"Answer: <answer>\". Problem: What is (5 + 3) × 2 - 4?\\n\"\"\"\n",
    "step1   = \"Step 1: Compute inside the parentheses first: 5 + 3 = 8.\\n\"\n",
    "step2   = \"Step 2: Multiply the result by 2: 8 × 2 = 16.\\n\"\n",
    "step3   = \"Step 3: Subtract 4: 16 - 4 = 12.\\n\"\n",
    "# step3   = \"Step 3: What should I do next?\\n\"\n",
    "answer  = \"Answer: 12\"\n",
    "\n",
    "# context = \"\"\"Solve the given problem with step by step reasoning and write final answer in the format of \"Answer: <answer>\". Problem: What is 1/2 + 1/4 + 1/4?\\n\"\"\"\n",
    "# step1   = \"Step 1: Add 1/4 and 1/4 first: 1/4 + 1/4 = 1/2.\\n\"\n",
    "# step2   = \"Step 2: Now add 1/2 + 1/2 = 1.\\n\"\n",
    "# step3   = \"Step 3: Final result is 1.\\n\"\n",
    "# answer  = \"Answer: 1\"\n",
    "\n",
    "# print(\"Answer tokenized:\", tokenizer.tokenize(answer))\n",
    "# print(\"Answer IDs:\", tokenizer.encode(answer, add_special_tokens=False))\n",
    "# show_topk(context)                  # before any step\n",
    "# show_topk(context + step1)          # after Step 1\n",
    "# show_topk(context + step1 + step2)  # etc.\n",
    "# show_topk(context + step1 + step2 + step3) \n",
    "\n",
    "print(\"H(Answer|context) ≈\", step_entropy(context, answer), \"bits\")\n",
    "print(\"H(Answer|context+Step1) ≈\", step_entropy(context + step1, answer), \"bits\")\n",
    "print(\"H(Answer|context+Step1+Step2) ≈\", step_entropy(context + step1 + step2, answer), \"bits\")\n",
    "print(\"H(Answer|context+Step1+Step2+Step3) ≈\", step_entropy(context + step1 + step2 + step3, answer), \"bits\")\n",
    "print(\"Information gain of Step1:\", info_gain(context, step1, answer), \"bits\")\n",
    "print(\"Information gain of Step2:\", info_gain(context + step1, step2, answer), \"bits\")\n",
    "print(\"Information gain of Step3:\", info_gain(context + step1 + step2, step3, answer), \"bits\")\n",
    "\n",
    "# prompt_ids = tokenizer(context + step1 + step2, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "# out = model.generate(\n",
    "#         prompt_ids,\n",
    "#         max_new_tokens=30,\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         temperature=0.2,     # deterministic\n",
    "#         do_sample=True\n",
    "#      )\n",
    "\n",
    "# gen_ids   = out.sequences[0, prompt_ids.size(-1):]  # 생성된 부분\n",
    "# scores    = out.scores                              # 길이 = #gen_tokens\n",
    "\n",
    "# # 토큰별 −log2 p 계산\n",
    "# nll_bits = 0.0\n",
    "# for t, (logits, tok_id) in enumerate(zip(scores, gen_ids)):\n",
    "#     probs = logits.squeeze(0).softmax(dim=-1)\n",
    "#     tok_id = tok_id.item()\n",
    "#     p     = probs[tok_id].item()\n",
    "#     nll_bits += -math.log2(p)\n",
    "#     print(f\"{t:02d}  {tokenizer.decode([tok_id])}  p={p:.3e}  −log2 p={-math.log2(p):.3f}\")\n",
    "\n",
    "# print(f\"Total NLL(bits) for generated segment = {nll_bits:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(A|c)        = 16.8074 bits\n",
      "H(A|c,S1)     = 20.7748 bits   →  I(S1;A|c) = -3.9674 bits\n",
      "H(A|c,S1,S2)  = 16.3746 bits   →  I(S2;A|c,S1) = 4.4002 bits (increment)\n",
      "H(A|c,S1,S2,S3)= 18.2501 bits   →  I(S3;A|c,S1,S2)= -1.8755 bits\n"
     ]
    }
   ],
   "source": [
    "def nll_bits(prompt: str, target: str) -> float:\n",
    "    \"\"\"Average NLL of 'target' given 'prompt', in **bits** per target token.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        full   = prompt + target\n",
    "        inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "        Lp     = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[:, :Lp] = -100          # ignore prompt tokens\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            reduction=\"sum\"            # total nll (nats)\n",
    "        )\n",
    "        ntoks = (labels != -100).sum()\n",
    "        nll_nats = loss.item() / ntoks\n",
    "        return nll_nats / math.log(2)  # nats → bits\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "context = \"Solve the given problem with step by step reasoning and write final answer in the format of \\\"Answer: <answer>\\\". Problem: What is (5 + 3) × 2 - 4?\\n\"\n",
    "step1   = \"Step 1: Compute inside the parentheses first: 5 + 3 = 8.\\n\"\n",
    "step2   = \"Step 2: Multiply the result by 2: 8 × 2 = 10.\\n\"\n",
    "step3   = \"Step 3: Subtract 4: 10 - 4 = 6.\\n\"\n",
    "answer  = \"Answer: 12\"\n",
    "\n",
    "# 1) 엔트로피(=평균 NLL) 계산\n",
    "nll0 = nll_bits(context, answer)\n",
    "nll1 = nll_bits(context + step1,                 answer)\n",
    "nll2 = nll_bits(context + step1 + step2,         answer)\n",
    "nll3 = nll_bits(context + step1 + step2 + step3, answer)\n",
    "\n",
    "# 2) mutual information\n",
    "mi1 = nll0 - nll1\n",
    "mi2 = nll1 - nll2\n",
    "mi3 = nll2 - nll3\n",
    "\n",
    "print(f\"H(A|c)        = {nll0:.4f} bits\")\n",
    "print(f\"H(A|c,S1)     = {nll1:.4f} bits   →  I(S1;A|c) = {mi1:.4f} bits\")\n",
    "print(f\"H(A|c,S1,S2)  = {nll2:.4f} bits   →  I(S2;A|c,S1) = {mi2:.4f} bits (increment)\")\n",
    "print(f\"H(A|c,S1,S2,S3)= {nll3:.4f} bits   →  I(S3;A|c,S1,S2)= {mi3:.4f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Monte-Carlo (k=5) ===\n",
      "H(A|c)          = 0.6414 bits\n",
      "H(A|c,S1)       = 0.0133 bits   →  I(S1;A|c)          = +0.6282\n",
      "H(A|c,S1,S2)    = 0.4116 bits   →  ΔI(S2|prev)        = -0.3983\n",
      "H(A|c,S1,S2,S3) = 0.1660 bits   →  ΔI(S3|prev)        = +0.2455\n",
      "\n",
      "=== Exact token-entropy ===\n",
      "H(A|c)          = 1.2748 bits\n",
      "H(A|c,S1)       = 0.7583 bits   →  I(S1;A|c)          = +0.5165\n",
      "H(A|c,S1,S2)    = 0.7090 bits   →  ΔI(S2|prev)        = +0.0492\n",
      "H(A|c,S1,S2,S3) = 0.5144 bits   →  ΔI(S3|prev)        = +0.1946\n"
     ]
    }
   ],
   "source": [
    "import math, torch, random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LOG2E = 1 / math.log(2)\n",
    "\n",
    "# ─────────────────────── 1. 기본 NLL 함수 ─────────────────────\n",
    "def nll_bits(prompt: str, target: str, avg=True) -> float:\n",
    "    \"\"\"\n",
    "    NLL(prompt→target) in bits.\n",
    "    If avg=True, return *average* bits / target-token,\n",
    "    else return *total* bits of the sequence.\n",
    "    \"\"\"\n",
    "    full   = prompt + target\n",
    "    inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "    Lp     = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[:, :Lp] = -100\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            reduction=\"sum\"          # total nats\n",
    "        )\n",
    "    ntoks = (labels != -100).sum()\n",
    "    bits  = loss.item() * LOG2E      # total bits\n",
    "    return bits / ntoks if avg else bits\n",
    "\n",
    "# ─────────────────────── 2-A. MC Sampling 방식 ─────────────────────\n",
    "def entropy_bits_mc(prompt: str, k: int = 5, max_new: int = 4, temperature: float = 0.7, top_p: float = 0.9) -> float:\n",
    "    \"\"\"\n",
    "    Monte-Carlo estimate of H(A|prompt) [bits per token].\n",
    "    Generates k continuations, then 평균[-log₂ p(sample | prompt)].\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "    BITS = []\n",
    "    for _ in range(k):\n",
    "        out_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )[0][input_ids.size(1):]           # strip prompt\n",
    "\n",
    "        sample = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "        BITS.append(entropy_bits_exact(prompt, sample))\n",
    "    return sum(BITS) / k                  # bits/token\n",
    "\n",
    "# ─────────────────────── 2-B. Exact 토큰-엔트로피 ─────────────────────\n",
    "def entropy_bits_exact(prompt: str, target: str) -> float:\n",
    "    \"\"\"True H(A|prompt) in bits/token, by ∑_t H(p_t). Memory-intensive: stores full probs tensor.\"\"\"\n",
    "    full   = prompt + target\n",
    "    inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "    Lp     = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.float()      # [1,L,V]\n",
    "\n",
    "    probs = logits.softmax(-1)                      # [...,V]\n",
    "    token_H = -(probs * probs.log()).sum(-1) * LOG2E  # bits/token\n",
    "\n",
    "    mask = torch.zeros_like(inputs[\"input_ids\"], dtype=torch.bool)\n",
    "    mask[:, Lp:] = True                             # answer tokens\n",
    "    return token_H[mask].sum().item() / mask.sum().item()\n",
    "\n",
    "# ─────────────────────── 3. 프롬프트 정의 ─────────────────────\n",
    "context = \"\"\"Solve the given problem with step by step reasoning in the format of \"Step k: <k-th rationale>\" and write final answer in the format of \"Answer: <answer>\". Problem: What is the sum of the digits of the number 84?\\n\"\"\"\n",
    "step1   = \"Step 1: The tens digit of 84 is 8.\\n\"\n",
    "step2   = \"Step 2: The ones digit of 84 is 4.\\n\"\n",
    "step3   = \"Step 3: Add the digits: 8 + 4 = 12.\\n\"\n",
    "answer  = \"Answer: 12\"\n",
    "\n",
    "# ─────────────────────── 4. 엔트로피 & MI 계산 ─────────────────────\n",
    "def mi_report(entropy_fn, label: str):\n",
    "    H0 = entropy_fn(context,                  answer)            # H(A|c)\n",
    "    H1 = entropy_fn(context+step1,            answer)            # H(A|c,S1)\n",
    "    H2 = entropy_fn(context+step1+step2,      answer)            # H(A|c,S1,S2)\n",
    "    H3 = entropy_fn(context+step1+step2+step3,answer)            # ...\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    print(f\"H(A|c)          = {H0:.4f} bits\")\n",
    "    print(f\"H(A|c,S1)       = {H1:.4f} bits   →  I(S1;A|c)          = {H0-H1:+.4f}\")\n",
    "    print(f\"H(A|c,S1,S2)    = {H2:.4f} bits   →  ΔI(S2|prev)        = {H1-H2:+.4f}\")\n",
    "    print(f\"H(A|c,S1,S2,S3) = {H3:.4f} bits   →  ΔI(S3|prev)        = {H2-H3:+.4f}\")\n",
    "\n",
    "# 4-A. MC 샘플링 (k 줄이려면 30~50 도 OK)\n",
    "mi_report(lambda p,t: entropy_bits_mc(p), \"Monte-Carlo (k=5)\")\n",
    "\n",
    "# 4-B. Exact  (target이 2-토큰이므로 메모리 부담 ↓)\n",
    "mi_report(entropy_bits_exact, \"Exact token-entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: I want to find the largest positive integer that divides both $20 !$ and $200,\\!000$ evenly.\n",
      "Step 2: One way to do this is to factor both numbers into prime factors and look for the common ones.\n",
      "Step 3: I know that $200,\\!000 = 2^5\\cdot 10^4 = 2^9\\cdot 5^4$.\n",
      "Step 4: To find the prime factorization of $20 !$, I can use the fact that it is the product of all the positive integers from $1$ to $20$.\n",
      "Step 5: For each prime number $p$ between $1$ and $20$, I can count how many multiples of $p$ are in that range.\n",
      "Step 6: For example, there are $10$ multiples of $2$ between $1$ and $20$, namely $2, 4, 6, \\dots, 20$.\n",
      "Step 7: But there are also $5$ multiples of $4$, which is $2^2$, and $2$ multiples of $8$, which is $2^3$, and $1$ multiple of $16$, which is $2^4$.\n",
      "Step 8: So, the total power of $2$ in $20 !$ is $10 + 5 + 2 + 1 = 18$.\n",
      "Step 9: Similarly, there are $4$ multiples of $5$, namely $5, 10, 15, 20$, so the power of $5$ in $20 !$ is $4$.\n",
      "Step 10: There are $6$ multiples of $3$, namely $3, 6, 9, \\dots, 18$, but there are also $2$ multiples of $9$, which is $3^2$, so the power of $3$ in $20 !$ is $6 + 2 = 8$.\n",
      "Step 11: There are $2$ multiples of $7$, namely $7$ and $14$, so the power of $7$ in $20 !$ is $2$.\n",
      "Step 12: There are $1$ multiple of each of the other prime numbers $11, 13, 17$, and $19$, so the powers of those primes in $20 !$ are $1$ each.\n",
      "Step 13: Therefore, the prime factorization of $20 !$ is $2^{18}\\cdot 3^8\\cdot 5^4\\cdot 7^2\\cdot 11\\cdot 13\\cdot 17\\cdot 19$.\n",
      "Step 14: To find the greatest common factor of $20 !$ and $200,\\!000$, I need to take the lowest power of each common prime factor.\n",
      "Step 15: The only common prime factors are $2$ and $5$, and the lowest powers are $9$ and $4$, respectively.\n",
      "Step 16: So, the greatest common factor is $2^9\\cdot 5^4 = 512\\cdot 625 = 320,\\!000$.\n",
      "\n",
      "# Answer\n",
      "\n",
      "320,000\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Solve the given problem with step by step reasoning and write final answer in the format of \"Answer: <answer>\". Probelm: What is the greatest common factor of $20 !$ and $200,\\\\!000$?  (Reminder: If $n$ is a positive integer, then $n!$ stands for the product $1\\\\cdot 2\\\\cdot 3\\\\cdot \\\\cdots \\\\cdot (n-1)\\\\cdot n$.)\"\"\"\n",
    "steps = [\n",
    "      \"I want to find the largest positive integer that divides both $20 !$ and $200,\\\\!000$ evenly.\",\n",
    "      \"One way to do this is to factor both numbers into prime factors and look for the common ones.\",\n",
    "      \"I know that $200,\\\\!000 = 2^5\\\\cdot 10^4 = 2^9\\\\cdot 5^4$.\",\n",
    "      \"To find the prime factorization of $20 !$, I can use the fact that it is the product of all the positive integers from $1$ to $20$.\",\n",
    "      \"For each prime number $p$ between $1$ and $20$, I can count how many multiples of $p$ are in that range.\",\n",
    "      \"For example, there are $10$ multiples of $2$ between $1$ and $20$, namely $2, 4, 6, \\\\dots, 20$.\",\n",
    "      \"But there are also $5$ multiples of $4$, which is $2^2$, and $2$ multiples of $8$, which is $2^3$, and $1$ multiple of $16$, which is $2^4$.\",\n",
    "      \"So, the total power of $2$ in $20 !$ is $10 + 5 + 2 + 1 = 18$.\",\n",
    "      \"Similarly, there are $4$ multiples of $5$, namely $5, 10, 15, 20$, so the power of $5$ in $20 !$ is $4$.\",\n",
    "      \"There are $6$ multiples of $3$, namely $3, 6, 9, \\\\dots, 18$, but there are also $2$ multiples of $9$, which is $3^2$, so the power of $3$ in $20 !$ is $6 + 2 = 8$.\",\n",
    "      \"There are $2$ multiples of $7$, namely $7$ and $14$, so the power of $7$ in $20 !$ is $2$.\",\n",
    "      \"There are $1$ multiple of each of the other prime numbers $11, 13, 17$, and $19$, so the powers of those primes in $20 !$ are $1$ each.\",\n",
    "      \"Therefore, the prime factorization of $20 !$ is $2^{18}\\\\cdot 3^8\\\\cdot 5^4\\\\cdot 7^2\\\\cdot 11\\\\cdot 13\\\\cdot 17\\\\cdot 19$.\",\n",
    "      \"To find the greatest common factor of $20 !$ and $200,\\\\!000$, I need to take the lowest power of each common prime factor.\",\n",
    "      \"The only common prime factors are $2$ and $5$, and the lowest powers are $9$ and $4$, respectively.\",\n",
    "      \"So, the greatest common factor is $2^9\\\\cdot 5^4 = 512\\\\cdot 625 = 320,\\\\!000$.\\n\\n# Answer\\n\\n320,000\"\n",
    "    ]\n",
    "\n",
    "num_steps = []\n",
    "for i, step in enumerate(steps):\n",
    "    numbering = f\"Step {i+1}: \" + step\n",
    "    num_steps.append(numbering)\n",
    "\n",
    "for idx in range(len(num_steps)):\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOG2E = torch.log2(torch.tensor(2.718281828459045))\n",
    "\n",
    "def next_token_probs(prompt: str, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"softmax(logits/τ) over the full vocab, shape (|V|,)\"\"\"\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids).logits[0, -1] / temperature\n",
    "    return torch.softmax(logits.float(), dim=-1)          # full-vocab probs\n",
    "\n",
    "def info_gain_kl(context: str, step: str, temperature: float = 1.0) -> float:\n",
    "    \"\"\" I≈KL( P(·|c,step) || P(·|c) ) measured on the distribution of the *first* next token (Answer: 직후). 결과 단위: bits \"\"\"\n",
    "    p = next_token_probs(context, temperature)\n",
    "    q = next_token_probs(context + step, temperature)\n",
    "\n",
    "    kl_nat = torch.sum(q * (q.log() - p.log()))           # nats\n",
    "    kl_bits = (kl_nat / _LOG2E).item()                    # convert nats→bits\n",
    "    return kl_bits                                        # ≥ 0 by definition\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Monte-Carlo 샘플링 기반 정보 이득  (엔트로피 근사)\n",
    "# ---------------------------------------------------------------------\n",
    "def sequence_nll(prompt: str, target: str) -> float:\n",
    "    \"\"\" Cross-entropy (bits) of `target` given `prompt`.\"\"\"\n",
    "    full = prompt + target\n",
    "    inputs = tokenizer(full, return_tensors=\"pt\").to(device)\n",
    "    p_len  = len(tokenizer(prompt)[\"input_ids\"])\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[:, :p_len] = -100                      # ignore prompt tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        loss   = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction=\"none\")\n",
    "    target_loss = loss[labels.view(-1) != -100]\n",
    "    return (target_loss.sum() / torch.log(torch.tensor(2.0))).item()  # bits\n",
    "\n",
    "def sample_answers(prompt: str,\n",
    "                   k: int = 8,\n",
    "                   max_new_tokens: int = 56,\n",
    "                   temperature: float = 0.8) -> list[str]:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=k,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "              )\n",
    "    # 잘라서 답변 부분만 디코딩\n",
    "    answer_tokens = outs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.batch_decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "def entropy_mc(prompt: str, k: int = 16, max_new_tokens: int = 32, temperature: float = 0.8) -> float:\n",
    "    \"\"\"Monte-Carlo estimate of H(A|prompt) in bits.\"\"\"\n",
    "    samples = sample_answers(prompt, k, max_new_tokens, temperature)\n",
    "    nlls = [sequence_nll(prompt, ans) for ans in samples]\n",
    "    return sum(nlls) / len(nlls)\n",
    "\n",
    "def info_gain_mc(context: str, step: str, k: int = 16, max_new_tokens: int = 32, temperature: float = 0.8) -> float:\n",
    "    \"\"\" I ≈ Ĥ(A|c) – Ĥ(A|c,step) via MC sampling. 단위 bits (양수가 정보 이득, 음수면 정보 손실) \"\"\"\n",
    "    h_no  = entropy_mc(context, k, max_new_tokens, temperature)\n",
    "    h_yes = entropy_mc(context + step, k, max_new_tokens, temperature)\n",
    "    return h_no - h_yes\n",
    "\n",
    "def answer_token(prompt: str) -> int:\n",
    "    \"\"\"\n",
    "    returns token id of the *first* token the model produces right after 'Answer:'.\n",
    "    여기선 숫자 한 글자(예: '12' → 'Ġ12' 토큰)라고 가정.\n",
    "    \"\"\"\n",
    "    # 'Answer:' 까지 디코딩 후 generate 1토큰\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(ids, max_new_tokens=1, do_sample=False)   # greedy\n",
    "    return int(out[0, -1])\n",
    "\n",
    "def info_gain_answer_kl(context: str,step: str, correct_answer: str, temperature: float = 1.0) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    (1) KL over full vocab  (≥0)\n",
    "    (2) Δ log2-prob of *gold answer token*  (= contribution sign)\n",
    "    \"\"\"\n",
    "    # 준비: Answer:  프롬프트\n",
    "    base = context + \"Answer:\"\n",
    "    base_with = context + step + \"Answer:\"\n",
    "\n",
    "    # 전체 분포\n",
    "    p = next_token_probs(base, temperature)      # (|V|)\n",
    "    q = next_token_probs(base_with, temperature)\n",
    "\n",
    "    # 1) KL  (nats→bits)\n",
    "    kl_bits = torch.sum(q * (q.log() - p.log())) / _LOG2E\n",
    "\n",
    "    # 2) 정답 토큰 확률 변화\n",
    "    # gold_id = answer_token(base)                 # 모델이 '정답'이라고 보는 토큰\n",
    "    # delta_log2 = (q[gold_id].log2() - p[gold_id].log2()).item()\n",
    "\n",
    "    gold_id = tokenizer(correct_answer, add_special_tokens=False)[\"input_ids\"][0]\n",
    "    delta_log2 = (q[gold_id].log2() - p[gold_id].log2()).item()\n",
    "\n",
    "    return kl_bits.item(), delta_log2\n",
    "\n",
    "\n",
    "def kl_and_delta_nll(context: str, step: str, answer: str, temperature: float = 1.0) -> tuple[float, float]:\n",
    "    \"\"\"Returns (KL_bits_full_answer , Δ-NLL_bits_full_answer)\n",
    "    • KL = Σ_t  KL( p_with(·|prefix_t) || p_no(·|prefix_t) ) ≥ 0\n",
    "    • Δ-NLL =  NLL_no − NLL_with   (정답 확률 ↑ → 양수)\"\"\"\n",
    "    # pre-encode once to speed up\n",
    "    prompt_no   = context + \"Answer:\"\n",
    "    prompt_with = context + step + \"Answer:\"\n",
    "    ans_ids     = tokenizer(answer, return_tensors=\"pt\").to(device)[\"input_ids\"][0]     # (T,)\n",
    "\n",
    "    # holders\n",
    "    total_kl_nat   = 0.0\n",
    "    total_dlog2    = 0.0            # Δ-log2P over the whole answer\n",
    "\n",
    "    # build running prefixes for with / no prompts\n",
    "    ids_no   = tokenizer(prompt_no,   return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "    ids_with = tokenizer(prompt_with, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "\n",
    "    for next_id in ans_ids:          # iterate over answer tokens\n",
    "        with torch.no_grad():\n",
    "            # logits for next token distribution\n",
    "            logit_no   = model(ids_no).logits[0, -1] / temperature\n",
    "            logit_with = model(ids_with).logits[0, -1] / temperature\n",
    "\n",
    "        p_with = torch.softmax(logit_with.float(), dim=-1)            # (|V|)\n",
    "        p_no   = torch.softmax(logit_no.float(),   dim=-1)\n",
    "\n",
    "        # ----- KL( p_with || p_no )  (using p_with as 'true' dist) -----\n",
    "        kl_nat = torch.sum(p_with * (torch.log(p_with) - torch.log(p_no)))\n",
    "        total_kl_nat += kl_nat.item()\n",
    "\n",
    "        # ----- Δ log2 P(next_id)  (direction) -----\n",
    "        log2_with = ( torch.log(p_with[next_id]).item() ) / _LOG2E\n",
    "        log2_no   = ( torch.log(p_no  [next_id]).item() ) / _LOG2E\n",
    "        total_dlog2 += (log2_with - log2_no)          # >0 ⇒ 정답 확률↑\n",
    "\n",
    "        # teacher-force next_id into both prefixes\n",
    "        ids_no   = torch.cat([ids_no,   next_id.view(1,1)], dim=1)\n",
    "        ids_with = torch.cat([ids_with, next_id.view(1,1)], dim=1)\n",
    "\n",
    "    kl_bits   = total_kl_nat / _LOG2E           # nats→bits\n",
    "    delta_nll = -total_dlog2                  # NLL_no − NLL_with  (bits)\n",
    "\n",
    "    return kl_bits, delta_nll                 # (≥0 , ±)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: KL= 1.770 bits,  Δ-NLL=+3.046 bits (↑⇒help)\n",
      "Step2: KL= 4.294 bits,  Δ-NLL=-3.089 bits (↓⇒hurt)\n",
      "Step3: KL= 0.458 bits,  Δ-NLL=+1.046 bits (↑⇒help)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 KL=0.483, Δlog₂P(ans)=+0.288\n",
      "Step2 KL=0.221, Δlog₂P(ans)=+0.023\n",
      "Step3 KL=0.373, Δlog₂P(ans)=-0.752\n"
     ]
    }
   ],
   "source": [
    "# context = \"Problem: What is (5 + 3) × 2 - 4?\\n\\n\"\n",
    "# step1   = \"Step 1: Compute inside the parentheses first: 5 + 3 = 8.\\n\"\n",
    "# step2   = \"Step 2: Multiply the result by 2: 8 × 2 = 16.\\n\"\n",
    "# step3   = \"Step 3: Subtract 4: 16 - 4 = 12.\\n\"\n",
    "# answer  = \"Answer: 12.\"\n",
    "\n",
    "context = \"Problem: What is 1/2 + 1/4 + 1/4?\\n\\n\"\n",
    "step1   = \"Step 1: Add 1/4 and 1/4 first: 1/4 + 1/4 = 1/2.\\n\"\n",
    "step2   = \"Step 2: Now add 1/2 + 1/2 = 1.\\n\"\n",
    "step3   = \"Step 3: All sum of the proabability is 1.\\n\"\n",
    "answer  = \" 1.\"\n",
    "\n",
    "# context = \"Problem: What is the sum of the digits of the number 84?\\n\\n\"\n",
    "# step1   = \"Step 1: The tens digit of 84 is 8.\\n\"\n",
    "# step2   = \"Step 2: The ones digit of 84 is 4.\\n\"\n",
    "# step3   = \"Step 3: The subtraction of 8-4 is 4.\\n\"\n",
    "# step4   = \"Step 4: Add the digits: 8 + 4 = 12.\\n\"\n",
    "# answer  = \" 12.\"\n",
    "\n",
    "\n",
    "for i,(s,ctx) in enumerate([(step1, context),\n",
    "                            (step2, context+step1),\n",
    "                            (step3, context+step1+step2),\n",
    "                            # (step4, context+step1+step2+step3)\n",
    "                            ], 1):\n",
    "    kl, dnll = kl_and_delta_nll(ctx, s, answer)\n",
    "    sign = \"↑\" if dnll>0 else \"↓\"\n",
    "    print(f\"Step{i}: KL={kl:6.3f} bits,  Δ-NLL={dnll:+6.3f} bits ({sign}⇒{'help' if dnll>0 else 'hurt'})\")\n",
    "\n",
    "kl1, d1 = info_gain_answer_kl(context, step1)\n",
    "kl2, d2 = info_gain_answer_kl(context+step1, step2)\n",
    "kl3, d3 = info_gain_answer_kl(context+step1+step2, step3)\n",
    "# kl4, d4 = info_gain_answer_kl(context+step1+step2+step3, step4)\n",
    "\n",
    "print(f\"Step1 KL={kl1:.3f}, Δlog₂P(ans)={d1:+.3f}\")\n",
    "print(f\"Step2 KL={kl2:.3f}, Δlog₂P(ans)={d2:+.3f}\")\n",
    "print(f\"Step3 KL={kl3:.3f}, Δlog₂P(ans)={d3:+.3f}\")\n",
    "# print(f\"Step4 KL={kl4:.3f}, Δlog₂P(ans)={d4:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rectangle area (5 cm × 3 cm) ===\n",
      "Step  1: KL=  2.52 bits,  Δ-NLL= -1.22 bits   hurt ❌\n",
      "Step  2: KL=  6.95 bits,  Δ-NLL= +5.48 bits   help ✅\n",
      "Step  3: KL=  3.25 bits,  Δ-NLL= -7.73 bits   hurt ❌\n",
      "→ Answer: 15 cm^2.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Solve for x (3x + 9 = 18) ===\n",
      "Step  1: KL=  3.51 bits,  Δ-NLL= -1.52 bits   hurt ❌\n",
      "Step  2: KL=  1.56 bits,  Δ-NLL= +3.23 bits   help ✅\n",
      "Step  3: KL=  0.47 bits,  Δ-NLL= -3.28 bits   hurt ❌\n",
      "Step  4: KL=  4.04 bits,  Δ-NLL= -0.38 bits   hurt ❌\n",
      "→ Answer: 3.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Prime factorization (60) ===\n",
      "Step  1: KL=  1.95 bits,  Δ-NLL= -4.33 bits   hurt ❌\n",
      "Step  2: KL=  0.82 bits,  Δ-NLL= +3.37 bits   help ✅\n",
      "Step  3: KL=  0.33 bits,  Δ-NLL= +1.60 bits   help ✅\n",
      "Step  4: KL=  0.86 bits,  Δ-NLL= -3.30 bits   hurt ❌\n",
      "Step  5: KL=  3.05 bits,  Δ-NLL= +3.88 bits   help ✅\n",
      "→ Answer: 2^2 × 3 × 5.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Simplify fraction (24/36) ===\n",
      "Step  1: KL=  1.54 bits,  Δ-NLL= -0.83 bits   hurt ❌\n",
      "Step  2: KL=  2.57 bits,  Δ-NLL= -3.89 bits   hurt ❌\n",
      "Step  3: KL=  1.30 bits,  Δ-NLL= +1.55 bits   help ✅\n",
      "Step  4: KL=  2.72 bits,  Δ-NLL= -0.41 bits   hurt ❌\n",
      "→ Answer: 2/3.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    # EXAMPLE 1  ──────────────────────────────────────────────────────────\n",
    "    # {\n",
    "    #     \"name\": \"Sum-of-digits (84)\",\n",
    "    #     \"context\": \"Problem: What is the sum of the digits of the number 84?\\n\\n\",\n",
    "    #     \"steps\": [\n",
    "    #         \"Step 1: The tens digit of 84 is 8.\\n\",                       # ✅\n",
    "    #         \"Step 2: The ones digit of 84 is 4.\\n\",                      # ✅\n",
    "    #         \"Step 3: The subtraction of 8-4 is 4.\\n\",                    # ❌ (irrelevant)\n",
    "    #         \"Step 4: Add the digits: 8 + 4 = 12.\\n\"                      # ✅\n",
    "    #     ],\n",
    "    #     \"answer\": \"Answer: 12.\"\n",
    "    # },\n",
    "    # EXAMPLE 2  ──────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"Rectangle area (5 cm × 3 cm)\",\n",
    "        \"context\": \"Problem: A rectangle has length 5 cm and width 3 cm. What is its area?\\n\\n\",\n",
    "        \"steps\": [\n",
    "            \"Step 1: The area of a rectangle is length × width.\\n\",      # ✅\n",
    "            \"Step 2: Add the dimensions: 5 + 3 = 8.\\n\",                  # ❌ (wrong op)\n",
    "            \"Step 3: Multiply: 5 × 3 = 15 square centimetres.\\n\"         # ✅\n",
    "        ],\n",
    "        \"answer\": \"Answer: 15 cm^2.\"\n",
    "    },\n",
    "    # EXAMPLE 3  ──────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"Solve for x (3x + 9 = 18)\",\n",
    "        \"context\": \"Problem: Solve for x: 3x + 9 = 18.\\n\\n\",\n",
    "        \"steps\": [\n",
    "            \"Step 1: Subtract 9 from both sides: 3x = 9.\\n\",             # ✅\n",
    "            \"Step 2: Divide both sides by 3: x = 3.\\n\",                  # ✅\n",
    "            \"Step 3: Check: 3(3) + 9 = 18, so x = 3 is correct.\\n\",      # ✅\n",
    "            \"Step 4: Therefore, x = 6.\\n\"                                # ❌ (contradict)\n",
    "        ],\n",
    "        \"answer\": \"Answer: 3.\"\n",
    "    },\n",
    "    # EXAMPLE 4  ──────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"Prime factorization (60)\",\n",
    "        \"context\": \"Problem: What is the prime factorization of 60?\\n\\n\",\n",
    "        \"steps\": [\n",
    "            \"Step 1: 60 = 6 × 10.\\n\",                                    # ✅\n",
    "            \"Step 2: 6 = 2 × 3.\\n\",                                      # ✅\n",
    "            \"Step 3: 10 = 2 × 5.\\n\",                                     # ✅\n",
    "            \"Step 4: So 60 = 2 × 2 × 3 × 5.\\n\",                          # ✅\n",
    "            \"Step 5: Combine two 2's into 4, so 60 = 4 × 3 × 5.\\n\"       # ❌ (not prime factors)\n",
    "        ],\n",
    "        \"answer\": \"Answer: 2^2 × 3 × 5.\"\n",
    "    },\n",
    "    # EXAMPLE 5  ──────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"Simplify fraction (24/36)\",\n",
    "        \"context\": \"Problem: Simplify the fraction 24/36.\\n\\n\",\n",
    "        \"steps\": [\n",
    "            \"Step 1: The GCD of 24 and 36 is 12.\\n\",                     # ✅\n",
    "            \"Step 2: Divide numerator by 6: 24 ÷ 6 = 4.\\n\",              # ❌ (wrong divisor)\n",
    "            \"Step 3: Divide numerator and denominator by 12: 24/12 = 2, 36/12 = 3.\\n\",  # ✅\n",
    "            \"Step 4: The simplified fraction is 2/3.\\n\"                 # ✅\n",
    "        ],\n",
    "        \"answer\": \"Answer: 2/3.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    print(f\"\\n=== {ex['name']} ===\")\n",
    "    ctx = ex[\"context\"]\n",
    "    for idx, step in enumerate(ex[\"steps\"], 1):\n",
    "        kl, dnll = kl_and_delta_nll(ctx, step, ex[\"answer\"], 0.8)\n",
    "        label = \"help ✅\" if dnll > 0 else \"hurt ❌\"\n",
    "        print(f\"Step {idx:>2}: KL={kl:6.2f} bits,  Δ-NLL={dnll:+6.2f} bits   {label}\")\n",
    "        ctx += step  # 다음 스텝 컨텍스트에 누적\n",
    "\n",
    "    print(f\"→ {ex['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,            # enable 4-bit (QLoRA-style) weights\n",
    "    bnb_4bit_quant_type=\"nf4\",    # NF4 gives the best accuracy for most LLMs\n",
    "    bnb_4bit_use_double_quant=True, # optional: second quantisation pass to save ~0.4 bits/param\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # faster matmuls on recent GPUs; fall back to float16 if needed\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      # let Accelerate split layers across all visible GPUs\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",     # keeps non-linear layers in their original dtype\n",
    "    trust_remote_code=True  # Qwen models need their custom code\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMConfig:\n",
    "    # MC config\n",
    "    model_name:             str = \"Qwen/Qwen2.5-Math-7B-Instruct\"    # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "    max_new_tokens:         int = 512\n",
    "    num_rollouts:           int = 8      \n",
    "    use_llm:                bool = True  # Use llm for masking\n",
    "    reward_type:            str = \"contri\"  # ori, contri, mi, naive, norm\n",
    "    # PRM Model config \n",
    "    hidden_size:        int = 512      # 256-1024 범위에서 적절\n",
    "    num_layers:         int = 3        # 2-4 범위에서 적절\n",
    "    dropout:            float = 0.2    # 0.1-0.3 범위에서 적절\n",
    "    # PRMTrainer config \n",
    "    batch_size:         int = 16       # 12 → 16으로 증가 (더 안정적)\n",
    "    learning_rate:      float = 3e-4   # 5e-4 → 3e-4로 감소 (더 안정적)\n",
    "    num_workers:        int = 4        # 적절\n",
    "    weight_decay:       float = 1e-2   # 적절\n",
    "    lr_scheduler:       str = \"cosine\" # 적절\n",
    "    dataset_size:       int = 0\n",
    "    warmup_steps:       int = 40       # 22 → 50으로 증가 (더 안정적)\n",
    "    grad_clip:          float = 1.0    # 적절\n",
    "    epochs:             int = 20       # 25 → 15로 감소 (early stopping 고려)\n",
    "    # Misc config\n",
    "    use_wandb:          bool = True\n",
    "    wandb_project:      str = \"mc_prm\"\n",
    "    run_name:           str = \"test_400_0715\"\n",
    "    checkpoint_dir:     str = \"./checkpoints/0715/contri\"\n",
    "    seed:               int = 42\n",
    "    # Inference config\n",
    "    num_candidates:     int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Qwen model: qwen2forcausallm, excluding presence_penalty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building GSM-8K reward-dataset:   0%|          | 0/1 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1-th Step, 0-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800 >>$800.\n",
      "Answer: \\boxed{800}\n",
      "```\n",
      "\n",
      "The final answer is \\(\\boxed{800}\\). Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 1-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800>>800 dollars\n",
      "Answer: \\boxed{800} To determine how much money Gretchen made over the weekend, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of drawings she sold.\n",
      "2. Multiply the total number of drawings by her charge per drawing.\n",
      "\n",
      "First, let's find the total number of drawings she sold. She sold 24 drawings on Saturday and 16 drawings on Sunday. So, we add these two numbers together:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Next, we need to find out how much money she made from these 40 drawings. Since she charges $20.00 per drawing, we multiply the total number of drawings by her charge per drawing:\n",
      "\\[ 40 \\times 20 = 800 \\]\n",
      "\n",
      "Therefore, the total amount of money Gretchen made over the weekend is \\(\\boxed{800}\\). Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 2-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800>>800 dollars.\n",
      "Step 3: Therefore, the total amount of money she made is \\boxed{800}. Step 1: She drew 24 on Saturday and 16 on Sunday for a total of \\(24 + 16 = 40\\) drawings.\n",
      "Step 2: She charges $20.00 per drawing, so she made \\(40 \\times 20 = 800\\) dollars.\n",
      "Step 3: Therefore, the total amount of money she made is \\(\\boxed{800}\\). Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 3-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800 >>800 dollars.\n",
      "Answer: \\boxed{800} To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of drawings she sold over the weekend.\n",
      "2. Multiply the total number of drawings by the price per drawing.\n",
      "\n",
      "**Step 1: Calculate the total number of drawings.**\n",
      "Gretchen sold 24 drawings on Saturday and 16 drawings on Sunday. So, the total number of drawings is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "**Step 2: Calculate the total amount of money made.**\n",
      "Gretchen charges $20.00 per drawing. Therefore, the total amount of money she made is:\n",
      "\\[ 40 \\times 20 = 800 \\]\n",
      "\n",
      "So, the total amount of money Gretchen made is \\(\\boxed{800}\\). Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 4-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800>>800 dollars.\n",
      "\n",
      "Answer: \\boxed{800} Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 5-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800>>800 dollars\n",
      "Answer: \\boxed{800} Pred Answer 800 Gold Answer 800\n",
      "[1-th Step, 6-th Original Rollout]  She charges $20.00 per drawing. Therefore, her total earnings are 40×20 = <<40×20=800>>800 dollars.\n",
      "Step 3: The total amount of money she made is \\boxed{800}. To determine how much money Gretchen made over the weekend, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of drawings she sold.\n",
      "2. Multiply the total number of drawings by her charge per drawing.\n",
      "\n",
      "Let's go through the steps in detail:\n",
      "\n",
      "**Step 1: Calculate the total number of drawings she sold.**\n",
      "Gretchen sold 24 drawings on Saturday and 16 drawings on Sunday. So, the total number of drawings is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "**Step 2: Calculate her total earnings.**\n",
      "Gretchen charges $20.00 per drawing. Therefore, her total earnings are:\n",
      "\\[ 40 \\times 20 = 800 \\]\n",
      "\n",
      "**Step 3: State the final answer.**\n",
      "The total amount of money she made is:\n",
      "\\[ \\boxed{800} \\] Pred Answer  Gold Answer 800\n",
      "[1-th Step, 7-th Original Rollout]  She charges $20.00 per drawing, so she made 40×20 = <<40×20=800>>800 dollars.\n",
      "Step 3: Therefore, the total amount of money she made is \\boxed{800}. Step 1: She drew 24 on Saturday and 16 on Sunday for a total of 24+16 = 40 drawings.\n",
      "Step 2: She charges $20.00 per drawing, so she made 40×20 = 800 dollars.\n",
      "Step 3: Therefore, the total amount of money she made is \\boxed{800}. Pred Answer 800 Gold Answer 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2-th Step, 0-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made over the weekend, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the two days.\n",
      "2. Multiply the total number of caricatures by the price per drawing.\n",
      "\n",
      "Step 1: She drew 24 caricatures on Saturday and 16 caricatures on Sunday. So, the total number of caricatures she drew is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Step 2: She charges $20.00 per drawing. Therefore, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "Thus, the total amount of money Gretchen made over the weekend is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "[2-th Step, 1-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by the price per drawing.\n",
      "\n",
      "Step 1: Calculate the total number of caricatures she drew.\n",
      "Gretchen drew 24 caricatures on Saturday and 16 caricatures on Sunday. Therefore, the total number of caricatures she drew is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Step 2: Calculate the total amount of money she made.\n",
      "Gretchen charges $20.00 per drawing. Since she drew 40 caricatures, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "So, the answer is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "[2-th Step, 2-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made over the weekend, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by her charge per drawing.\n",
      "\n",
      "**Step 1: Calculate the total number of caricatures drawn.**\n",
      "\n",
      "Gretchen drew 24 caricatures on Saturday and 16 caricatures on Sunday. Therefore, the total number of caricatures drawn is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "**Step 2: Calculate the total amount of money made.**\n",
      "\n",
      "Gretchen charges $20.00 per drawing. Since she drew 40 caricatures, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "So, the total amount of money Gretchen made over the weekend is:\n",
      "\\[ \\boxed{800} \\] Pred Answer  Gold Answer 800\n",
      "[2-th Step, 3-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by her charge per drawing.\n",
      "\n",
      "Step 1: Calculate the total number of caricatures drawn.\n",
      "Gretchen drew 24 caricatures on Saturday and 16 caricatures on Sunday. So, the total number of caricatures drawn is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Step 2: Calculate the total amount of money made.\n",
      "Gretchen charges $20.00 per drawing. Since she drew 40 caricatures, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "Therefore, the answer is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "[2-th Step, 4-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by the price per drawing.\n",
      "\n",
      "Step 1: She drew 24 caricatures on Saturday and 16 caricatures on Sunday. So, the total number of caricatures she drew is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Step 2: She charges $20.00 per drawing. Therefore, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "So, the answer is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "[2-th Step, 5-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by her charge per drawing.\n",
      "\n",
      "First, we find the total number of caricatures she drew:\n",
      "\\[ 24 \\text{ (on Saturday)} + 16 \\text{ (on Sunday)} = 40 \\text{ caricatures} \\]\n",
      "\n",
      "Next, we calculate the total amount of money she made by multiplying the total number of caricatures by her charge per drawing:\n",
      "\\[ 40 \\text{ caricatures} \\times 20 \\text{ dollars per caricature} = 800 \\text{ dollars} \\]\n",
      "\n",
      "Therefore, the total amount of money Gretchen made is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "[2-th Step, 6-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made over the weekend, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew.\n",
      "2. Multiply the total number of caricatures by the price per drawing.\n",
      "\n",
      "First, let's find the total number of caricatures she drew. She drew 24 on Saturday and 16 on Sunday. So, we add these two numbers together:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Next, we need to find out how much money she made from drawing 40 caricatures. She charges $20.00 per drawing, so we multiply the total number of caricatures by the price per drawing:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "Therefore, the total amount of money Gretchen made over the weekend is:\n",
      "\\[ \\boxed{800} \\] Pred Answer  Gold Answer 800\n",
      "[2-th Step, 7-th Original Rollout] 800\n",
      "\n",
      "To determine how much money Gretchen made, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of caricatures she drew over the weekend.\n",
      "2. Multiply the total number of caricatures by the price per drawing.\n",
      "\n",
      "Step 1: She drew 24 caricatures on Saturday and 16 caricatures on Sunday. So, the total number of caricatures she drew is:\n",
      "\\[ 24 + 16 = 40 \\]\n",
      "\n",
      "Step 2: She charges $20.00 per drawing. Therefore, the total amount of money she made is:\n",
      "\\[ 20 \\times 40 = 800 \\]\n",
      "\n",
      "So, the answer is:\n",
      "\\[\n",
      "\\boxed{800}\n",
      "\\] Pred Answer None Gold Answer 800\n",
      "perturbed step: ['Step 1: \"She drew [MASKED] on Saturday and [MASKED] on Sunday for a total of [MASKED] + [MASKED] = [MASKED] drawings\"\\n\\nTo solve the problem, we need to identify the key numbers and expressions in the sentence that are crucial for calculating the total number of drawings.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed step: ['Step 1: She drew 24 on Saturday and 16 on Sunday for a total of 24+16 = <<24+16=40>>40 drawings', 'Step 2: \"She charges [MASKED] per drawing and she drew [MASKED] caricatures so she made [MASKED]*[MASKED] = [MASKED]\"\\n\\nTo solve the problem, we need to identify the key components of the sentence that are necessary to determine the total amount of money she made. These']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building GSM-8K reward-dataset: 100%|██████████| 1/1 [00:57<00:00, 57.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Gretchen draws caricatures in the park on the weekends.  She charges $20.00 per drawing.  If she sold 24 on Saturday and 16 on Sunday, how much money did she make?',\n",
       "  'completion': ['Step 1: She drew 24 on Saturday and 16 on Sunday for a total of 24+16 = <<24+16=40>>40 drawings',\n",
       "   'Step 2: She charges $20.00 per drawing and she drew 40 caricatures so she made $20*40 = $<<20*40=800>>800'],\n",
       "  'ori_rewards': [0.875, 0.0],\n",
       "  'ptb_rewards': [1.0, 1.0],\n",
       "  'contributions': [-0.125, -1.0],\n",
       "  'mi_rewards': [-4.25440533955892, 4.919126192728678],\n",
       "  'naive_rewards': [0.875, 4.9191],\n",
       "  'gold_answer': '800'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from mc_shaped_reward import MCRewardShaped\n",
    "\n",
    "cfg = PRMConfig()\n",
    "mcrs = MCRewardShaped(config=cfg , model=model, tokenizer=tokenizer)\n",
    "gsm8k_raw = mcrs.gsm8k_reward_dataset(split=\"train\", start=5001, take=1)\n",
    "gsm8k_raw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\",  torch_dtype=torch.bfloat16, trust_remote_code=True,).eval()\n",
    "\n",
    "data = {\n",
    "    \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "    \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue. On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard. On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "    \"response\": [\n",
    "      \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "      \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "      \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "      \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_rewards(logits, token_masks):\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "    all_scores_res = []\n",
    "    for i in range(probabilities.size(0)):\n",
    "        sample = probabilities[i] # seq_len, num_labels [452, 2]\n",
    "        positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels [# of steps, label]\n",
    "        non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "        all_scores_res.append(non_zero_elements_list)\n",
    "    return all_scores_res\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": data['system']},\n",
    "    {\"role\": \"user\", \"content\": data['query']},\n",
    "    {\"role\": \"assistant\", \"content\": \"<extra_0>\".join(data['response']) + \"<extra_0>\"},\n",
    "]\n",
    "conversation_str = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(conversation_str, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model(input_ids=input_ids)\n",
    "\n",
    "step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "token_masks = (input_ids == step_sep_id)\n",
    "step_reward = make_step_rewards(outputs[0], token_masks)\n",
    "print(step_reward)  # [[1.0, 0.1904296875, 0.9765625, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token '<extra_0>': ID 151651\n",
      "Token '<extra_0>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '<extra_1>': ID 27\n",
      "Token '<extra_1>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '<extra_2>': ID 27\n",
      "Token '<extra_2>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '|STEP|': ID 91\n",
      "Token '|STEP|': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    }
   ],
   "source": [
    "def check_token_learning():\n",
    "    test_tokens = [\"<extra_0>\", \"<extra_1>\", \"<extra_2>\", \"|STEP|\"]\n",
    "    for token in test_tokens:\n",
    "        try:\n",
    "            token_id = tokenizer.encode(token)[0]\n",
    "            print(f\"Token '{token}': ID {token_id}\")\n",
    "            if hasattr(model, 'get_input_embeddings'):\n",
    "                embedding = model.get_input_embeddings()\n",
    "                token_embedding = embedding(torch.tensor([token_id]))\n",
    "                print(f\"  Embedding shape: {token_embedding.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Token '{token}': Error - {e}\")\n",
    "\n",
    "check_token_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorrect Step Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import copy\n",
    "\n",
    "def create_incorrect_steps(gold_steps: List[str], incorrect_type: str = \"wrong_calculation\") -> Tuple[List[str], int]:\n",
    "    if not gold_steps:\n",
    "        return gold_steps\n",
    "\n",
    "    perturbed_steps = gold_steps.copy()\n",
    "    insert_position = random.randint(1, len(gold_steps))   # 1 ≤ pos ≤ len\n",
    "    \n",
    "    if incorrect_type == \"wrong_calculation\":\n",
    "        wrong_calculations = [\n",
    "            f\"Step {insert_position + 1}: 5 + 3 = 9\",  # 5 + 3 = 8\n",
    "            f\"Step {insert_position + 1}: 10 * 2 = 15\",  # 10 * 2 = 20\n",
    "            f\"Step {insert_position + 1}: 20 / 4 = 6\",  # 20 / 4 = 5\n",
    "            f\"Step {insert_position + 1}: 7 - 3 = 5\",  # 7 - 3 = 4\n",
    "            f\"Step {insert_position + 1}: 2^2 = 5\",  # 2^2 = 4\n",
    "            f\"Step {insert_position + 1}: sqrt(16) = 3\",  # sqrt(16) = 4\n",
    "            f\"Step {insert_position + 1}: 3 * 7 = 24\",  # 3 * 7 = 21\n",
    "            f\"Step {insert_position + 1}: 15 / 3 = 6\",  # 15 / 3 = 5\n",
    "        ]\n",
    "        insert_step = random.choice(wrong_calculations)\n",
    "    elif incorrect_type == \"logical_error\":\n",
    "        logical_errors = [\n",
    "            f\"Step {insert_position + 1}: Since we need to find the total, we should multiply by 0 instead of adding.\",\n",
    "            f\"Step {insert_position + 1}: To solve this, we need to subtract the larger number from the smaller one.\",\n",
    "            f\"Step {insert_position + 1}: The answer should be negative because we're dealing with positive numbers.\",\n",
    "            f\"Step {insert_position + 1}: We can ignore the units since they don't affect the calculation.\",\n",
    "            f\"Step {insert_position + 1}: The order of operations doesn't matter here, so we can do addition first.\",\n",
    "        ]\n",
    "        insert_step = random.choice(logical_errors)\n",
    "    elif incorrect_type == \"irrelevant\":\n",
    "        irrelevant_steps = [\n",
    "            f\"Step {insert_position + 1}: The weather is nice today.\",\n",
    "            f\"Step {insert_position + 1}: I like literatures very much.\",\n",
    "            f\"Step {insert_position + 1}: This reminds me of my school days.\",\n",
    "            f\"Step {insert_position + 1}: The sky is blue and beautiful.\",\n",
    "            f\"Step {insert_position + 1}: I should drink more water.\",\n",
    "            f\"Step {insert_position + 1}: Python is the language of the universe.\",\n",
    "        ]\n",
    "        insert_step = random.choice(irrelevant_steps)\n",
    "    elif incorrect_type == \"repetition\":\n",
    "        if len(gold_steps) > 1:\n",
    "            repeat_step = gold_steps[insert_position - 1]  # 이전 step\n",
    "            step_num = insert_position + 1\n",
    "            step_content = repeat_step.split(\":\", 1)[1] if \":\" in repeat_step else \"\"\n",
    "            insert_step = f\"Step {step_num}:{step_content}\"\n",
    "        else:\n",
    "            insert_position = 1\n",
    "            step_num = insert_position + 1\n",
    "            repeat_step = gold_steps[0]\n",
    "            step_content = repeat_step.split(\":\", 1)[1] if \":\" in repeat_step else \"\"\n",
    "            insert_step = f\"Step {step_num}:{step_content}\"\n",
    "    else:\n",
    "        insert_step = f\"Step {insert_position + 1}: 5 + 3 = 9\"\n",
    "    \n",
    "    perturbed_steps.insert(insert_position, insert_step)\n",
    "\n",
    "    for i in range(insert_position + 1, len(perturbed_steps)):\n",
    "            if perturbed_steps[i].startswith(\"Step \"):\n",
    "                step_num = i + 1\n",
    "                step_content = perturbed_steps[i].split(\":\", 1)[1] if \":\" in perturbed_steps[i] else \"\"\n",
    "                perturbed_steps[i] = f\"Step {step_num}:{step_content}\"\n",
    "    \n",
    "    return perturbed_steps, insert_position\n",
    "\n",
    "def add_incorrect_completion(entry: Dict[str, Any], incorrect_type: str = \"wrong_calculation\", negative_reward: float = -1.0) -> Dict[str, Any]:\n",
    "    new_entry = copy.deepcopy(entry)\n",
    "    original_completion = entry.get(\"completion\", [])\n",
    "    # incorrect completion 생성\n",
    "    incorrect_completion, insert_position = create_incorrect_steps(original_completion, incorrect_type)\n",
    "    new_entry[\"completion\"] = incorrect_completion\n",
    "    # incorrect rewards 생성\n",
    "    for key, val in entry.items():\n",
    "        if isinstance(val, list) and len(val) == len(original_completion) and key != \"completion\":\n",
    "            new_vec = val.copy()\n",
    "            if key == \"contributions\":\n",
    "                new_vec.insert(insert_position, negative_reward)\n",
    "            elif key == \"mi_filtered\":\n",
    "                new_vec.insert(insert_position, 0.0)\n",
    "            else:\n",
    "                new_vec.insert(insert_position, negative_reward)\n",
    "            new_entry[key] = new_vec\n",
    "    # meta data 추가\n",
    "    new_entry[\"is_incorrect\"] = True\n",
    "    new_entry[\"incorrect_type\"] = incorrect_type\n",
    "\n",
    "    return new_entry\n",
    "\n",
    "def extend_dataset_with_incorrect_steps(dataset: List[Dict[str, Any]], incorrect_types: List[str] = None,negative_rewards: List[float] = None, ratio: float = 0.5) -> List[Dict[str, Any]]:\n",
    "    if incorrect_types is None:\n",
    "        incorrect_types = [\"wrong_calculation\", \"logical_error\", \"irrelevant_step\", \"repetition\"]\n",
    "    if negative_rewards is None:\n",
    "        negative_rewards = [-1.0] * len(incorrect_types)\n",
    "    \n",
    "    # negative_rewards를 incorrect_types와 매칭\n",
    "    if len(negative_rewards) != len(incorrect_types):\n",
    "        negative_rewards = negative_rewards * (len(incorrect_types) // len(negative_rewards) + 1)\n",
    "        negative_rewards = negative_rewards[:len(incorrect_types)]\n",
    "    \n",
    "    extended_dataset = []\n",
    "    for entry in dataset:\n",
    "        extended_dataset.append(entry)\n",
    "        if random.random() < ratio:\n",
    "            incorrect_type = random.choice(incorrect_types) # 랜덤하게 incorrect type 선택\n",
    "            type_idx = incorrect_types.index(incorrect_type)\n",
    "            negative_reward = negative_rewards[type_idx]\n",
    "            incorrect_entry = add_incorrect_completion(entry, incorrect_type, negative_reward)\n",
    "            extended_dataset.append(incorrect_entry)\n",
    "    \n",
    "    return extended_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 7473\n",
      "Extended dataset size: 11141\n",
      "Added 3668 incorrect samples\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral.json\"\n",
    "output_path = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral_incorrect.json\"\n",
    "\n",
    "with open(input_path, \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "extended_dataset = extend_dataset_with_incorrect_steps(dataset=dataset, ratio=0.5)\n",
    "print(f\"Extended dataset size: {len(extended_dataset)}\")\n",
    "print(f\"Added {len(extended_dataset) - len(dataset)} incorrect samples\")\n",
    "\n",
    "# 확장된 dataset 저장\n",
    "with open(output_path, \"w\") as file2:\n",
    "    json.dump(extended_dataset, file2, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct samples: 7473\n",
      "Incorrect samples: 3668\n",
      "\n",
      "Incorrect types distribution:\n",
      "  repetition: 911\n",
      "  wrong_calculation: 928\n",
      "  irrelevant_step: 923\n",
      "  logical_error: 906\n"
     ]
    }
   ],
   "source": [
    "# 통계 출력\n",
    "correct_count = sum(1 for entry in extended_dataset if not entry.get(\"is_incorrect\", False))\n",
    "incorrect_count = sum(1 for entry in extended_dataset if entry.get(\"is_incorrect\", False))\n",
    "\n",
    "print(f\"Correct samples: {correct_count}\")\n",
    "print(f\"Incorrect samples: {incorrect_count}\")\n",
    "\n",
    "# 타입별 통계\n",
    "type_counts = {}\n",
    "for entry in extended_dataset:\n",
    "    if entry.get(\"is_incorrect\", False):\n",
    "        incorrect_type = entry.get(\"incorrect_type\", \"unknown\")\n",
    "        type_counts[incorrect_type] = type_counts.get(incorrect_type, 0) + 1\n",
    "\n",
    "print(\"\\nIncorrect types distribution:\")\n",
    "for incorrect_type, count in type_counts.items():\n",
    "    print(f\"  {incorrect_type}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Fintuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTPRM(nn.Module):\n",
    "    def __init__(self, base_model_name: str, lora_rank: int = 16, lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if hasattr(self.backbone, \"score\"):\n",
    "            # For Qwen2.5-Math-PRM-7B\n",
    "            in_feat = self.backbone.score[0].in_features\n",
    "            self.backbone.score = nn.Sequential(\n",
    "                nn.Linear(in_feat, in_feat),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_feat, 1, bias=True)  # 2 → 1\n",
    "            )\n",
    "            self.reg_head = None\n",
    "        else:\n",
    "            # Other AutoModel(Causal LM)\n",
    "            hidden = self.backbone.config.hidden_size\n",
    "            self.reg_head = nn.Sequential(\n",
    "                nn.Linear(hidden, hidden // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden // 4, 1)\n",
    "            )\n",
    "\n",
    "        # Add Lora Adapter\n",
    "        self.backbone = prepare_model_for_kbit_training(self.backbone)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_cfg)\n",
    "        self._activate_head_params()   \n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        hidden = out.hidden_states[-1]                     # (B, L, H)\n",
    "\n",
    "        # Last token vector\n",
    "        if attention_mask is None:   \n",
    "            rep = hidden[:, -1, :]\n",
    "        else:\n",
    "            seq_len = attention_mask.sum(1) - 1           # (B,)\n",
    "            rep = hidden[torch.arange(hidden.size(0), device=hidden.device), seq_len, :]\n",
    "\n",
    "        # head 통과\n",
    "        if self.reg_head is None:\n",
    "            pred = self.backbone.score(rep).squeeze(-1)\n",
    "        else:\n",
    "            pred = self.reg_head(rep).squeeze(-1)\n",
    "\n",
    "        if labels is not None:              # training / eval\n",
    "            loss = F.mse_loss(pred, labels.float())\n",
    "            return loss, pred   \n",
    "        else:                               # pure inference\n",
    "            return pred\n",
    "\n",
    "    def _activate_head_params(self):\n",
    "        if self.reg_head is not None:\n",
    "            for p in self.reg_head.parameters():\n",
    "                p.requires_grad_(True)\n",
    "        else:\n",
    "            for p in self.backbone.score.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def get_parameter_stats(self):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        module_stats = {}\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                \n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['trainable'] += param.numel()\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "            else:\n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "        \n",
    "        return {\n",
    "            'total_params': all_param,\n",
    "            'trainable_params': trainable_params,\n",
    "            'trainable_ratio': trainable_params / all_param * 100,\n",
    "            'module_stats': module_stats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.72s/it]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FTPRM(\n",
       "  (backbone): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Qwen2ForProcessRewardModel(\n",
       "        (model): Qwen2Model(\n",
       "          (embed_tokens): Embedding(152064, 3584)\n",
       "          (layers): ModuleList(\n",
       "            (0-27): 28 x Qwen2DecoderLayer(\n",
       "              (self_attn): Qwen2SdpaAttention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): Qwen2RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Qwen2MLP(\n",
       "                (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "        )\n",
       "        (score): Sequential(\n",
       "          (0): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3584, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = FTPRM(base_model_name=model_name)\n",
    "model.to(device)\n",
    "# model.get_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# PRM Custom Class\n",
    "class FTLM(nn.Module):\n",
    "    def __init__(self, base_model_name: str, lora_rank: int = 16, lora_alpha: int = 32, mlp_ratio: int = 4, value_head_prefix: str = \"value_head\", \n",
    "                 normalize_reward: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use AutoModelForCausalLM for pure CausalLM fine-tuning\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Add Lora Adapter for efficient fine-tuning\n",
    "        self.backbone = prepare_model_for_kbit_training(self.backbone)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_cfg)\n",
    "\n",
    "        # Value head for reward prediction\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        mlp_hidden = hidden // mlp_ratio\n",
    "        head = nn.Sequential(\n",
    "            nn.Linear(hidden, mlp_hidden, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_hidden, 1, bias=False),\n",
    "        )\n",
    "        self.value_head_prefix = value_head_prefix\n",
    "        setattr(self, value_head_prefix, head)\n",
    "\n",
    "        # head 가중치 학습 가능하도록 보장\n",
    "        for p in head.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        \n",
    "        self.normalize_reward = normalize_reward\n",
    "        self.register_buffer(\"mean\", torch.zeros(1), persistent=False)\n",
    "        self.register_buffer(\"std\",  torch.ones(1),  persistent=False)\n",
    "\n",
    "        # 캐시 비활성 (gradient checkpointing 호환)\n",
    "        self.backbone.config.use_cache = False\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels=None, return_hidden: bool = False):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.backbone.config.pad_token_id).long()\n",
    "\n",
    "        # position_ids = cumulative mask\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 0)\n",
    "\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        last_hidden = outputs.hidden_states[-1]        # (B, T, H)\n",
    "        # Index of last non-pad token  → (B, 1)\n",
    "        eos_idx = attention_mask.size(1) - 1 - attention_mask.long().fliplr().argmax(-1, keepdim=True)\n",
    "\n",
    "        values = getattr(self, self.value_head_prefix)(last_hidden).squeeze(-1)   # (B, T)\n",
    "        reward = values.gather(1, eos_idx).squeeze(1)                             # (B,)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(reward, labels.float())\n",
    "            return loss, reward\n",
    "        else:\n",
    "            # if (not self.training) and self.normalize_reward:\n",
    "            #     reward = (reward - self.mean) / (self.std + 1e-8)\n",
    "            return (reward, last_hidden) if return_hidden else reward\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def get_parameter_stats(self):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        module_stats = {}\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                \n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['trainable'] += param.numel()\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "            else:\n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "        \n",
    "        return {\n",
    "            'total_params': all_param,\n",
    "            'trainable_params': trainable_params,\n",
    "            'trainable_ratio': trainable_params / all_param * 100,\n",
    "            'module_stats': module_stats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4,366,276,992\n",
      "Trainable parameters: 13,304,704\n",
      "Trainable ratio: 0.30%\n",
      "FTLM(\n",
      "  (backbone): PeftModel(\n",
      "    (base_model): LoraModel(\n",
      "      (model): Qwen2ForCausalLM(\n",
      "        (model): Qwen2Model(\n",
      "          (embed_tokens): Embedding(152064, 3584)\n",
      "          (layers): ModuleList(\n",
      "            (0-27): 28 x Qwen2DecoderLayer(\n",
      "              (self_attn): Qwen2Attention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (mlp): Qwen2MLP(\n",
      "                (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (value_head): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=896, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=896, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = FTLM(base_model_name=model_name, value_head_prefix=\"value_head\")\n",
    "model.to(device)\n",
    "\n",
    "stats = model.get_parameter_stats()\n",
    "print(f\"Total parameters: {stats['total_params']:,}\")\n",
    "print(f\"Trainable parameters: {stats['trainable_params']:,}\")\n",
    "print(f\"Trainable ratio: {stats['trainable_ratio']:.2f}%\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PRMCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids, rewards = zip(*batch)\n",
    "        lengths = [len(ids) for ids in input_ids]\n",
    "        max_len = max(lengths)\n",
    "        if self.pad_to_multiple_of:\n",
    "            max_len = int(math.ceil(max_len / self.pad_to_multiple_of) * self.pad_to_multiple_of)\n",
    "\n",
    "        padded = [\n",
    "            torch.cat([ids, ids.new_full((max_len - len(ids),), self.tokenizer.pad_token_id)])\n",
    "            for ids in input_ids\n",
    "        ]\n",
    "        input_ids = torch.stack(padded)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": rewards,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward type: cmi\n",
      "Full dataset size: 26720\n",
      "Finish Loading Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading Trainer\n",
      "loss: 0.6680973768234253 pred shape: torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 3830919681,\n",
       " 'trainable_params': 22944769,\n",
       " 'trainable_ratio': 0.598936310614861,\n",
       " 'module_stats': {'backbone': {'trainable': 22944769, 'total': 3830919681}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from prm_dataset import StepwisePRMDataset\n",
    "from config import PRMConfig\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral.json\", \"r\") as file:\n",
    "    gsm8k_raw = json.load(file)\n",
    "\n",
    "cfg = PRMConfig()\n",
    "full_ds = StepwisePRMDataset(gsm8k_raw, tokenizer, cfg.max_new_tokens, reward_type=\"cmi\")\n",
    "print(f\"Full dataset size: {len(full_ds)}\") \n",
    "\n",
    "indices = list(range(len(full_ds)))\n",
    "split_idx = int(0.9 * len(full_ds)) if len(full_ds) > 1 else 1\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:] if len(full_ds) > 1 else indices[:1]\n",
    "\n",
    "train_ds = Subset(full_ds, train_indices)\n",
    "valid_ds = Subset(full_ds, val_indices)\n",
    "\n",
    "collate = PRMCollator(tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False, collate_fn=collate)\n",
    "print(\"Finish Loading Dataset\")\n",
    "\n",
    "output_dir = \"/home/leena/ccc_eval/mcts_prm/prm_training/checkpoints/pt_prm\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,       # ← 변수명 교정\n",
    "    data_collator=collate        # ← 반드시 추가\n",
    ")\n",
    "print(\"Finish Loading Trainer\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = collate([train_ds[i] for i in range(4)])\n",
    "    out = model(**{k: v.to(device) for k, v in b.items()})\n",
    "    print(\"loss:\", out[0].item(), \"pred shape:\", out[1].shape)\n",
    "\n",
    "model.get_parameter_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step_rewards(model, tokenizer, problem, steps):\n",
    "    \"\"\"\n",
    "    problem: str\n",
    "    steps  : List[str]  # [\"Step 1: ...\", \"Step 2: ...\", ...]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prefix = [f\"Problem: {problem}\"]\n",
    "    rewards = []\n",
    "    for s in steps:\n",
    "        prefix.append(s)\n",
    "        txt = \"\\n\".join(prefix)\n",
    "        enc = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            reward = model(**enc).item()      # forward returns pred\n",
    "        rewards.append(reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- Training utils ------------------------------- #\n",
    "def train(model: RewardRegressionModel, loader: DataLoader, optimizer, scheduler, device, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=\"train\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model: RewardRegressionModel, loader: DataLoader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"eval\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Model ---------------------------------------------------------------\n",
    "model = RewardRegressionModel(\n",
    "    base_model_name=args.base_model,\n",
    "    lora_rank=args.lora_rank,\n",
    ")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimiser & scheduler ----------------------------------------------\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optim_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=args.lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "total_steps = len(train_loader) * args.epochs // args.gradient_accumulation\n",
    "warmup_steps = int(total_steps * args.warmup_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# Loss function -------------------------------------------------------\n",
    "if args.loss == \"mse\":\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif args.loss == \"huber\":\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "else:  # logcosh\n",
    "    loss_fn = lambda pred, tgt: torch.mean(torch.log(torch.cosh(pred - tgt)))\n",
    "\n",
    "# Gradient accumulation ----------------------------------------------\n",
    "scaler = torch.cuda.amp.GradScaler() if args.mixed_precision in (\"bf16\", \"fp16\") else None\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{args.epochs}\")\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=\"train\")\n",
    "    for step, batch in enumerate(pbar, 1):\n",
    "        with torch.cuda.amp.autocast(enabled=args.mixed_precision in (\"bf16\", \"fp16\")):\n",
    "            preds = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            )\n",
    "            loss = loss_fn(preds, batch[\"labels\"].to(device)) / args.gradient_accumulation\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if step % args.gradient_accumulation == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        train_loss += loss.item() * args.gradient_accumulation * batch[\"input_ids\"].size(0)\n",
    "        pbar.set_postfix(loss=loss.item() * args.gradient_accumulation)\n",
    "\n",
    "    train_loss /= len(train_ds)\n",
    "    val_loss = evaluate(model, valid_loader, device, loss_fn)\n",
    "    print(f\"Epoch {epoch} | train MSE {train_loss:.4f} | valid MSE {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        ckpt_path = os.path.join(args.output_dir, f\"best_epoch{epoch}_loss{val_loss:.4f}.pt\")\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"tokenizer\": tokenizer.__dict__,\n",
    "            \"args\": vars(args),\n",
    "        }, ckpt_path)\n",
    "        print(f\"Saved new best checkpoint → {ckpt_path}\")\n",
    "\n",
    "print(\"Training finished. Best validation MSE:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Project imports\n",
    "from prm_trainer_mse import PRMTrainerMSE\n",
    "from config import PRMConfig\n",
    "from data_generation.contri_reward import ContriRewardvLLM\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_or_generate_data(config: PRMConfig, dataset_name: str = \"olympiad\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load or generate training data\n",
    "    \"\"\"\n",
    "    if dataset_name == \"olympiad\":\n",
    "        # Generate OlympiadBench data\n",
    "        logger.info(\"Generating OlympiadBench dataset...\")\n",
    "        reward_generator = ContriRewardvLLM(config)\n",
    "        \n",
    "        # Collect data from different splits\n",
    "        train_entries = []\n",
    "        val_entries = []\n",
    "        \n",
    "        # Generate training data\n",
    "        for entry in reward_generator.olympiad_reward_dataset_vllm(\n",
    "            split=\"train\", \n",
    "            start=0, \n",
    "            take=config.dataset_size if config.dataset_size > 0 else None\n",
    "        ):\n",
    "            train_entries.append(entry)\n",
    "        \n",
    "        # Generate validation data (smaller subset)\n",
    "        val_size = min(100, len(train_entries) // 5)  # 20% or max 100\n",
    "        for entry in reward_generator.olympiad_reward_dataset_vllm(\n",
    "            split=\"validation\", \n",
    "            start=0, \n",
    "            take=val_size\n",
    "        ):\n",
    "            val_entries.append(entry)\n",
    "        \n",
    "        logger.info(f\"Generated {len(train_entries)} training samples and {len(val_entries)} validation samples\")\n",
    "        \n",
    "        return train_entries, val_entries\n",
    "    \n",
    "    elif dataset_name == \"gsm8k\":\n",
    "        # Generate GSM8K data\n",
    "        logger.info(\"Generating GSM8K dataset...\")\n",
    "        reward_generator = ContriRewardvLLM(config)\n",
    "        \n",
    "        train_entries = []\n",
    "        val_entries = []\n",
    "        \n",
    "        for entry in reward_generator.gsm8k_reward_dataset_vllm(\n",
    "            split=\"train\",\n",
    "            start=0,\n",
    "            take=config.dataset_size if config.dataset_size > 0 else None\n",
    "        ):\n",
    "            train_entries.append(entry)\n",
    "        \n",
    "        val_size = min(100, len(train_entries) // 5)\n",
    "        for entry in reward_generator.gsm8k_reward_dataset_vllm(\n",
    "            split=\"test\",\n",
    "            start=0,\n",
    "            take=val_size\n",
    "        ):\n",
    "            val_entries.append(entry)\n",
    "        \n",
    "        logger.info(f\"Generated {len(train_entries)} training samples and {len(val_entries)} validation samples\")\n",
    "        \n",
    "        return train_entries, val_entries\n",
    "    \n",
    "    elif dataset_name == \"math\":\n",
    "        # Generate MATH data\n",
    "        logger.info(\"Generating MATH dataset...\")\n",
    "        reward_generator = ContriRewardvLLM(config)\n",
    "        \n",
    "        train_entries = []\n",
    "        val_entries = []\n",
    "        \n",
    "        for entry in reward_generator.math_reward_dataset_vllm(\n",
    "            split=\"train\",\n",
    "            start=0,\n",
    "            take=config.dataset_size if config.dataset_size > 0 else None\n",
    "        ):\n",
    "            train_entries.append(entry)\n",
    "        \n",
    "        val_size = min(100, len(train_entries) // 5)\n",
    "        for entry in reward_generator.math_reward_dataset_vllm(\n",
    "            split=\"validation\",\n",
    "            start=0,\n",
    "            take=val_size\n",
    "        ):\n",
    "            val_entries.append(entry)\n",
    "        \n",
    "        logger.info(f\"Generated {len(train_entries)} training samples and {len(val_entries)} validation samples\")\n",
    "        \n",
    "        return train_entries, val_entries\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "def load_pretrained_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Load pretrained model and tokenizer\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train PRM with MSE loss\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"config.py\", help=\"Path to config file\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"olympiad\", \n",
    "                       choices=[\"olympiad\", \"gsm8k\", \"math\"], help=\"Dataset to use\")\n",
    "    parser.add_argument(\"--from-scratch\", action=\"store_true\", help=\"Train from scratch\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, help=\"Path to checkpoint to resume from\")\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"./checkpoints\", help=\"Output directory\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load configuration\n",
    "    config = PRMConfig()\n",
    "    \n",
    "    # Override config with command line args\n",
    "    if args.output_dir:\n",
    "        config.checkpoint_dir = args.output_dir\n",
    "    \n",
    "    # Set dataset size if not specified\n",
    "    if config.dataset_size == 0:\n",
    "        config.dataset_size = 1000  # Default dataset size\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_pretrained_model(config.model_name)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = PRMTrainerMSE(\n",
    "        cfg=config,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        from_scratch=args.from_scratch\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint if specified\n",
    "    if args.checkpoint:\n",
    "        trainer.load_checkpoint(args.checkpoint)\n",
    "        logger.info(f\"Resumed training from checkpoint: {args.checkpoint}\")\n",
    "    \n",
    "    # Load or generate data\n",
    "    train_entries, val_entries = load_or_generate_data(config, args.dataset)\n",
    "    \n",
    "    # Start training\n",
    "    logger.info(\"Starting PRM training with MSE loss...\")\n",
    "    trainer.train(train_entries, val_entries)\n",
    "    \n",
    "    logger.info(\"Training completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

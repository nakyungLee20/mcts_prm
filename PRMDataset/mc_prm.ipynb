{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/peer/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    # \"meta-llama/Meta-Llama-3-8B\" \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" \"Qwen/Qwen2.5-Math-7B-Instruct\" \"Qwen/Qwen2.5-Math-7B\"\n",
    "    max_new_tokens: int = 512\n",
    "    num_rollouts: int = 5\n",
    "    reward_threshold: float = 0.2\n",
    "    samples_per_question: int = 1\n",
    "    # PRM config\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 5e-4\n",
    "    hidden_size: int = 256\n",
    "    num_workers: int = 4\n",
    "    # Misc config\n",
    "    use_wandb: bool = False\n",
    "    checkpoint_dir: str = \"checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                        UTILITY: ANSWER NORMALISATION                         #\n",
    "################################################################################\n",
    "import sympy as sp\n",
    "\n",
    "def _strip_markup(ans: str) -> str:\n",
    "    \"\"\"Remove common LaTeX/markup & variable tags.\"\"\"\n",
    "    # Remove LaTeX inline math wrappers \\( … \\) or \\[ … \\]\n",
    "    ans = re.sub(r\"\\\\[\\[(](.*?)[\\\\\\])]\", r\"\\1\", ans)\n",
    "    # Remove \\boxed{…}\n",
    "    ans = re.sub(r\"\\\\boxed\\{([^}]*)\\}\", r\"\\1\", ans)\n",
    "    # Remove variable assignments like \"y =\" or \"x=\" at start\n",
    "    ans = re.sub(r\"^[a-zA-Z]\\s*=\\s*\", \"\", ans)\n",
    "    # Trim outer $ … $ if present\n",
    "    ans = ans.strip()\n",
    "    if ans.startswith(\"$\") and ans.endswith(\"$\"):\n",
    "        ans = ans[1:-1]\n",
    "    return ans.strip()\n",
    "\n",
    "def _sanitize(text: str) -> str:\n",
    "    \"\"\"Normalise a candidate answer string for comparison.\"\"\"\n",
    "    text = _strip_markup(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\s\\.;:,]+$\", \"\", text)     # trailing punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)              # collapse spaces\n",
    "    return text\n",
    "\n",
    "def _to_float(expr: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(eval(expr.replace(\"^\", \"**\")))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _numeric_equiv(a: str, b: str) -> bool:\n",
    "    \"\"\"Return True if `a` and `b` are numerically equivalent or exact match.\"\"\"\n",
    "    a_clean, b_clean = map(_sanitize, (a, b))\n",
    "    if a_clean == b_clean:\n",
    "        return True\n",
    "\n",
    "    # Attempt simple numeric evaluation\n",
    "    a_val, b_val = _to_float(a_clean), _to_float(b_clean)\n",
    "    if a_val is not None and b_val is not None:\n",
    "        return math.isclose(a_val, b_val, rel_tol=1e-6)\n",
    "\n",
    "    if sp is not None:\n",
    "        try:\n",
    "            a_expr = sp.sympify(a_clean.replace(\"^\", \"**\"))\n",
    "            b_expr = sp.sympify(b_clean.replace(\"^\", \"**\"))\n",
    "            return sp.simplify(a_expr - b_expr) == 0\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def system_prompt(type):\n",
    "    prompt = \"\"\n",
    "    if type == \"sample\":\n",
    "        prompt = \"\"\"You are a math-problem expert. Your task is to complete the step-by-step solution for the problem provided. Write each reasoning step on its own line in the exact form \\\"Step k: [your reasoning step]\\n\\\", numbering start from Step 1. When the final answer is obtained, write exactly one final line, \\\"Answer: [Final answer]\\\". Do NOT add explanations, extra steps, or any text after the \"Answer:\" line.\n",
    "            \n",
    "Format Guide with Examples:\n",
    "## Example 1 ##\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "## Example 2 ##\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Follow this structure exactly. Once you have written the \"Answer: \" line, stop generating.\"\"\"\n",
    "    if type == \"rollout\":\n",
    "        prompt = \"\"\"You are a math problem-solving expert. Continue solving the given problem step by step, strictly following the required format. Each new step must begin with \\\"Step k+1: ...\\\", \\\"Step k+2:...\\\", and so on, continuing from the last given step number. When the final answer is reached, write only one final line starting with: \\\"Answer: [Final Answer]\\\". Do not add any explanations, extra commentary, or additional text after the \"Answer:\" line. Your output must follow this exact step-by-step format with no deviations.\n",
    "\n",
    "Format Guide with Examples:\n",
    "## Example 1 ##\n",
    "Current solution steps:\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "\n",
    "Continue and finish the solution:\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "## Example 2 ##\n",
    "Current solution steps:\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "\n",
    "Continue and finish the solution:\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Keep the reasoning steps precise and factual. Your job is to complete the solution cleanly using this format structure.\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class MCReward:\n",
    "    STEP_PATTERN = re.compile(r\"Step\\s+\\d+:\")\n",
    "    ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*(?:$|\\n)\")\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def generate_solutions(self, question: str, sys_prompt: str, num_solutions: int):\n",
    "        prompt = f\"{sys_prompt}\\n\\n{question}\\n\"  # Prompt the model to start the step-by-step solution\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.model.device)\n",
    "        # Generate multiple solutions via sampling\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_solutions,\n",
    "            temperature=0.7,         # sampling temperature for diversity (adjust as needed)\n",
    "            top_p=0.8,               # top-p sampling for diversity\n",
    "            pad_token_id=self.tokenizer.eos_token_id  # pad token ID to avoid warning for some models\n",
    "        )\n",
    "        solutions = []\n",
    "        prompt_len = input_ids.shape[-1]\n",
    "        for i in range(num_solutions):\n",
    "            # Each output is the concatenation of the prompt and the generated completion.\n",
    "            generated_ids = outputs[i]\n",
    "            # Extract only the newly generated tokens (skip the prompt tokens).\n",
    "            gen_ids = generated_ids[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            solutions.append(text)\n",
    "        return solutions\n",
    "    \n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"\n",
    "        Given the model's generated solution text, split it into a list of steps and the final answer string.\n",
    "        Expects each step to start with 'Step X:' and the answer to start with 'Answer:'.\n",
    "        \"\"\"\n",
    "        steps = []\n",
    "        # answer = None\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"Step\"):\n",
    "                steps.append(line)\n",
    "            # if line.startswith(\"Answer:\"):\n",
    "            #     # Extract everything after \"Answer:\" as the answer\n",
    "            #     answer = line.split(\"Answer:\", 1)[1].strip()\n",
    "            #     # Stop if answer is found (anything after answer line is not needed)\n",
    "            #     break\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\n{question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            # prefix_steps = steps[:i+1]  \n",
    "            # prefix_text = f\"{sys_prompt}\\n\\n{question}\\n\" + \"\\n\".join(prefix_steps) + \"\\n\"\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(steps[: i + 1]) + \"\\n\", return_tensors=\"pt\").to(self.device) # steps up to current step i (0-indexed)\n",
    "            # Decide how to prompt the next part:\n",
    "            # if i < total_steps - 1:\n",
    "            #     next_step_num = i + 2  # (because i is 0-indexed and Step numbering is 1-indexed)\n",
    "            #     prefix_text += f\"Step {next_step_num}:\"\n",
    "            # else:\n",
    "            #     # If this is the last step of the solution, prompt the final answer.\n",
    "            #     prefix_text += \"Answer:\"\n",
    "            if i < total_steps - 1:\n",
    "                next_label = f\"Step {i + 2}:\"\n",
    "            else:\n",
    "                next_label = \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            # Build full prefix ids (avoid Python concat inefficiency by cat)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "\n",
    "            # prefix_ids = self.tokenizer.encode(prefix_text, return_tensors='pt').to(self.model.device)\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.7,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1] \n",
    "            # Check each rollout's final answer against the gold answer\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "                reward = correct_count / float(self.config.num_rollouts)\n",
    "                print(f\"[Rollout {idx}]:\", pred_answer, \"vs\", gold_answer, \"Reward:\", reward)\n",
    "            rewards.append(reward)\n",
    "            # for j in range(self.config.num_rollouts):\n",
    "            #     gen_ids = rollout_outputs[j][prefix_ids.shape[-1]:]  # only new tokens\n",
    "            #     completion_text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            #     print(f\"[Rollout {j}]:\", completion_text, \"\\n\")\n",
    "            #     # We find the last occurrence of \"Answer:\" in the completion (in case it's used earlier)\n",
    "            #     answer_index = completion_text.rfind(\"Answer:\")\n",
    "            #     if answer_index != -1:\n",
    "            #         predicted_answer = completion_text[answer_index + len(\"Answer:\"):].strip()\n",
    "            #     else:\n",
    "            #         # If the model didn't explicitly output \"Answer:\", treat the whole completion as answer\n",
    "            #         predicted_answer = completion_text.strip()\n",
    "            #     # Exact string match (after stripping) to the gold answer\n",
    "            #     if predicted_answer == str(gold_answer).strip():\n",
    "            #         correct_count += 1\n",
    "            #     print(f\"[Rollout {j}]:\",predicted_answer, gold_answer)\n",
    "            # reward = correct_count / float(self.config.num_rollouts)\n",
    "            # rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets(self, problems: List):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        for problem in problems:\n",
    "            question = problem[\"question\"]\n",
    "            # gold_answer = problem[\"gold_answer\"]\n",
    "            gold_answer = _sanitize(problem[\"gold_answer\"])\n",
    "            # Generate one or more solutions for this question\n",
    "            sample_prompt = system_prompt(\"sample\")\n",
    "            rollout_prompt = system_prompt(\"rollout\")\n",
    "            solutions = self.generate_solutions(question, sys_prompt=sample_prompt, num_solutions=self.config.samples_per_question)\n",
    "            for sol_text in solutions:\n",
    "                steps, answer = self.parse_solution(sol_text)\n",
    "                print(\"Parsed solution:\", steps, answer)\n",
    "                if answer is None: # If no answer was found in the solution (edge case), skip this solution\n",
    "                    continue\n",
    "                # Compute intermediate rewards for each step in this solution\n",
    "                rewards = self.compute_step_rewards(question, sys_prompt=rollout_prompt, steps=steps, gold_answer=gold_answer)\n",
    "                # Prepare the output entry\n",
    "                entry = {\n",
    "                    \"question\": question,\n",
    "                    \"completion\": steps,      # list of \"Step i: ...\" strings\n",
    "                    \"rewards\": rewards,       # list of reward values for each step\n",
    "                    \"answer\": answer,         # model's final answer from this solution\n",
    "                    \"gold_answer\": gold_answer\n",
    "                }\n",
    "                dataset.append(entry)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name =  \"Qwen/Qwen2.5-Math-7B\" # \"Qwen/Qwen2.5-Math-7B-Instruct\"  #\"Qwen/Qwen2.5-Math-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "problems = [\n",
    "    {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \n",
    "     \"gold_answer\": \"5\"},\n",
    "    # Add more problems as needed...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed solution: ['Step 1: Distribute the 3 on the right side: 2y - 7 = 3y - 12.', 'Step 2: Move all terms involving y to one side and constant terms to the other: 2y - 3y = -12 + 7.', 'Step 3: Simplify both sides: -y = -5.', 'Step 4: Divide both sides by -1 to solve for y: y = 5.'] 5\n",
      "[Rollout 0]: 5 vs 5 Reward: 0.2\n",
      "[Rollout 1]: 5 vs 5 Reward: 0.4\n",
      "[Rollout 2]: 5 vs 5 Reward: 0.6\n",
      "[Rollout 3]: 5 vs 5 Reward: 0.8\n",
      "[Rollout 4]: 5 vs 5 Reward: 1.0\n",
      "[Rollout 0]: 5 vs 5 Reward: 0.2\n",
      "[Rollout 1]: 5 vs 5 Reward: 0.4\n",
      "[Rollout 2]: 5 vs 5 Reward: 0.6\n",
      "[Rollout 3]: 5 vs 5 Reward: 0.8\n",
      "[Rollout 4]: 5 vs 5 Reward: 1.0\n",
      "[Rollout 0]: 5 vs 5 Reward: 0.2\n",
      "[Rollout 1]: 5 vs 5 Reward: 0.4\n",
      "[Rollout 2]: 5 vs 5 Reward: 0.6\n",
      "[Rollout 3]: 5 vs 5 Reward: 0.8\n",
      "[Rollout 4]: 5 vs 5 Reward: 1.0\n",
      "[Rollout 0]: boxed{5}\\) vs 5 Reward: 0.0\n",
      "[Rollout 1]: 5 vs 5 Reward: 0.2\n",
      "[Rollout 2]: 5 vs 5 Reward: 0.4\n",
      "[Rollout 3]: 5 vs 5 Reward: 0.6\n",
      "[Rollout 4]: 5 vs 5 Reward: 0.8\n",
      "{'question': 'Solve for y: 2y - 7 = 3(y - 4).', 'completion': ['Step 1: Distribute the 3 on the right side: 2y - 7 = 3y - 12.', 'Step 2: Move all terms involving y to one side and constant terms to the other: 2y - 3y = -12 + 7.', 'Step 3: Simplify both sides: -y = -5.', 'Step 4: Divide both sides by -1 to solve for y: y = 5.'], 'rewards': [1.0, 1.0, 1.0, 0.8], 'answer': '5', 'gold_answer': '5'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mcr = MCReward(config=PRMConfig , model=model, tokenizer=tokenizer)\n",
    "dataset = mcr.build_datasets(problems)\n",
    "\n",
    "# Print or inspect the dataset\n",
    "for entry in dataset:\n",
    "    print(entry)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "    cfg = PRMConfig(num_rollouts=3, samples_per_question=1)\n",
    "    mcr = MCReward(cfg, model, tokenizer)\n",
    "\n",
    "    problems = [\n",
    "        {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "    ]\n",
    "\n",
    "    ds = mcr.build_dataset(problems)\n",
    "    for row in ds:\n",
    "        print(row)\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        num_layers: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            hidden_size (int): Size of hidden layers\n",
    "            output_size (int): Size of output\n",
    "            dropout (float): Dropout rate\n",
    "            num_layers (Optional[int]): Number of hidden layers\n",
    "        \"\"\"\n",
    "        super(ProcessRewardModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers or 2\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_layers = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            in_features = hidden_size if i == 0 else hidden_size // (2 ** i)\n",
    "            out_features = hidden_size // (2 ** (i + 1))\n",
    "            hidden_layers.extend([\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.LayerNorm(out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        last_hidden_size = hidden_size // (2 ** (self.num_layers - 1))\n",
    "        self.fc_out = nn.Linear(last_hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        x = self.dropout(torch.relu(self.ln1(self.fc1(x))))\n",
    "        \n",
    "        # Hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        return x\n",
    "    \n",
    "    def get_complexity(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Total number of parameters\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('omega_prm.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PRMTrainer:\n",
    "    def __init__(self, mc: MCReward, config: PRMConfig):\n",
    "        self.mc   = mc\n",
    "        self.config    = config\n",
    "        self.device = mc.device\n",
    "        self.tok    = mc.tokenizer\n",
    "\n",
    "        # PRM 자체 초기화\n",
    "        feat_dim = mc.model.config.hidden_size        # LLM hidden size\n",
    "        self.prm = ProcessRewardModel(feat_dim, self.config.hidden_size, output_size=1).to(self.device)\n",
    "        self.opt = optim.AdamW(self.prm.parameters(), lr=self.config.learning_rate, weight_decay=0.01)\n",
    "        # self.crit = nn.BCELoss()\n",
    "        self.crit = nn.MSELoss()\n",
    "\n",
    "        # Initialize wandb if enabled\n",
    "        if self.config.use_wandb:\n",
    "            wandb.init(project=\"mc-prm\", name=\"prm-train\", config=vars(self.config))\n",
    "        # Create checkpoint directory\n",
    "        Path(self.config.checkpoint_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------------ utils\n",
    "    @torch.no_grad()\n",
    "    def _encode_features(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        LLM 의 hidden state → [CLS-pooling] 식 임베딩.\n",
    "        input_ids : [B, T]\n",
    "        return    : [B, feat_dim]\n",
    "        \"\"\"\n",
    "        # emb_layer = self.model.get_input_embeddings()          # shared embedding matrix\n",
    "        # emb = (emb_layer(token_ids)).mean(dim=1) \n",
    "        out = self.mcts.model(input_ids=input_ids,\n",
    "                              output_hidden_states=True,\n",
    "                              return_dict=True)\n",
    "        # 마지막 hidden-state의 0-번 토큰(CLS) 임베딩 사용\n",
    "        return out.hidden_states[-1][:, 0, :]\n",
    "\n",
    "    # ------------------------------------------------------ data preparation\n",
    "\n",
    "        ds = PRMDataset(texts, lbls,\n",
    "                        tokenizer=self.tok,\n",
    "                        max_length=self.cfg.max_length)\n",
    "        \n",
    "        print(\"PRMDataset(size={}, avg_len={:.1f})\".format(len(ds), sum(len(t.split()) for t in texts)/len(texts)))\n",
    "        return ds, structured\n",
    "\n",
    "    # ---------------------------------------------------------- train / valid\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool) -> float:\n",
    "        self.prm.train() if train else self.prm.eval()\n",
    "        tot = 0.0\n",
    "        for step, (ids, r) in enumerate(loader):\n",
    "            ids, r = ids.to(self.device), r.to(self.device)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats = self._encode_features(ids)\n",
    "                out   = self.prm(feats)\n",
    "                loss  = self.crit(out, r)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "            tot += loss.item()\n",
    "\n",
    "            if self.cfg.use_wandb and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    # \"epoch\"     : self.cur_epoch,   # train_prm 에서 설정\n",
    "                    \"step\"      : step\n",
    "                })\n",
    "        return tot / len(loader)\n",
    "\n",
    "    def train_prm(\n",
    "        self,\n",
    "        train_questions: List[str],\n",
    "        val_questions  : List[str],\n",
    "        num_epochs: int = 5,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        # 1) 데이터 수집\n",
    "        train_ds, _ = self.build_dataset(train_questions)\n",
    "        val_ds,   _ = self.build_dataset(val_questions)\n",
    "        print(\"train ds:\", train_ds)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.cfg.batch_size, shuffle=True, num_workers=self.cfg.num_workers)\n",
    "        val_loader = DataLoader(val_ds, batch_size=self.cfg.batch_size,shuffle=False, num_workers=self.cfg.num_workers)\n",
    "\n",
    "        # 2) 학습 loop\n",
    "        hist = {\"train\": [], \"val\": []}\n",
    "        best = float(\"inf\")\n",
    "        for ep in range(num_epochs):\n",
    "            tr = self._run_epoch(train_loader, train=True)\n",
    "            vl = self._run_epoch(val_loader,   train=False)\n",
    "            hist[\"train\"].append(tr)\n",
    "            hist[\"val\"].append(vl)\n",
    "            print(f\"[EP {ep}] train {tr:.4f} | val {vl:.4f}\")\n",
    "\n",
    "            if self.cfg.use_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\"     : ep,\n",
    "                    \"train_loss\": tr,\n",
    "                    \"val_loss\"  : vl,\n",
    "                })\n",
    "            if vl < best:\n",
    "                best = vl\n",
    "                self.save_checkpoint(ep, vl)\n",
    "                # torch.save(self.prm.state_dict(), Path(self.cfg.checkpoint_dir) / \"best_prm.pt\")\n",
    "        return hist\n",
    "\n",
    "    # -------------------------------------------------------------- metrics\n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"params\": sum(p.numel() for p in self.prm.parameters()),\n",
    "        }\n",
    "    \n",
    "    # -------------------------------------------------------------- save checkpoints\n",
    "    def save_checkpoint(self, epoch: int, validation_loss: float):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.prm.state_dict(),\n",
    "            'optimizer_state_dict': self.opt.state_dict(),\n",
    "            'validation_loss': validation_loss,\n",
    "            'config': self.cfg.__dict__\n",
    "        }\n",
    "        path = Path(self.cfg.checkpoint_dir) / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f\"Saved checkpoint to {path}\")\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.prm.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            logger.info(f\"Loaded checkpoint from {path}\")\n",
    "            return checkpoint['epoch'], checkpoint['validation_loss']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/peer/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    max_new_tokens: int = 386\n",
    "    num_rollouts: int = 5\n",
    "    reward_threshold: float = 0.2\n",
    "    samples_per_question: int = 1\n",
    "    use_llm: bool = True\n",
    "    use_contri: bool = True\n",
    "    # PRMTrainer config\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 5e-4\n",
    "    hidden_size: int = 256\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 2\n",
    "    # Misc config\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"mc_prm\"\n",
    "    run_name: str = \"test_gsm8k_0623\"\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                        UTILITY: ANSWER NORMALISATION                         #\n",
    "################################################################################\n",
    "import sympy as sp\n",
    "\n",
    "def _strip_markup(ans: str) -> str:\n",
    "    \"\"\"Remove common LaTeX/markup & variable tags.\"\"\"\n",
    "    # # Remove LaTeX inline math wrappers \\( … \\) or \\[ … \\]\n",
    "    # ans = re.sub(r\"\\\\[\\[(](.*?)[\\\\\\])]\", r\"\\1\", ans)\n",
    "    # # Remove \\boxed{…}\n",
    "    # ans = re.sub(r\"\\\\boxed\\{([^}]*)\\}\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\\\\\[.*?\\\\\\]\", \"\", ans)\n",
    "    ans = re.sub(r\"\\$\\$.*?\\$\\$\", \"\", ans)\n",
    "    # Remove inline LaTeX: \\( ... \\) and $...$\n",
    "    ans = re.sub(r\"\\\\\\((.*?)\\\\\\)\", r\"\\1\", ans)\n",
    "    ans = re.sub(r\"\\$(.*?)\\$\", r\"\\1\", ans)\n",
    "    # Remove \\boxed{...}\n",
    "    ans = re.sub(r\"\\\\boxed\\s*{([^}]*)}\", r\"\\1\", ans)\n",
    "    # Remove LaTeX commands like \\text{...}, \\frac{...}, etc.\n",
    "    ans = re.sub(r\"\\\\[a-zA-Z]+\\s*(\\{[^{}]*\\})?\", \"\", ans)\n",
    "    # Remove variable assignments like \"y =\" or \"x=\" at start\n",
    "    ans = re.sub(r\"^[a-zA-Z]\\s*=\\s*\", \"\", ans)\n",
    "    # Trim outer $ … $ if present\n",
    "    ans = ans.strip()\n",
    "    if ans.startswith(\"$\") and ans.endswith(\"$\"):\n",
    "        ans = ans[1:-1]\n",
    "    return ans.strip()\n",
    "\n",
    "def _sanitize(text: str) -> str:\n",
    "    \"\"\"Normalise a candidate answer string for comparison.\"\"\"\n",
    "    text = _strip_markup(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[\\s\\.;:,]+$\", \"\", text)     # trailing punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)              # collapse spaces\n",
    "    return text\n",
    "\n",
    "def _to_float(expr: str) -> Optional[float]:\n",
    "    try:\n",
    "        return float(eval(expr.replace(\"^\", \"**\")))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _numeric_equiv(a: str, b: str) -> bool:\n",
    "    \"\"\"Return True if `a` and `b` are numerically equivalent or exact match.\"\"\"\n",
    "    a_clean, b_clean = map(_sanitize, (a, b))\n",
    "    if a_clean == b_clean:\n",
    "        return True\n",
    "\n",
    "    # Attempt simple numeric evaluation\n",
    "    a_val, b_val = _to_float(a_clean), _to_float(b_clean)\n",
    "    if a_val is not None and b_val is not None:\n",
    "        return math.isclose(a_val, b_val, rel_tol=1e-6)\n",
    "\n",
    "    if sp is not None:\n",
    "        try:\n",
    "            a_expr = sp.sympify(a_clean.replace(\"^\", \"**\"))\n",
    "            b_expr = sp.sympify(b_clean.replace(\"^\", \"**\"))\n",
    "            return sp.simplify(a_expr - b_expr) == 0\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def system_prompt(type):\n",
    "    prompt = \"\"\n",
    "    if type == \"sample\":\n",
    "        prompt = \"\"\"You are a math-problem expert. Your task is to complete the step-by-step solution for the problem provided. Write each reasoning step on its own line in the exact form \\\"Step k: [your reasoning step]\\n\\\", numbering start from Step 1. When the final answer is obtained, write exactly one final line, \\\"Answer: [Final answer]\\\". Do NOT add explanations, extra steps, or any text after the \"Answer:\" line.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Follow the FORMAT GUIDE structure exactly. Generate rationales step-by-step, not directly to the final answer. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    if type == \"rollout\":\n",
    "        prompt = \"\"\"You are a math problem-solving expert. Continue solving the given problem step by step, strictly following the required format. Each new step must begin with \\\"Step k+1: ...\\\", \\\"Step k+2:...\\\", and so on, continuing from the last given step number. When the final answer is reached, write only one final line starting with: \\\"Answer: [Final Answer]\\\". Do not add any explanations, extra commentary, or additional text after the \"Answer:\" line. Your output must follow this exact step-by-step format with no deviations.\n",
    "\n",
    "**Format Guide**: (You MUST write \"Step \" before numbering the step.)\n",
    "Step 1: [Step 1 reasoning]\\n\n",
    "Step 2: [Step 2 reasoning]\\n\n",
    "...\n",
    "Step k: [Step k reasoning]\\n\n",
    "Continue and finish the solution:\n",
    "Step k+1: [Step k+1 reasoning]\\n\n",
    "...\n",
    "Answer: [Final answer]\n",
    "\n",
    "Format Guide with Examples:\n",
    "<Example 1>\n",
    "Current solution steps:\n",
    "Problem: Find the sum of the first 8 positive even integers.\n",
    "Step 1: The first 8 even integers are 2, 4, 6, 8, 10, 12, 14, 16.\n",
    "Step 2: Use the formula for an arithmetic series: S = n·(first + last)/2.\n",
    "Continue and finish the solution:\n",
    "Step 3: Substitute n=8, first=2, last=16 to get S = 8·(2+16)/2 = 8·9 = 72.\n",
    "Answer: 72\n",
    "\n",
    "<Example 2>\n",
    "Current solution steps:\n",
    "Problem: Determine the next number in the sequence 2, 4, 8, 16.\n",
    "Step 1: Notice each term is obtained by multiplying the previous term by 2.\n",
    "Continue and finish the solution:\n",
    "Step 2: Multiply 16 by 2, 16 * 2 = 32.\n",
    "Answer: 32\n",
    "\n",
    "Keep the reasoning steps precise and factual and complete the solution. Follow the FORMAT GUIDE structure exactly. **Do NOT** write anything after the final 'Answer:' line. Always start stepwise reasoning with \"Step {i-th}: \" form.\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "# STEP_PATTERN = re.compile(r\"Step\\s+\\d+:\")\n",
    "# ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*(?:$|\\n)\")\n",
    "\n",
    "class MCReward:\n",
    "    STEP_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Step\\s*              # word 'Step' (case-insensitive)\n",
    "        (\\d+)                # capture step number\n",
    "        \\s*[:.\\-]            # separator (: . or -)\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    "    )\n",
    "    ANSWER_PATTERN = re.compile(\n",
    "        r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "            Answer               # word 'Answer'\n",
    "            \\s*[:.\\-]\\s*         # separator\n",
    "            (.+?)\\s*$            # capture everything after\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    "    )\n",
    "    ## Masked rewards ##\n",
    "    OP_TOKENS = [\"add\", \"plus\", \"sum\", \"subtract\", \"minus\",\n",
    "             \"multiply\", \"times\", \"product\", \"divide\", \"quotient\"]\n",
    "    _MASK_PATTERN = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "        # {ops_pattern}|                # operator patterns\n",
    "            \\b\\d+(?:\\.\\d+)?\\b         # integers / decimals\n",
    "          | \\b\\d+/\\d+\\b                 # simple fractions\n",
    "        #   | \\b[a-zA-Z]\\b                 # single‑letter variables\n",
    "        )\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: \"PRMConfig\", model, tokenizer):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Function to generate one or more step-by-step solutions for a given question.\n",
    "    def generate_solutions(self, question: str, sys_prompt: str, num_solutions: int):\n",
    "        prompt = f\"{sys_prompt}\\n\\n{question}\\n\"  # Prompt the model to start the step-by-step solution\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.model.device)\n",
    "        # Generate multiple solutions via sampling\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_solutions,\n",
    "            temperature=0.8,         # sampling temperature for diversity (adjust as needed)\n",
    "            top_p=0.8,               # top-p sampling for diversity\n",
    "            pad_token_id=self.tokenizer.eos_token_id  # pad token ID to avoid warning for some models\n",
    "        )\n",
    "        solutions = []\n",
    "        prompt_len = input_ids.shape[-1]\n",
    "        for i in range(num_solutions):\n",
    "            # Each output is the concatenation of the prompt and the generated completion.\n",
    "            generated_ids = outputs[i]\n",
    "            # Extract only the newly generated tokens (skip the prompt tokens).\n",
    "            gen_ids = generated_ids[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            solutions.append(text)\n",
    "            print(f\"{i}-th Sampled Solutions:\",text)\n",
    "        return solutions\n",
    "    \n",
    "    def gsm8k_solutions(self, question: str, gold_solution: str):\n",
    "        # 1. Split lines *before* the final answer marker (#### …)\n",
    "        lines: List[str] = []\n",
    "        gold_answer: str = \"\"\n",
    "        _ANSWER_RE = re.compile(r\"####\\s*(.+?)\\s*$\")\n",
    "\n",
    "        for raw_ln in gold_solution.splitlines():\n",
    "            ln = raw_ln.strip()\n",
    "            if not ln:\n",
    "                continue  # skip empty\n",
    "            ans_match = _ANSWER_RE.match(ln)\n",
    "            if ans_match:\n",
    "                gold_answer = ans_match.group(1).strip()\n",
    "                break  # everything after #### is ignored\n",
    "            lines.append(ln)\n",
    "\n",
    "        if not gold_answer:\n",
    "            raise ValueError(\"Could not find final answer marker '#### <answer>' in gold_solution.\")\n",
    "\n",
    "        # 2. Prefix each explanatory line with \"Step i:\"\n",
    "        solution_steps = [f\"Step {i + 1}: {txt}\" for i, txt in enumerate(lines)]\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"solution\": solution_steps,\n",
    "            \"gold_answer\": gold_answer,\n",
    "        }\n",
    "\n",
    "    # Function to parse a solution text into steps and final answer.\n",
    "    def _extract_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Try multiple heuristics / regexes to pull out an answer string.\"\"\"\n",
    "        # Primary regex (robust to Answer:, Answer ‑, etc.)\n",
    "        match = self.ANSWER_PATTERN.search(text)\n",
    "        if match:\n",
    "            return _sanitize(match.group(1))\n",
    "        \n",
    "        # Fallback 1: last non‑empty line if it looks simple / numeric\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            candidate = lines[-1]\n",
    "            if re.search(r\"\\d\", candidate):  # contains digit\n",
    "                return _sanitize(candidate)\n",
    "\n",
    "        # Fallback 2: look for last line that starts with 'Answer'\n",
    "        for line in reversed(text.splitlines()):\n",
    "            if line.strip().lower().startswith(\"answer\"):\n",
    "                return _sanitize(line.split(\"Answer\", 1)[-1])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_solution(self, solution_text: str):\n",
    "        \"\"\"Split each step to start with 'Step X:' and the answer to start with 'Answer:'.\"\"\"\n",
    "        steps = []\n",
    "        # Split by lines to identify steps and answer\n",
    "        for line in solution_text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # if line.startswith(\"Step\"):\n",
    "            #     steps.append(line)\n",
    "            if self.STEP_PATTERN.match(line):\n",
    "                cleaned = re.sub(r'^[\\s>#*\\-]+', '', line)\n",
    "                steps.append(cleaned)\n",
    "            answer = self._extract_answer(solution_text)\n",
    "        return steps, answer\n",
    "    \n",
    "    # Function to estimate intermediate rewards for each step via rollouts.\n",
    "    def compute_step_rewards(self, question, sys_prompt, steps, gold_answer):\n",
    "        \"\"\"\n",
    "        For each prefix ending at a given step in 'steps', generate rollouts and compute the reward \n",
    "        (fraction of rollouts ending in the correct answer). Returns a list of reward values corresponding to each step.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        # Pre‑encode static prefix (sys_prompt + question) once for efficiency\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(steps[: i + 1]) + \"\\n\", return_tensors=\"pt\").to(self.device) # steps up to current step i (0-indexed)\n",
    "            # Decide how to prompt the next part:\n",
    "            if i < total_steps - 1:\n",
    "                next_label = f\"Step {i + 2}:\"\n",
    "            else:\n",
    "                next_label = \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            # Build full prefix ids (avoid Python concat inefficiency by cat)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "            # prefix_ids = self.tokenizer.encode(prefix_text, return_tensors='pt').to(self.model.device)\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1] \n",
    "            # Check each rollout's final answer against the gold answer\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                # print(f\"[{i+1}-th Step, {idx}-th Original Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            reward = correct_count / float(self.config.num_rollouts)\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    \n",
    "    # Masked solution paths\n",
    "    def model_masking(self, text: str, *, max_new_tokens: int = 64) -> str:\n",
    "        prompt = \"In the sentence below, mask any word or expression that seems crucial for solving the math step. This may include key numbers, variables, or action words (like operations), but you should decide what matters. Replace each important item with '[MASKED]'. Keep everything else unchanged. Return ONE line.\\n\\nSentence: \\\"{sent}\\\"\\nRewritten:\".format(sent=text)\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        out_ids   = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0, top_p=0.0,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out_ids[0][input_ids.shape[-1]:],\n",
    "                                     skip_special_tokens=True).strip()\n",
    "\n",
    "    def perturbed_step_rewards(self, question: str, sys_prompt: str, steps: List[str], gold_answer: str, use_llm: bool = True) -> List[float]:\n",
    "        \"\"\"Compute MC correctness rates *after masking* the current step.\n",
    "        Each step `i` is replaced with a *perturbed* version where important\n",
    "        tokens (numbers, fractions, single‑letter variables) are substituted by\n",
    "        the literal string ``[MASKED]``. All preceding steps remain intact.\n",
    "        \"\"\"\n",
    "        ptb_rewards: List[float] = []\n",
    "        total_steps = len(steps)\n",
    "        base_prompt = f\"{sys_prompt}\\n\\nProblem: {question}\\n\"\n",
    "        base_ids = self.tokenizer.encode(base_prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        for i in range(total_steps):\n",
    "            # 1. Perturb *only* step i\n",
    "            orig_step = steps[i] \n",
    "            step_match = re.match(r\"^[\\s>#*\\-]*Step\\s*\\d+\\s*[:.\\-]\\s*\", orig_step, flags=re.I)\n",
    "            prefix = step_match.group(0) if step_match else \"\"\n",
    "            # ② 나머지 부분(body)만 마스킹\n",
    "            body   = steps[i][len(prefix):]                       # 접두사 뒷부분\n",
    "            if use_llm:\n",
    "                masked_body = self.model_masking(body)\n",
    "            else:\n",
    "                masked_body = self._MASK_PATTERN.sub(\"[MASKED]\", body)\n",
    "            # ③ 접두사 + 마스킹된 body\n",
    "            masked_step = prefix + masked_body    \n",
    "            ptb_prefix_steps = steps[:i] + [masked_step]\n",
    "            print(\"perturbed step:\", ptb_prefix_steps)\n",
    "\n",
    "            prefix_tokens = self.tokenizer.encode(\"\\n\".join(ptb_prefix_steps) + \"\\n\", return_tensors=\"pt\").to(self.device)\n",
    "            next_label = f\"Step {i + 2}:\" if i < total_steps - 1 else \"Answer:\"\n",
    "            cont_ids = self.tokenizer.encode(next_label, return_tensors=\"pt\").to(self.device)\n",
    "            prefix_ids = torch.cat([base_ids, prefix_tokens, cont_ids], dim=-1)\n",
    "\n",
    "            rollout_outputs = self.model.generate(\n",
    "                prefix_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=self.config.num_rollouts,\n",
    "                temperature=0.8,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            new_token_start = prefix_ids.shape[-1]\n",
    "            correct_count = 0\n",
    "            for idx, seq in enumerate(rollout_outputs):\n",
    "                completion = self.tokenizer.decode(seq[new_token_start:], skip_special_tokens=True)\n",
    "                pred_answer = self._extract_answer(completion)\n",
    "                # print(f\"[{i+1}-th Step, {idx}-th Rollout]\", completion, \"Pred Answer\", pred_answer)\n",
    "                if pred_answer is not None and _numeric_equiv(pred_answer, gold_answer):\n",
    "                    correct_count += 1\n",
    "            ptb_rewards.append(correct_count / float(self.config.num_rollouts))\n",
    "        return ptb_rewards\n",
    "\n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets(self, problems: List):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        for problem in problems:\n",
    "            question = problem[\"question\"]\n",
    "            # gold_answer = problem[\"gold_answer\"]\n",
    "            gold_answer = _sanitize(problem[\"gold_answer\"])\n",
    "            # Generate one or more solutions for this question\n",
    "            sample_prompt = system_prompt(\"sample\")\n",
    "            rollout_prompt = system_prompt(\"rollout\")\n",
    "            solutions = self.generate_solutions(question, sys_prompt=sample_prompt, num_solutions=self.config.samples_per_question)\n",
    "            \n",
    "            for sol_text in solutions:\n",
    "                steps, answer = self.parse_solution(sol_text)\n",
    "                # print(\"Parsed solution:\", steps, answer)\n",
    "                if answer is None: # If no answer was found in the solution (edge case), skip this solution\n",
    "                    continue\n",
    "                # 2. Compute *original* & *perturbed* per‑step rewards\n",
    "                # ----------------------------------------------------------\n",
    "                ori_rewards = self.compute_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                ptb_rewards = self.perturbed_step_rewards(\n",
    "                    question=question,\n",
    "                    sys_prompt=rollout_prompt,\n",
    "                    steps=steps,\n",
    "                    gold_answer=gold_answer,\n",
    "                )\n",
    "                # Align lengths (robustness)\n",
    "                if len(ptb_rewards) != len(ori_rewards):\n",
    "                    ptb_rewards = ptb_rewards[: len(ori_rewards)]\n",
    "                contributions = [o - p for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "                # print(steps, \"\\n\", rewards)\n",
    "                # Prepare the output entry\n",
    "                entry = {\n",
    "                    \"question\": question,\n",
    "                    \"completion\": steps,          # list[str] (Step i: ...)\n",
    "                    \"ori_rewards\": ori_rewards,    # list[float]\n",
    "                    \"ptb_rewards\": ptb_rewards,    # list[float]\n",
    "                    \"contributions\": contributions,  # ori − ptb\n",
    "                    \"answer\": answer,\n",
    "                    \"gold_answer\": gold_answer,\n",
    "                }\n",
    "                dataset.append(entry)\n",
    "        return dataset\n",
    "    \n",
    "    # Build datasets based on input datas\n",
    "    def build_datasets_gsm8k(self, split: Optional[str] = None):\n",
    "        dataset = []  # will hold the output list of dicts\n",
    "        rollout_prompt = system_prompt(\"rollout\")\n",
    "        ds_full = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "        # problems  = ds_full[split]\n",
    "        problems  = ds_full[split].select(range(2))\n",
    "\n",
    "        for problem in tqdm(problems):\n",
    "            parsed = self.gsm8k_solutions(problem[\"question\"], problem[\"answer\"])\n",
    "            question = parsed[\"question\"]\n",
    "            steps = parsed[\"solution\"]\n",
    "            gold_answer = _sanitize(parsed[\"gold_answer\"])\n",
    "            # print(\"Parsed:\", question, \"\\n\", steps, \"\\nGold:\", gold_answer)\n",
    "            \n",
    "            ori_rewards = self.compute_step_rewards(question, rollout_prompt, steps, gold_answer)\n",
    "            ptb_rewards = self.perturbed_step_rewards(question, rollout_prompt, steps, gold_answer, self.config.use_llm)\n",
    "            print(\"original rewards:\", ori_rewards)\n",
    "            print(\"perturbed rewards:\", ptb_rewards)\n",
    "            # Align lengths (robustness)\n",
    "            if len(ptb_rewards) != len(ori_rewards):\n",
    "                ptb_rewards = ptb_rewards[: len(ori_rewards)]\n",
    "            contributions = [round(o - p, 4) for o, p in zip(ori_rewards, ptb_rewards)]\n",
    "            print(\"contributions:\", contributions)\n",
    "\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"completion\": steps,          # list[str] (Step i: ...)\n",
    "                \"ori_rewards\": ori_rewards,    # list[float]\n",
    "                \"ptb_rewards\": ptb_rewards,    # list[float]\n",
    "                \"contributions\": contributions,  # ori − ptb\n",
    "                \"answer\": gold_answer,\n",
    "                \"gold_answer\": gold_answer,\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class StepwisePRMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    build_datasets() 가 반환한 entries(list[dict])를\n",
    "    (input_ids, scalar_reward) 샘플들로 변환한다.\n",
    "\n",
    "    한 entry = {question, completion[steps], rewards[float], …}\n",
    "    →  (Problem + Step1,   r1)\n",
    "        (Problem + Step1 \\nStep2,   r2) …\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        use_contr: bool = True,\n",
    "        *,\n",
    "        cache_encodings: bool = True,\n",
    "        preprocess: bool = True,\n",
    "    ):\n",
    "        self.tokenizer   = tokenizer\n",
    "        self.max_length  = max_length\n",
    "        self.use_contri    = use_contr\n",
    "        self.cache       = {} if cache_encodings else None\n",
    "        self.samples: List[Tuple[str, float]] = []\n",
    "\n",
    "        for e in entries:\n",
    "            q_txt   = e[\"question\"]\n",
    "            steps   = e[\"completion\"]\n",
    "            o_rewards = e[\"ori_rewards\"]\n",
    "            contri = e[\"contributions\"]\n",
    "            assert len(steps) == len(o_rewards)\n",
    "\n",
    "            if self.use_contri:\n",
    "                rewards = contri\n",
    "            else:\n",
    "                rewards = o_rewards\n",
    "\n",
    "            prefix_lines = [f\"Problem: {q_txt}\"]\n",
    "            for step_txt, r in zip(steps, rewards):\n",
    "                prefix_lines.append(step_txt)\n",
    "                full_txt = \"\\n\".join(prefix_lines)\n",
    "                if preprocess:\n",
    "                    full_txt = self._clean(full_txt)\n",
    "                self.samples.append((full_txt, float(r)))   # (text, reward)\n",
    "\n",
    "    # --------------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(txt: str) -> str:\n",
    "        \"\"\"whitespace normalize + 소문자화(선택적) 등 간단 전처리\"\"\"\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return txt\n",
    "\n",
    "    # --------------------------------------------------------------------- dunder\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, reward = self.samples[idx]\n",
    "\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            ids = self.cache[text]\n",
    "        else:\n",
    "            ids = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[text] = ids\n",
    "\n",
    "        return ids, torch.tensor(reward, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        num_layers: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            hidden_size (int): Size of hidden layers\n",
    "            output_size (int): Size of output\n",
    "            dropout (float): Dropout rate\n",
    "            num_layers (Optional[int]): Number of hidden layers\n",
    "        \"\"\"\n",
    "        super(ProcessRewardModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers or 2\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_layers = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            in_features = hidden_size if i == 0 else hidden_size // (2 ** i)\n",
    "            out_features = hidden_size // (2 ** (i + 1))\n",
    "            hidden_layers.extend([\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.LayerNorm(out_features),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        last_hidden_size = hidden_size // (2 ** (self.num_layers - 1))\n",
    "        self.fc_out = nn.Linear(last_hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input layer\n",
    "        x = self.dropout(torch.relu(self.ln1(self.fc1(x))))\n",
    "        # Hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        # Output layer\n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        return x\n",
    "    \n",
    "    def get_complexity(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('omega_prm.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    (1) entries(list[dict]) → StepwisePRMDataset\n",
    "    (2) LLM encoder + PRM head fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PRMConfig, model, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "        # ----------------------------- Backbone model LLM (frozen or fine-tuned)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model  = model\n",
    "        self.model.eval()       # LLM은 feature extractor로 freeze\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        feat_dim = self.model.config.hidden_size\n",
    "        self.prm = ProcessRewardModel(feat_dim, hidden_size=cfg.hidden_size, output_size=1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.prm.to(self.device)\n",
    "\n",
    "        self.opt  = optim.AdamW(self.prm.parameters(), lr=cfg.learning_rate)\n",
    "        # self.crit = nn.MSELoss()\n",
    "        self.crit = nn.BCELoss()\n",
    "\n",
    "        self.ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "        self.ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.wandb_run = None\n",
    "        if cfg.use_wandb:                                  # <-- config에 플래그\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=cfg.wandb_project,                 # e.g. \"omega-prm\"\n",
    "                name=cfg.run_name,                         # e.g. \"qwen7b-prm\"\n",
    "                config=vars(cfg),                          # 모든 하이퍼파라미터 로깅\n",
    "                # reinit=True,\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------- features\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\n",
    "        \"\"\"\n",
    "        out = self.model(\n",
    "            input_ids=ids,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        return out.hidden_states[-1][:, 0, :]     # CLS embedding\n",
    "\n",
    "    # ----------------------------------------------------------------- loop util\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool, epoch_idx: int) -> float:\n",
    "        self.prm.train(train)\n",
    "        total = 0.0\n",
    "        for step, (ids, reward) in enumerate(loader):\n",
    "            ids, reward = ids.to(self.device), reward.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats  = self._encode(ids)\n",
    "                pred   = self.prm(feats).squeeze(-1)\n",
    "                loss   = self.crit(pred, reward)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "            # -------- minibatch logging --------\n",
    "            if self.wandb_run and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"epoch\": epoch_idx + step / len(loader),\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                    \"grad_norm\": sum(p.grad.data.norm(2).item()\n",
    "                                     for p in self.prm.parameters()\n",
    "                                     if p.grad is not None),\n",
    "                })\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ----------------------------------------------------------------- public\n",
    "    def fit(self, train_entries: List[dict], val_entries: List[dict]) -> Dict[str, List[float]]:\n",
    "        train_ds = StepwisePRMDataset(train_entries, self.tokenizer, self.cfg.max_new_tokens, self.cfg.use_contri)\n",
    "        val_ds   = StepwisePRMDataset(val_entries,   self.tokenizer, self.cfg.max_new_tokens, self.cfg.use_contri)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, batch_size=self.cfg.batch_size, shuffle=False,\n",
    "            num_workers=self.cfg.num_workers, pin_memory=True,\n",
    "        )\n",
    "\n",
    "        history = {\"train\": [], \"val\": []}\n",
    "        best_val = float(\"inf\")\n",
    "\n",
    "        for ep in range(self.cfg.epochs):\n",
    "            tr_loss = self._run_epoch(train_loader, train=True,  epoch_idx=ep)\n",
    "            vl_loss = self._run_epoch(val_loader,   train=False, epoch_idx=ep)\n",
    "\n",
    "            # -------- epoch logging --------\n",
    "            if self.wandb_run:\n",
    "                wandb.log({\"train_loss\": tr_loss,\"val_loss\": vl_loss,\"epoch\": ep})\n",
    "\n",
    "            history[\"train\"].append(tr_loss)\n",
    "            history[\"val\"].append(vl_loss)\n",
    "            print(f\"[Epoch {ep+1}/{self.cfg.epochs}] train={tr_loss:.4f}  val={vl_loss:.4f}\")\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if vl_loss < best_val:\n",
    "                best_val = vl_loss\n",
    "                self._save_checkpoint(\"best_prm.pt\", epoch=ep, val_loss=vl_loss)\n",
    "        \n",
    "        self._save_checkpoint(\"last_prm.pt\", epoch=self.cfg.epochs - 1, val_loss=vl_loss)\n",
    "        return history\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Checkpoint helpers\n",
    "    def _save_checkpoint(self, filename: str, *, epoch: int, val_loss: float) -> None:\n",
    "        path = self.ckpt_dir / filename\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"prm_state\": self.prm.state_dict(),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"config\": vars(self.cfg),              # hyper‑params for reproducibility\n",
    "            \"model_name_or_path\": getattr(self.model, \"name_or_path\", None),\n",
    "            \"tokenizer_config\": self.tokenizer.__dict__.get(\"init_kwargs\", {}),\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        print(f\"[CKPT] Saved ⇒ {path}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # @classmethod\n",
    "    # def load_for_inference(\n",
    "    #     cls,\n",
    "    #     checkpoint_path: str | Path,\n",
    "    #     *,\n",
    "    #     device: Optional[torch.device] = None,\n",
    "    #     base_model: Optional[str | \"AutoModelForCausalLM\"] = None,\n",
    "    #     tokenizer: Optional[\"AutoTokenizer\"] = None,\n",
    "    # ) -> \"PRMTrainer\":\n",
    "    #     \"\"\"Instantiate **frozen** backbone + PRM head from a checkpoint for\n",
    "    #     *inference* (no optimiser).\n",
    "    #     \"\"\"\n",
    "    #     ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    #     cfg_dict = ckpt.get(\"config\", {})\n",
    "\n",
    "    #     # rebuild cfg object (simple Namespace‑style fallback)\n",
    "    #     from types import SimpleNamespace\n",
    "\n",
    "    #     cfg = SimpleNamespace(**cfg_dict)\n",
    "\n",
    "    #     # load / reuse backbone + tokenizer\n",
    "    #     if isinstance(base_model, str) or base_model is None:\n",
    "    #         base_model_name = base_model or ckpt.get(\"model_name_or_path\")\n",
    "    #         backbone = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    #     else:\n",
    "    #         backbone = base_model\n",
    "\n",
    "    #     if tokenizer is None:\n",
    "    #         tokenizer = AutoTokenizer.from_pretrained(backbone.name_or_path)\n",
    "\n",
    "    #     trainer = cls(cfg, backbone, tokenizer, device=device)\n",
    "    #     trainer.prm.load_state_dict(ckpt[\"prm_state\"])\n",
    "    #     trainer.prm.eval()\n",
    "    #     print(f\"[CKPT] Loaded PRM weights from {checkpoint_path}\")\n",
    "    #     return trainer\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Simple inference helper\n",
    "    @torch.no_grad()\n",
    "    def predict_reward(self, text: str) -> float:\n",
    "        ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "        feat = self._encode(ids)\n",
    "        return float(torch.sigmoid(self.prm(feat)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish load model and config!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-7B\"      # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "cfg = PRMConfig()\n",
    "print(\"Finish load model and config!\")\n",
    "\n",
    "# problems = [\n",
    "#     {\"question\": \"Each notebook costs $5. Sarah buys 4 notebooks and pays with a $50 bill. How much change does she get?\", \"gold_answer\": \"30\"},\n",
    "#     {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "#     # Add more problems as needed...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbed step: ['Step 1: Natalia sold 48/2 = [MASKED] clips in May.']\n",
      "perturbed step: ['Step 1: Natalia sold 48/2 = <<48/2=24>>24 clips in May.', 'Step 2: Natalia sold [MASKED] clips altogether in April and May.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:35<00:35, 35.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original rewards: [1.0, 1.0]\n",
      "perturbed rewards: [0.0, 0.4]\n",
      "contributions: [1.0, 0.6]\n",
      "perturbed step: ['Step 1: \"Weng earns [MASKED] = $[MASKED] per minute.\"']\n",
      "perturbed step: ['Step 1: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.', 'Step 2: \"Working 50 minutes, she earned [MASKED] x [MASKED] = [MASKED].\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:12<00:00, 36.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original rewards: [0.2, 0.2]\n",
      "perturbed rewards: [0.0, 0.0]\n",
      "contributions: [0.2, 0.2]\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'completion': ['Step 1: Natalia sold 48/2 = <<48/2=24>>24 clips in May.', 'Step 2: Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.'], 'ori_rewards': [1.0, 1.0], 'ptb_rewards': [0.0, 0.4], 'contributions': [1.0, 0.6], 'answer': '72', 'gold_answer': '72'}\n",
      "--------------------------------------------------------------------------------\n",
      "{'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'completion': ['Step 1: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.', 'Step 2: Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.'], 'ori_rewards': [0.2, 0.2], 'ptb_rewards': [0.0, 0.0], 'contributions': [0.2, 0.2], 'answer': '10', 'gold_answer': '10'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = PRMConfig()\n",
    "mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "reward_ds_small = mcr.build_datasets_gsm8k(split=\"train\")\n",
    "\n",
    "# Print or inspect the dataset\n",
    "for entry in reward_ds_small:\n",
    "    print(entry)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "# entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "# random.shuffle(entries_raw)\n",
    "# split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "split_idx = 1\n",
    "train_entries   = reward_ds_small[:split_idx]\n",
    "val_entries     = reward_ds_small[split_idx:] or reward_ds_small[:1]   # 최소 1개 확보\n",
    "\n",
    "trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "history = trainer.fit(train_entries, val_entries)\n",
    "print(\"Training complete. Loss history:\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    model_name =  \"Qwen/Qwen2.5-Math-7B\" # \"Qwen/Qwen2.5-Math-7B-Instruct\"  #\"Qwen/Qwen2.5-Math-7B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    cfg = PRMConfig()\n",
    "    print(\"Finish load model and config!\")\n",
    "\n",
    "    problems = [\n",
    "        {\"question\": \"A train travels 120 km in 2 hours and then 180 km in 3 hours. What is the average speed of the train?\", \"gold_answer\": \"60\"},\n",
    "        {\"question\": \"Solve for y: 2y - 7 = 3(y - 4).\", \"gold_answer\": \"5\"},\n",
    "        # Add more problems as needed...\n",
    "    ]\n",
    "    \n",
    "    mcr = MCReward(config=cfg , model=model, tokenizer=tokenizer)\n",
    "    entries_raw = mcr.build_datasets(problems)\n",
    "\n",
    "    # Print or inspect the dataset\n",
    "    for entry in entries_raw:\n",
    "        print(entry)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    random.shuffle(entries_raw)\n",
    "    # split_idx       = int(0.9 * len(entries_raw)) if len(entries_raw) > 1 else 1\n",
    "    split_idx = 1\n",
    "    train_entries   = entries_raw[:split_idx]\n",
    "    val_entries     = entries_raw[split_idx:] or entries_raw[:1]   # 최소 1개 확보\n",
    "\n",
    "    trainer = PRMTrainer(cfg, model=model, tokenizer=tokenizer)\n",
    "    history = trainer.fit(train_entries, val_entries)\n",
    "    print(\"Training complete. Loss history:\", history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
